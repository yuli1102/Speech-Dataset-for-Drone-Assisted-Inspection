{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"   #(xxxx is your specific GPU ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from MyEarlyStopping import MyEarlyStopping\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# from keras.optimizers import adam\n",
    "\n",
    "import timeit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import LabelEncoder  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_number = 5\n",
    "train_image = 10 #10:20%, 20: 40%, 30:60%\n",
    "train_image_s2 = 30 #10:20%, 20: 40%, 30:60%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training dataset (40%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1714 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "train_generator = train_data.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_Data/train',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = train_generator.filenames\n",
    "image_no = [i.split(\"/\")[1].split(\"_\")[2].split(\".\")[0] for i in image_names]\n",
    "image_no = np.array(list(map(int, image_no)))\n",
    "ALL_participant_class = [i.split(\"/\")[1].split(\"_\")[1] for i in image_names]\n",
    "ALL_participant_class = np.array(list(map(int, ALL_participant_class)))\n",
    "command_class = train_generator.classes\n",
    "All_participant_class = tf.keras.utils.to_categorical(ALL_participant_class-1, num_classes=train_number)\n",
    "All_command_class = tf.keras.utils.to_categorical(command_class, num_classes=10)\n",
    "All_command_uniform = All_command_class*0+1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Inputs = [next(train_generator)[0][0] for _ in range(len(train_generator))]\n",
    "All_Inputs = np.array(All_Inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_indexs_train = ((image_no<train_image)&(ALL_participant_class!=2))|((image_no<train_image_s2)&(ALL_participant_class==2))\n",
    "Train_Inputs = All_Inputs[select_indexs_train]\n",
    "Train_participant_class = All_participant_class[select_indexs_train]\n",
    "Train_participant_uniform = Train_participant_class*0+1/train_number\n",
    "Train_command_class = All_command_class[select_indexs_train]\n",
    "Train_command_uniform = Train_command_class*0+1/10\n",
    "# sum(Train_participant_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 543 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_generator = val_data.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_Data/validation',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = val_generator.filenames\n",
    "participant_class = [i.split(\"/\", 1)[1].split(\"_\")[1] for i in image_names]\n",
    "participant_class = np.array(list(map(int, participant_class)))\n",
    "command_class = val_generator.classes\n",
    "Val_participant_class = tf.keras.utils.to_categorical(participant_class-1, num_classes=train_number)\n",
    "Val_participant_uniform = Val_participant_class*0+1/train_number\n",
    "Val_command_class = tf.keras.utils.to_categorical(command_class, num_classes=10)\n",
    "Val_command_uniform = Val_command_class*0+1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Val_Inputs = [next(val_generator)[0][0] for _ in range(len(val_generator))]\n",
    "Val_Inputs = np.array(Val_Inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 543 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator = test_data.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_Data/test',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = test_generator.filenames\n",
    "participant_class = [i.split(\"/\", 1)[1].split(\"_\")[1] for i in image_names]\n",
    "test_unit_participant_class = np.array(list(map(int, participant_class)))\n",
    "test_unit_command_class = test_generator.classes\n",
    "Test_participant_class = tf.keras.utils.to_categorical(test_unit_participant_class-1, num_classes=train_number)\n",
    "Test_participant_uniform = Test_participant_class*0+1/train_number\n",
    "Test_command_class = tf.keras.utils.to_categorical(test_unit_command_class, num_classes=10)\n",
    "Test_command_uniform = Test_command_class*0+1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Inputs = [next(test_generator)[0][0] for _ in range(len(test_generator))]\n",
    "Test_Inputs = np.array(Test_Inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Inputs_1 = Test_Inputs[np.where(test_unit_participant_class == 1)]\n",
    "Test_command_class_1 = Test_command_class[np.where(test_unit_participant_class == 1)]\n",
    "Test_Inputs_2 = Test_Inputs[np.where(test_unit_participant_class == 2)]\n",
    "Test_command_class_2 = Test_command_class[np.where(test_unit_participant_class == 2)]\n",
    "Test_Inputs_3 = Test_Inputs[np.where(test_unit_participant_class == 3)]\n",
    "Test_command_class_3 = Test_command_class[np.where(test_unit_participant_class == 3)]\n",
    "Test_Inputs_4 = Test_Inputs[np.where(test_unit_participant_class == 4)]\n",
    "Test_command_class_4 = Test_command_class[np.where(test_unit_participant_class == 4)]\n",
    "Test_Inputs_5 = Test_Inputs[np.where(test_unit_participant_class == 5)]\n",
    "Test_command_class_5 = Test_command_class[np.where(test_unit_participant_class == 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker 6 Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_6 = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator_6 = test_data_6.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_subject/p_6_split/test',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = test_generator_6.filenames\n",
    "command_class = [i.split(\"/\", 1)[0]for i in image_names]\n",
    "le = LabelEncoder()\n",
    "test_unit_command_class_6 = le.fit_transform(command_class)\n",
    "Test_command_class_6 = tf.keras.utils.to_categorical(test_unit_command_class_6, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Inputs_6 = [next(test_generator_6)[0][0] for _ in range(len(test_generator_6))]\n",
    "Test_Inputs_6 = np.array(Test_Inputs_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker 7 Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_7 = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator_7 = test_data_6.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_subject/p_7_split/test',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = test_generator_7.filenames\n",
    "command_class = [i.split(\"/\", 1)[0]for i in image_names]\n",
    "le = LabelEncoder()\n",
    "test_unit_command_class_7 = le.fit_transform(command_class)\n",
    "Test_command_class_7 = tf.keras.utils.to_categorical(test_unit_command_class_7, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Inputs_7 = [next(test_generator_7)[0][0] for _ in range(len(test_generator_7))]\n",
    "Test_Inputs_7 = np.array(Test_Inputs_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker 8 Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 101 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_8 = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator_8 = test_data_6.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_subject/p_8_split/test',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = test_generator_8.filenames\n",
    "command_class = [i.split(\"/\", 1)[0]for i in image_names]\n",
    "le = LabelEncoder()\n",
    "test_unit_command_class_8 = le.fit_transform(command_class)\n",
    "Test_command_class_8 = tf.keras.utils.to_categorical(test_unit_command_class_8, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Inputs_8 = [next(test_generator_8)[0][0] for _ in range(len(test_generator_8))]\n",
    "Test_Inputs_8 = np.array(Test_Inputs_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pd file store performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Size</th>\n",
       "      <th>Time</th>\n",
       "      <th>Partcp_Acc_p15</th>\n",
       "      <th>Command_Acc_p15</th>\n",
       "      <th>Partcp_Acc_p1</th>\n",
       "      <th>Command_Acc_p1</th>\n",
       "      <th>Partcp_Acc_p2</th>\n",
       "      <th>Command_Acc_p2</th>\n",
       "      <th>Partcp_Acc_p3</th>\n",
       "      <th>Command_Acc_p3</th>\n",
       "      <th>Partcp_Acc_p4</th>\n",
       "      <th>Command_Acc_p4</th>\n",
       "      <th>Partcp_Acc_p5</th>\n",
       "      <th>Command_Acc_p5</th>\n",
       "      <th>Acc_p6</th>\n",
       "      <th>Acc_p7</th>\n",
       "      <th>Acc_p8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>47.540156</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.9484</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.3366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>48.307249</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>46.201615</td>\n",
       "      <td>0.9613</td>\n",
       "      <td>0.9448</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9358</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>43.692362</td>\n",
       "      <td>0.9650</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9358</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>44.504580</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>0.9484</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>44.229166</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>44.535541</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>42.948731</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.2574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>45.444911</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.9503</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>42.707019</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.4059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>67.289367</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9640</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>64.328163</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>61.978212</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>60.036626</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>58.215136</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>49.347837</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>59.879732</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.3366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>57.413757</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9640</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>50.219540</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>53.871787</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>80.966223</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>77.376384</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.2178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>79.340686</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>63.791425</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>78.678261</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.2376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>71.382334</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>76.130331</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9853</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>70.832085</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>73.439023</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.2277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>72.148432</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>47.769887</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>49.705597</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.9576</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>44.363904</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9576</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9478</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.2376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>46.206470</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9650</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>47.192904</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>47.685570</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.9558</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>44.867667</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>44.795298</td>\n",
       "      <td>0.9853</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>40.871670</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.9558</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.9391</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>42.479896</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>51.959552</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>52.339809</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9478</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9358</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>53.653035</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.8899</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>49.510542</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9640</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9304</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>49.548754</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>53.723940</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9391</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>50.595989</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9174</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>48.617480</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.2475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>47.476377</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>48.240296</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model             Size       Time  Partcp_Acc_p15  Command_Acc_p15  \\\n",
       "0   Group              20%  47.540156          0.9724           0.9484   \n",
       "1   Group              20%  48.307249          0.9779           0.9521   \n",
       "2   Group              20%  46.201615          0.9613           0.9448   \n",
       "3   Group              20%  43.692362          0.9650           0.9521   \n",
       "4   Group              20%  44.504580          0.9687           0.9484   \n",
       "5   Group              20%  44.229166          0.9705           0.9521   \n",
       "6   Group              20%  44.535541          0.9632           0.9540   \n",
       "7   Group              20%  42.948731          0.9724           0.9595   \n",
       "8   Group              20%  45.444911          0.9724           0.9503   \n",
       "9   Group              20%  42.707019          0.9669           0.9540   \n",
       "10  Group              40%  67.289367          0.9945           0.9761   \n",
       "11  Group              40%  64.328163          0.9890           0.9761   \n",
       "12  Group              40%  61.978212          0.9926           0.9797   \n",
       "13  Group              40%  60.036626          0.9926           0.9779   \n",
       "14  Group              40%  58.215136          0.9890           0.9761   \n",
       "15  Group              40%  49.347837          0.9871           0.9761   \n",
       "16  Group              40%  59.879732          0.9908           0.9779   \n",
       "17  Group              40%  57.413757          0.9908           0.9761   \n",
       "18  Group              40%  50.219540          0.9908           0.9761   \n",
       "19  Group              40%  53.871787          0.9945           0.9742   \n",
       "20  Group              60%  80.966223          0.9926           0.9871   \n",
       "21  Group              60%  77.376384          0.9926           0.9797   \n",
       "22  Group              60%  79.340686          0.9945           0.9816   \n",
       "23  Group              60%  63.791425          0.9890           0.9871   \n",
       "24  Group              60%  78.678261          0.9926           0.9797   \n",
       "25  Group              60%  71.382334          0.9945           0.9834   \n",
       "26  Group              60%  76.130331          0.9908           0.9853   \n",
       "27  Group              60%  70.832085          0.9945           0.9871   \n",
       "28  Group              60%  73.439023          0.9945           0.9890   \n",
       "29  Group              60%  72.148432          0.9945           0.9834   \n",
       "30  Group   mix_20%&s2_40%  47.769887          0.9834           0.9669   \n",
       "31  Group   mix_20%&s2_40%  49.705597          0.9816           0.9576   \n",
       "32  Group   mix_20%&s2_40%  44.363904          0.9797           0.9576   \n",
       "33  Group   mix_20%&s2_40%  46.206470          0.9945           0.9650   \n",
       "34  Group   mix_20%&s2_40%  47.192904          0.9797           0.9632   \n",
       "35  Group   mix_20%&s2_40%  47.685570          0.9761           0.9558   \n",
       "36  Group   mix_20%&s2_40%  44.867667          0.9816           0.9687   \n",
       "37  Group   mix_20%&s2_40%  44.795298          0.9853           0.9724   \n",
       "38  Group   mix_20%&s2_40%  40.871670          0.9742           0.9558   \n",
       "39  Group   mix_20%&s2_40%  42.479896          0.9834           0.9687   \n",
       "40  Group  mix_20%&s12_40%  51.959552          0.9834           0.9687   \n",
       "41  Group  mix_20%&s12_40%  52.339809          0.9705           0.9669   \n",
       "42  Group  mix_20%&s12_40%  53.653035          0.9724           0.9705   \n",
       "43  Group  mix_20%&s12_40%  49.510542          0.9705           0.9669   \n",
       "44  Group  mix_20%&s12_40%  49.548754          0.9834           0.9669   \n",
       "45  Group  mix_20%&s12_40%  53.723940          0.9761           0.9669   \n",
       "46  Group  mix_20%&s12_40%  50.595989          0.9761           0.9595   \n",
       "47  Group  mix_20%&s12_40%  48.617480          0.9797           0.9632   \n",
       "48  Group  mix_20%&s12_40%  47.476377          0.9834           0.9705   \n",
       "49  Group  mix_20%&s12_40%  48.240296          0.9797           0.9687   \n",
       "\n",
       "    Partcp_Acc_p1  Command_Acc_p1  Partcp_Acc_p2  Command_Acc_p2  \\\n",
       "0           0.991          0.9550         0.9259          0.8796   \n",
       "1           1.000          0.9369         0.9352          0.8796   \n",
       "2           0.991          0.9369         0.9074          0.8611   \n",
       "3           1.000          0.9459         0.9074          0.8704   \n",
       "4           0.991          0.9550         0.9259          0.8796   \n",
       "5           1.000          0.9550         0.9074          0.8796   \n",
       "6           0.991          0.9369         0.9259          0.8981   \n",
       "7           1.000          0.9550         0.9074          0.8889   \n",
       "8           0.991          0.9459         0.9167          0.8796   \n",
       "9           0.991          0.9459         0.9167          0.8704   \n",
       "10          0.991          0.9640         0.9907          0.9352   \n",
       "11          0.982          0.9730         0.9815          0.9259   \n",
       "12          0.991          0.9730         0.9907          0.9444   \n",
       "13          0.991          0.9730         0.9907          0.9352   \n",
       "14          0.982          0.9730         0.9815          0.9352   \n",
       "15          0.982          0.9730         0.9815          0.9352   \n",
       "16          0.991          0.9730         0.9815          0.9444   \n",
       "17          0.991          0.9640         0.9815          0.9444   \n",
       "18          0.991          0.9730         0.9815          0.9352   \n",
       "19          1.000          0.9730         0.9907          0.9259   \n",
       "20          0.991          0.9820         0.9815          0.9722   \n",
       "21          0.982          0.9820         1.0000          0.9537   \n",
       "22          0.982          0.9820         1.0000          0.9444   \n",
       "23          0.973          0.9910         0.9815          0.9630   \n",
       "24          0.982          0.9820         1.0000          0.9537   \n",
       "25          0.982          0.9820         1.0000          0.9537   \n",
       "26          0.982          0.9910         0.9907          0.9537   \n",
       "27          0.991          0.9820         0.9907          0.9722   \n",
       "28          0.982          0.9910         1.0000          0.9722   \n",
       "29          0.982          0.9910         1.0000          0.9444   \n",
       "30          0.991          0.9459         0.9815          0.9352   \n",
       "31          0.991          0.9369         1.0000          0.9259   \n",
       "32          0.991          0.9550         0.9815          0.9259   \n",
       "33          0.991          0.9550         1.0000          0.9259   \n",
       "34          0.982          0.9369         0.9907          0.9444   \n",
       "35          0.991          0.9279         0.9907          0.9352   \n",
       "36          0.991          0.9550         1.0000          0.9537   \n",
       "37          1.000          0.9550         1.0000          0.9630   \n",
       "38          1.000          0.9459         0.9815          0.9167   \n",
       "39          0.982          0.9550         1.0000          0.9352   \n",
       "40          1.000          0.9730         0.9907          0.9444   \n",
       "41          1.000          0.9730         0.9907          0.9444   \n",
       "42          1.000          0.9910         0.9907          0.9259   \n",
       "43          0.991          0.9640         0.9907          0.9444   \n",
       "44          1.000          0.9550         0.9907          0.9444   \n",
       "45          0.991          0.9820         0.9907          0.9444   \n",
       "46          1.000          0.9550         0.9815          0.9352   \n",
       "47          0.991          0.9730         0.9907          0.9352   \n",
       "48          0.991          0.9910         0.9815          0.9259   \n",
       "49          1.000          0.9730         0.9907          0.9444   \n",
       "\n",
       "    Partcp_Acc_p3  Command_Acc_p3  Partcp_Acc_p4  Command_Acc_p4  \\\n",
       "0          0.9652          0.9565         0.9817          0.9633   \n",
       "1          0.9739          0.9652         0.9908          0.9908   \n",
       "2          0.9826          0.9652         0.9358          0.9725   \n",
       "3          0.9826          0.9652         0.9358          0.9908   \n",
       "4          0.9739          0.9652         0.9541          0.9541   \n",
       "5          0.9826          0.9739         0.9633          0.9633   \n",
       "6          0.9565          0.9652         0.9541          0.9817   \n",
       "7          0.9913          0.9739         0.9633          0.9908   \n",
       "8          0.9913          0.9652         0.9633          0.9725   \n",
       "9          0.9739          0.9652         0.9541          0.9908   \n",
       "10         1.0000          0.9913         0.9908          0.9908   \n",
       "11         0.9913          0.9913         0.9908          0.9908   \n",
       "12         0.9913          0.9913         0.9908          0.9908   \n",
       "13         0.9913          0.9913         0.9908          0.9908   \n",
       "14         0.9913          0.9826         0.9908          0.9908   \n",
       "15         0.9913          0.9913         0.9817          0.9908   \n",
       "16         0.9913          0.9826         0.9908          0.9908   \n",
       "17         0.9913          0.9826         0.9908          0.9908   \n",
       "18         0.9913          0.9913         0.9908          0.9908   \n",
       "19         0.9913          0.9913         0.9908          0.9817   \n",
       "20         1.0000          0.9913         0.9908          0.9908   \n",
       "21         0.9913          0.9826         0.9908          0.9817   \n",
       "22         1.0000          0.9913         0.9908          0.9908   \n",
       "23         1.0000          0.9913         0.9908          0.9908   \n",
       "24         0.9913          0.9826         0.9908          0.9817   \n",
       "25         1.0000          0.9913         0.9908          0.9908   \n",
       "26         0.9913          0.9913         0.9908          0.9908   \n",
       "27         1.0000          0.9913         0.9908          0.9908   \n",
       "28         1.0000          0.9913         0.9908          0.9908   \n",
       "29         1.0000          0.9913         0.9908          0.9908   \n",
       "30         0.9739          0.9826         0.9725          0.9817   \n",
       "31         0.9739          0.9652         0.9450          0.9817   \n",
       "32         0.9652          0.9478         0.9908          0.9725   \n",
       "33         0.9913          0.9652         0.9908          0.9908   \n",
       "34         0.9739          0.9652         0.9541          0.9817   \n",
       "35         0.9565          0.9565         0.9633          0.9817   \n",
       "36         0.9652          0.9652         0.9541          0.9817   \n",
       "37         0.9652          0.9739         0.9633          0.9817   \n",
       "38         0.9391          0.9565         0.9541          0.9725   \n",
       "39         0.9739          0.9826         0.9633          0.9817   \n",
       "40         0.9826          0.9565         0.9541          0.9817   \n",
       "41         0.9478          0.9652         0.9358          0.9725   \n",
       "42         0.9826          0.9652         0.8899          0.9817   \n",
       "43         0.9304          0.9565         0.9541          0.9725   \n",
       "44         0.9826          0.9652         0.9450          0.9817   \n",
       "45         0.9739          0.9391         0.9450          0.9817   \n",
       "46         0.9826          0.9565         0.9174          0.9633   \n",
       "47         0.9652          0.9565         0.9541          0.9725   \n",
       "48         0.9739          0.9739         0.9725          0.9817   \n",
       "49         0.9652          0.9565         0.9450          0.9817   \n",
       "\n",
       "    Partcp_Acc_p5  Command_Acc_p5  Acc_p6  Acc_p7  Acc_p8  \n",
       "0            1.00            0.99    0.67   0.528  0.3366  \n",
       "1            0.99            0.99    0.59   0.528  0.2970  \n",
       "2            0.99            0.99    0.67   0.432  0.2673  \n",
       "3            1.00            0.99    0.62   0.504  0.2673  \n",
       "4            1.00            0.99    0.59   0.472  0.3267  \n",
       "5            1.00            0.99    0.62   0.456  0.3069  \n",
       "6            0.99            0.99    0.67   0.496  0.3267  \n",
       "7            1.00            0.99    0.67   0.496  0.2574  \n",
       "8            1.00            0.99    0.60   0.552  0.2673  \n",
       "9            1.00            1.00    0.66   0.480  0.4059  \n",
       "10           1.00            1.00    0.65   0.472  0.2772  \n",
       "11           1.00            1.00    0.63   0.480  0.2871  \n",
       "12           1.00            1.00    0.69   0.400  0.3168  \n",
       "13           1.00            1.00    0.70   0.456  0.3267  \n",
       "14           1.00            1.00    0.70   0.448  0.2871  \n",
       "15           1.00            0.99    0.70   0.472  0.2772  \n",
       "16           1.00            1.00    0.70   0.440  0.3366  \n",
       "17           1.00            1.00    0.67   0.440  0.3168  \n",
       "18           1.00            0.99    0.63   0.456  0.2772  \n",
       "19           1.00            1.00    0.65   0.424  0.2673  \n",
       "20           1.00            1.00    0.69   0.440  0.2772  \n",
       "21           1.00            1.00    0.72   0.456  0.2178  \n",
       "22           1.00            1.00    0.68   0.440  0.2970  \n",
       "23           1.00            1.00    0.68   0.424  0.3069  \n",
       "24           1.00            1.00    0.68   0.424  0.2376  \n",
       "25           1.00            1.00    0.66   0.408  0.2475  \n",
       "26           1.00            1.00    0.73   0.416  0.2673  \n",
       "27           1.00            1.00    0.70   0.440  0.2673  \n",
       "28           1.00            1.00    0.70   0.424  0.2277  \n",
       "29           1.00            1.00    0.70   0.416  0.2772  \n",
       "30           1.00            0.99    0.69   0.552  0.3267  \n",
       "31           1.00            0.98    0.69   0.512  0.3168  \n",
       "32           0.97            0.99    0.72   0.536  0.2376  \n",
       "33           1.00            0.99    0.70   0.480  0.2673  \n",
       "34           1.00            0.99    0.69   0.504  0.2871  \n",
       "35           0.98            0.98    0.72   0.472  0.2871  \n",
       "36           1.00            0.99    0.66   0.496  0.2673  \n",
       "37           1.00            0.99    0.68   0.544  0.3069  \n",
       "38           1.00            0.99    0.71   0.480  0.2970  \n",
       "39           1.00            0.99    0.73   0.536  0.2673  \n",
       "40           0.99            0.99    0.66   0.504  0.3168  \n",
       "41           0.98            0.98    0.67   0.424  0.2673  \n",
       "42           1.00            0.99    0.67   0.544  0.3168  \n",
       "43           0.99            1.00    0.67   0.464  0.3168  \n",
       "44           1.00            0.99    0.65   0.488  0.2970  \n",
       "45           0.98            0.99    0.66   0.488  0.3069  \n",
       "46           1.00            0.99    0.64   0.456  0.3069  \n",
       "47           1.00            0.98    0.65   0.480  0.2475  \n",
       "48           1.00            0.98    0.59   0.472  0.3168  \n",
       "49           1.00            0.99    0.69   0.496  0.3069  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd file store performance\n",
    "# Perfomance = pd.DataFrame(columns = ['Model','Time','Partcp_Acc_p15','Command_Acc_p15','Partcp_Acc_p1','Command_Acc_p1',\n",
    "#                                       'Partcp_Acc_p2','Command_Acc_p2','Partcp_Acc_p3','Command_Acc_p3',\n",
    "#                                       'Partcp_Acc_p4','Command_Acc_p4','Partcp_Acc_p5','Command_Acc_p5','Acc_p6','Acc_p7','Acc_p8'])\n",
    "\n",
    "Perfomance = pd.read_csv('Performance_0608_training_Data_Size.csv')\n",
    "# Perfomance\n",
    "Perfomance                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "actv_fun_1_1 = \"relu\" \n",
    "actv_fun_1_2 = \"sigmoid\"\n",
    "shape_1_1 = 128\n",
    "shape_1_2 = 256\n",
    "actv_fun_2_1 = \"sigmoid\" \n",
    "actv_fun_2_2 = \"sigmoid\"\n",
    "shape_2_1 = 512\n",
    "shape_2_2 = 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 391ms/step - loss: 3.6495 - participant_output_loss: 1.4227 - command_output_loss: 2.2206 - command_output_1_loss: 7.9962e-04 - participant_output_1_loss: 0.0054 - participant_output_accuracy: 0.4514 - command_output_accuracy: 0.3514 - command_output_1_accuracy: 0.0057 - participant_output_1_accuracy: 0.0943 - val_loss: 3.4181 - val_participant_output_loss: 1.4038 - val_command_output_loss: 2.0139 - val_command_output_1_loss: 2.7688e-04 - val_participant_output_1_loss: 1.6873e-04 - val_participant_output_accuracy: 0.3389 - val_command_output_accuracy: 0.5506 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1418\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 2.9315 - participant_output_loss: 1.0199 - command_output_loss: 1.9112 - command_output_1_loss: 2.3149e-04 - participant_output_1_loss: 2.2275e-04 - participant_output_accuracy: 0.6271 - command_output_accuracy: 0.7286 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.4029 - val_loss: 2.9929 - val_participant_output_loss: 1.1479 - val_command_output_loss: 1.8447 - val_command_output_1_loss: 1.1643e-04 - val_participant_output_1_loss: 1.1387e-04 - val_participant_output_accuracy: 0.5709 - val_command_output_accuracy: 0.7293 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2026\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 2.6005 - participant_output_loss: 0.8357 - command_output_loss: 1.7646 - command_output_1_loss: 7.7898e-05 - participant_output_1_loss: 1.1789e-04 - participant_output_accuracy: 0.7786 - command_output_accuracy: 0.8371 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.0757 - val_loss: 2.7224 - val_participant_output_loss: 0.9928 - val_command_output_loss: 1.7295 - val_command_output_1_loss: 5.8119e-05 - val_participant_output_1_loss: 8.5219e-05 - val_participant_output_accuracy: 0.7053 - val_command_output_accuracy: 0.8435 - val_command_output_1_accuracy: 0.0110 - val_participant_output_1_accuracy: 0.2891\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 185ms/step - loss: 2.3572 - participant_output_loss: 0.7089 - command_output_loss: 1.6481 - command_output_1_loss: 6.6926e-05 - participant_output_1_loss: 7.0591e-05 - participant_output_accuracy: 0.8414 - command_output_accuracy: 0.9086 - command_output_1_accuracy: 0.0129 - participant_output_1_accuracy: 0.3443 - val_loss: 2.4834 - val_participant_output_loss: 0.8613 - val_command_output_loss: 1.6219 - val_command_output_1_loss: 4.5107e-05 - val_participant_output_1_loss: 6.3844e-05 - val_participant_output_accuracy: 0.7882 - val_command_output_accuracy: 0.8785 - val_command_output_1_accuracy: 0.0939 - val_participant_output_1_accuracy: 0.1418\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 2.1303 - participant_output_loss: 0.5953 - command_output_loss: 1.5349 - command_output_1_loss: 5.9227e-05 - participant_output_1_loss: 5.7580e-05 - participant_output_accuracy: 0.9057 - command_output_accuracy: 0.9286 - command_output_1_accuracy: 0.2800 - participant_output_1_accuracy: 0.1243 - val_loss: 2.2393 - val_participant_output_loss: 0.7299 - val_command_output_loss: 1.5093 - val_command_output_1_loss: 8.6552e-05 - val_participant_output_1_loss: 5.7547e-05 - val_participant_output_accuracy: 0.8913 - val_command_output_accuracy: 0.8987 - val_command_output_1_accuracy: 0.1897 - val_participant_output_1_accuracy: 0.1915\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 187ms/step - loss: 1.9191 - participant_output_loss: 0.4991 - command_output_loss: 1.4199 - command_output_1_loss: 7.0312e-05 - participant_output_1_loss: 4.9446e-05 - participant_output_accuracy: 0.9371 - command_output_accuracy: 0.9543 - command_output_1_accuracy: 0.0729 - participant_output_1_accuracy: 0.2686 - val_loss: 2.0428 - val_participant_output_loss: 0.6348 - val_command_output_loss: 1.4078 - val_command_output_1_loss: 6.1835e-05 - val_participant_output_1_loss: 5.6890e-05 - val_participant_output_accuracy: 0.9227 - val_command_output_accuracy: 0.9282 - val_command_output_1_accuracy: 0.0055 - val_participant_output_1_accuracy: 0.1252\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 184ms/step - loss: 1.7294 - participant_output_loss: 0.4183 - command_output_loss: 1.3110 - command_output_1_loss: 5.9692e-05 - participant_output_1_loss: 4.5981e-05 - participant_output_accuracy: 0.9729 - command_output_accuracy: 0.9671 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1743 - val_loss: 1.8924 - val_participant_output_loss: 0.5831 - val_command_output_loss: 1.3093 - val_command_output_1_loss: 4.0324e-05 - val_participant_output_1_loss: 5.4345e-05 - val_participant_output_accuracy: 0.9282 - val_command_output_accuracy: 0.9521 - val_command_output_1_accuracy: 0.0055 - val_participant_output_1_accuracy: 0.1400\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 1.5719 - participant_output_loss: 0.3651 - command_output_loss: 1.2067 - command_output_1_loss: 3.7525e-05 - participant_output_1_loss: 4.4669e-05 - participant_output_accuracy: 0.9814 - command_output_accuracy: 0.9800 - command_output_1_accuracy: 0.2643 - participant_output_1_accuracy: 0.1814 - val_loss: 1.7156 - val_participant_output_loss: 0.5053 - val_command_output_loss: 1.2102 - val_command_output_1_loss: 4.5521e-05 - val_participant_output_1_loss: 5.2710e-05 - val_participant_output_accuracy: 0.9484 - val_command_output_accuracy: 0.9595 - val_command_output_1_accuracy: 0.5341 - val_participant_output_1_accuracy: 0.1639\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 144ms/step - loss: 1.4206 - participant_output_loss: 0.3194 - command_output_loss: 1.1012 - command_output_1_loss: 5.1250e-05 - participant_output_1_loss: 4.2642e-05 - participant_output_accuracy: 0.9843 - command_output_accuracy: 0.9900 - command_output_1_accuracy: 0.2543 - participant_output_1_accuracy: 0.1614 - val_loss: 1.5907 - val_participant_output_loss: 0.4703 - val_command_output_loss: 1.1203 - val_command_output_1_loss: 4.8599e-05 - val_participant_output_1_loss: 5.1083e-05 - val_participant_output_accuracy: 0.9429 - val_command_output_accuracy: 0.9540 - val_command_output_1_accuracy: 0.0203 - val_participant_output_1_accuracy: 0.2523\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 1.2803 - participant_output_loss: 0.2793 - command_output_loss: 1.0008 - command_output_1_loss: 5.5181e-05 - participant_output_1_loss: 3.9856e-05 - participant_output_accuracy: 0.9900 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.0043 - participant_output_1_accuracy: 0.1914 - val_loss: 1.4591 - val_participant_output_loss: 0.4297 - val_command_output_loss: 1.0293 - val_command_output_1_loss: 3.9293e-05 - val_participant_output_1_loss: 4.9485e-05 - val_participant_output_accuracy: 0.9558 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1676\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 268ms/step - loss: 1.1526 - participant_output_loss: 0.2449 - command_output_loss: 0.9076 - command_output_1_loss: 3.6468e-05 - participant_output_1_loss: 3.8530e-05 - participant_output_accuracy: 0.9929 - command_output_accuracy: 0.9943 - command_output_1_accuracy: 0.1300 - participant_output_1_accuracy: 0.2057 - val_loss: 1.3320 - val_participant_output_loss: 0.3937 - val_command_output_loss: 0.9382 - val_command_output_1_loss: 4.7556e-05 - val_participant_output_1_loss: 4.9560e-05 - val_participant_output_accuracy: 0.9558 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.2505 - val_participant_output_1_accuracy: 0.1731\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 186ms/step - loss: 1.0377 - participant_output_loss: 0.2205 - command_output_loss: 0.8171 - command_output_1_loss: 3.4110e-05 - participant_output_1_loss: 3.7122e-05 - participant_output_accuracy: 0.9929 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.0929 - participant_output_1_accuracy: 0.1757 - val_loss: 1.2308 - val_participant_output_loss: 0.3590 - val_command_output_loss: 0.8717 - val_command_output_1_loss: 3.2538e-05 - val_participant_output_1_loss: 4.9943e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9595 - val_command_output_1_accuracy: 0.0203 - val_participant_output_1_accuracy: 0.2910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 0.9318 - participant_output_loss: 0.1923 - command_output_loss: 0.7394 - command_output_1_loss: 3.1955e-05 - participant_output_1_loss: 3.5662e-05 - participant_output_accuracy: 0.9986 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.0500 - participant_output_1_accuracy: 0.1671 - val_loss: 1.1180 - val_participant_output_loss: 0.3249 - val_command_output_loss: 0.7930 - val_command_output_1_loss: 3.1383e-05 - val_participant_output_1_loss: 4.8778e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9595 - val_command_output_1_accuracy: 0.0166 - val_participant_output_1_accuracy: 0.2689\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 0.8259 - participant_output_loss: 0.1659 - command_output_loss: 0.6600 - command_output_1_loss: 2.9938e-05 - participant_output_1_loss: 3.4858e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0157 - participant_output_1_accuracy: 0.1729 - val_loss: 1.0265 - val_participant_output_loss: 0.2963 - val_command_output_loss: 0.7301 - val_command_output_1_loss: 2.8567e-05 - val_participant_output_1_loss: 4.7564e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.0184 - val_participant_output_1_accuracy: 0.2505\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 144ms/step - loss: 0.7418 - participant_output_loss: 0.1490 - command_output_loss: 0.5928 - command_output_1_loss: 3.0600e-05 - participant_output_1_loss: 3.3319e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0143 - participant_output_1_accuracy: 0.1714 - val_loss: 0.9531 - val_participant_output_loss: 0.2863 - val_command_output_loss: 0.6668 - val_command_output_1_loss: 3.6904e-05 - val_participant_output_1_loss: 4.6346e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.0534 - val_participant_output_1_accuracy: 0.2155\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 167ms/step - loss: 0.6606 - participant_output_loss: 0.1338 - command_output_loss: 0.5267 - command_output_1_loss: 3.5247e-05 - participant_output_1_loss: 3.0938e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.2514 - participant_output_1_accuracy: 0.1629 - val_loss: 0.8809 - val_participant_output_loss: 0.2699 - val_command_output_loss: 0.6109 - val_command_output_1_loss: 3.7699e-05 - val_participant_output_1_loss: 4.6558e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.1897 - val_participant_output_1_accuracy: 0.2394\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 183ms/step - loss: 0.5939 - participant_output_loss: 0.1192 - command_output_loss: 0.4746 - command_output_1_loss: 3.0168e-05 - participant_output_1_loss: 2.9768e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0571 - participant_output_1_accuracy: 0.1729 - val_loss: 0.8121 - val_participant_output_loss: 0.2505 - val_command_output_loss: 0.5616 - val_command_output_1_loss: 2.2872e-05 - val_participant_output_1_loss: 4.4085e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9724 - val_command_output_1_accuracy: 0.0074 - val_participant_output_1_accuracy: 0.2118\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 153ms/step - loss: 0.5381 - participant_output_loss: 0.1095 - command_output_loss: 0.4285 - command_output_1_loss: 2.9988e-05 - participant_output_1_loss: 2.8045e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0271 - participant_output_1_accuracy: 0.1671 - val_loss: 0.7650 - val_participant_output_loss: 0.2432 - val_command_output_loss: 0.5217 - val_command_output_1_loss: 2.8533e-05 - val_participant_output_1_loss: 4.4915e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9687 - val_command_output_1_accuracy: 0.1768 - val_participant_output_1_accuracy: 0.1860\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 145ms/step - loss: 0.4825 - participant_output_loss: 0.0977 - command_output_loss: 0.3848 - command_output_1_loss: 2.0401e-05 - participant_output_1_loss: 2.7130e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1114 - participant_output_1_accuracy: 0.1757 - val_loss: 0.6939 - val_participant_output_loss: 0.2142 - val_command_output_loss: 0.4796 - val_command_output_1_loss: 2.5620e-05 - val_participant_output_1_loss: 4.4074e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.0295 - val_participant_output_1_accuracy: 0.2063\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 153ms/step - loss: 0.4352 - participant_output_loss: 0.0888 - command_output_loss: 0.3463 - command_output_1_loss: 2.7831e-05 - participant_output_1_loss: 2.5553e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0314 - participant_output_1_accuracy: 0.1800 - val_loss: 0.6478 - val_participant_output_loss: 0.2025 - val_command_output_loss: 0.4452 - val_command_output_1_loss: 3.9631e-05 - val_participant_output_1_loss: 4.2925e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.0037 - val_participant_output_1_accuracy: 0.2560\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 343ms/step - loss: 3.5609 - participant_output_loss: 1.3011 - command_output_loss: 2.2530 - command_output_1_loss: 6.9089e-04 - participant_output_1_loss: 0.0061 - participant_output_accuracy: 0.4843 - command_output_accuracy: 0.2786 - command_output_1_accuracy: 0.0514 - participant_output_1_accuracy: 0.1686 - val_loss: 3.3485 - val_participant_output_loss: 1.2753 - val_command_output_loss: 2.0727 - val_command_output_1_loss: 2.0702e-04 - val_participant_output_1_loss: 2.1640e-04 - val_participant_output_accuracy: 0.5009 - val_command_output_accuracy: 0.4751 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0074\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 210ms/step - loss: 2.9231 - participant_output_loss: 0.9246 - command_output_loss: 1.9982 - command_output_1_loss: 1.1925e-04 - participant_output_1_loss: 1.7788e-04 - participant_output_accuracy: 0.7343 - command_output_accuracy: 0.6371 - command_output_1_accuracy: 0.0029 - participant_output_1_accuracy: 0.6014 - val_loss: 2.9265 - val_participant_output_loss: 0.9827 - val_command_output_loss: 1.9436 - val_command_output_1_loss: 7.0179e-05 - val_participant_output_1_loss: 9.9460e-05 - val_participant_output_accuracy: 0.7680 - val_command_output_accuracy: 0.7440 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1621\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 2.6157 - participant_output_loss: 0.7283 - command_output_loss: 1.8872 - command_output_1_loss: 7.5468e-05 - participant_output_1_loss: 8.3200e-05 - participant_output_accuracy: 0.8600 - command_output_accuracy: 0.7443 - command_output_1_accuracy: 0.0229 - participant_output_1_accuracy: 0.0557 - val_loss: 2.6604 - val_participant_output_loss: 0.8120 - val_command_output_loss: 1.8483 - val_command_output_1_loss: 4.6556e-05 - val_participant_output_1_loss: 9.6097e-05 - val_participant_output_accuracy: 0.8711 - val_command_output_accuracy: 0.7956 - val_command_output_1_accuracy: 0.3831 - val_participant_output_1_accuracy: 0.0700\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 2.3554 - participant_output_loss: 0.5721 - command_output_loss: 1.7832 - command_output_1_loss: 8.3669e-05 - participant_output_1_loss: 5.7442e-05 - participant_output_accuracy: 0.9200 - command_output_accuracy: 0.8386 - command_output_1_accuracy: 0.7029 - participant_output_1_accuracy: 0.1014 - val_loss: 2.4247 - val_participant_output_loss: 0.6708 - val_command_output_loss: 1.7537 - val_command_output_1_loss: 1.2262e-04 - val_participant_output_1_loss: 5.0881e-05 - val_participant_output_accuracy: 0.9116 - val_command_output_accuracy: 0.8195 - val_command_output_1_accuracy: 0.6354 - val_participant_output_1_accuracy: 0.2228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 204ms/step - loss: 2.1517 - participant_output_loss: 0.4704 - command_output_loss: 1.6811 - command_output_1_loss: 1.3821e-04 - participant_output_1_loss: 4.3503e-05 - participant_output_accuracy: 0.9514 - command_output_accuracy: 0.8771 - command_output_1_accuracy: 0.1700 - participant_output_1_accuracy: 0.2029 - val_loss: 2.2211 - val_participant_output_loss: 0.5641 - val_command_output_loss: 1.6569 - val_command_output_1_loss: 1.0803e-04 - val_participant_output_1_loss: 4.3905e-05 - val_participant_output_accuracy: 0.9374 - val_command_output_accuracy: 0.8674 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1878\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 208ms/step - loss: 1.9618 - participant_output_loss: 0.3861 - command_output_loss: 1.5756 - command_output_1_loss: 6.1308e-05 - participant_output_1_loss: 3.8865e-05 - participant_output_accuracy: 0.9714 - command_output_accuracy: 0.9443 - command_output_1_accuracy: 0.0029 - participant_output_1_accuracy: 0.1386 - val_loss: 2.0800 - val_participant_output_loss: 0.5227 - val_command_output_loss: 1.5572 - val_command_output_1_loss: 6.5488e-05 - val_participant_output_1_loss: 4.3917e-05 - val_participant_output_accuracy: 0.9208 - val_command_output_accuracy: 0.9263 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2044\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 225ms/step - loss: 1.8095 - participant_output_loss: 0.3380 - command_output_loss: 1.4714 - command_output_1_loss: 9.9962e-05 - participant_output_1_loss: 3.6552e-05 - participant_output_accuracy: 0.9771 - command_output_accuracy: 0.9686 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1700 - val_loss: 1.9264 - val_participant_output_loss: 0.4757 - val_command_output_loss: 1.4506 - val_command_output_1_loss: 7.1910e-05 - val_participant_output_1_loss: 4.1443e-05 - val_participant_output_accuracy: 0.9411 - val_command_output_accuracy: 0.9355 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1750\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 219ms/step - loss: 1.6515 - participant_output_loss: 0.2888 - command_output_loss: 1.3626 - command_output_1_loss: 6.6229e-05 - participant_output_1_loss: 3.5069e-05 - participant_output_accuracy: 0.9871 - command_output_accuracy: 0.9786 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1743 - val_loss: 1.7828 - val_participant_output_loss: 0.4225 - val_command_output_loss: 1.3602 - val_command_output_1_loss: 6.8118e-05 - val_participant_output_1_loss: 4.0235e-05 - val_participant_output_accuracy: 0.9558 - val_command_output_accuracy: 0.9319 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1308\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 211ms/step - loss: 1.5043 - participant_output_loss: 0.2399 - command_output_loss: 1.2644 - command_output_1_loss: 6.3525e-05 - participant_output_1_loss: 3.3673e-05 - participant_output_accuracy: 0.9914 - command_output_accuracy: 0.9857 - command_output_1_accuracy: 0.1871 - participant_output_1_accuracy: 0.1657 - val_loss: 1.6364 - val_participant_output_loss: 0.3658 - val_command_output_loss: 1.2705 - val_command_output_1_loss: 4.0050e-05 - val_participant_output_1_loss: 4.0303e-05 - val_participant_output_accuracy: 0.9613 - val_command_output_accuracy: 0.9448 - val_command_output_1_accuracy: 0.4567 - val_participant_output_1_accuracy: 0.2026\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 217ms/step - loss: 1.3706 - participant_output_loss: 0.2055 - command_output_loss: 1.1650 - command_output_1_loss: 6.6371e-05 - participant_output_1_loss: 3.2929e-05 - participant_output_accuracy: 0.9929 - command_output_accuracy: 0.9871 - command_output_1_accuracy: 0.1186 - participant_output_1_accuracy: 0.1529 - val_loss: 1.4975 - val_participant_output_loss: 0.3215 - val_command_output_loss: 1.1759 - val_command_output_1_loss: 7.6379e-05 - val_participant_output_1_loss: 4.2299e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9503 - val_command_output_1_accuracy: 0.0166 - val_participant_output_1_accuracy: 0.1455\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 266ms/step - loss: 1.2439 - participant_output_loss: 0.1769 - command_output_loss: 1.0668 - command_output_1_loss: 6.1684e-05 - participant_output_1_loss: 3.3271e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9914 - command_output_1_accuracy: 0.1886 - participant_output_1_accuracy: 0.1171 - val_loss: 1.3991 - val_participant_output_loss: 0.3083 - val_command_output_loss: 1.0907 - val_command_output_1_loss: 4.2598e-05 - val_participant_output_1_loss: 4.0806e-05 - val_participant_output_accuracy: 0.9613 - val_command_output_accuracy: 0.9540 - val_command_output_1_accuracy: 0.1971 - val_participant_output_1_accuracy: 0.1179\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 209ms/step - loss: 1.1243 - participant_output_loss: 0.1511 - command_output_loss: 0.9732 - command_output_1_loss: 3.3790e-05 - participant_output_1_loss: 3.0812e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9943 - command_output_1_accuracy: 0.0914 - participant_output_1_accuracy: 0.1757 - val_loss: 1.2837 - val_participant_output_loss: 0.2747 - val_command_output_loss: 1.0090 - val_command_output_1_loss: 3.5335e-05 - val_participant_output_1_loss: 4.0955e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9595 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1436\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 169ms/step - loss: 1.0171 - participant_output_loss: 0.1290 - command_output_loss: 0.8880 - command_output_1_loss: 3.2675e-05 - participant_output_1_loss: 2.9782e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.1257 - participant_output_1_accuracy: 0.1429 - val_loss: 1.1886 - val_participant_output_loss: 0.2460 - val_command_output_loss: 0.9424 - val_command_output_1_loss: 3.2900e-05 - val_participant_output_1_loss: 4.0314e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9503 - val_command_output_1_accuracy: 0.1068 - val_participant_output_1_accuracy: 0.1823\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 215ms/step - loss: 0.9231 - participant_output_loss: 0.1136 - command_output_loss: 0.8094 - command_output_1_loss: 3.7353e-05 - participant_output_1_loss: 2.8814e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0214 - participant_output_1_accuracy: 0.1614 - val_loss: 1.0998 - val_participant_output_loss: 0.2349 - val_command_output_loss: 0.8648 - val_command_output_1_loss: 3.2120e-05 - val_participant_output_1_loss: 3.9780e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9595 - val_command_output_1_accuracy: 0.0074 - val_participant_output_1_accuracy: 0.1529\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 212ms/step - loss: 0.8335 - participant_output_loss: 0.1005 - command_output_loss: 0.7330 - command_output_1_loss: 3.6955e-05 - participant_output_1_loss: 2.7301e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1686 - participant_output_1_accuracy: 0.1543 - val_loss: 1.0110 - val_participant_output_loss: 0.2166 - val_command_output_loss: 0.7944 - val_command_output_1_loss: 2.6268e-05 - val_participant_output_1_loss: 4.0004e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.4291 - val_participant_output_1_accuracy: 0.1657\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 172ms/step - loss: 0.7504 - participant_output_loss: 0.0885 - command_output_loss: 0.6618 - command_output_1_loss: 2.9947e-05 - participant_output_1_loss: 2.6529e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1429 - participant_output_1_accuracy: 0.1714 - val_loss: 0.9313 - val_participant_output_loss: 0.2003 - val_command_output_loss: 0.7309 - val_command_output_1_loss: 3.9229e-05 - val_participant_output_1_loss: 4.0095e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 176ms/step - loss: 0.6787 - participant_output_loss: 0.0795 - command_output_loss: 0.5992 - command_output_1_loss: 3.9261e-05 - participant_output_1_loss: 2.5925e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0014 - participant_output_1_accuracy: 0.1700 - val_loss: 0.8595 - val_participant_output_loss: 0.1798 - val_command_output_loss: 0.6797 - val_command_output_1_loss: 3.4823e-05 - val_participant_output_1_loss: 3.9544e-05 - val_participant_output_accuracy: 0.9742 - val_command_output_accuracy: 0.9558 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1547\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 175ms/step - loss: 0.6148 - participant_output_loss: 0.0724 - command_output_loss: 0.5423 - command_output_1_loss: 3.0627e-05 - participant_output_1_loss: 2.5561e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0357 - participant_output_1_accuracy: 0.1643 - val_loss: 0.8022 - val_participant_output_loss: 0.1749 - val_command_output_loss: 0.6272 - val_command_output_1_loss: 3.0462e-05 - val_participant_output_1_loss: 4.0305e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9576 - val_command_output_1_accuracy: 0.0552 - val_participant_output_1_accuracy: 0.1326\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 172ms/step - loss: 0.5578 - participant_output_loss: 0.0658 - command_output_loss: 0.4919 - command_output_1_loss: 2.3285e-05 - participant_output_1_loss: 2.4862e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1100 - participant_output_1_accuracy: 0.1600 - val_loss: 0.7437 - val_participant_output_loss: 0.1670 - val_command_output_loss: 0.5766 - val_command_output_1_loss: 2.0992e-05 - val_participant_output_1_loss: 3.9513e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.0037 - val_participant_output_1_accuracy: 0.1400\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 227ms/step - loss: 0.5053 - participant_output_loss: 0.0609 - command_output_loss: 0.4444 - command_output_1_loss: 1.9942e-05 - participant_output_1_loss: 2.4030e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0043 - participant_output_1_accuracy: 0.1700 - val_loss: 0.7005 - val_participant_output_loss: 0.1646 - val_command_output_loss: 0.5358 - val_command_output_1_loss: 1.6641e-05 - val_participant_output_1_loss: 3.9299e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.0239 - val_participant_output_1_accuracy: 0.1326\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 339ms/step - loss: 3.5720 - participant_output_loss: 1.3880 - command_output_loss: 2.1793 - command_output_1_loss: 6.4200e-04 - participant_output_1_loss: 0.0041 - participant_output_accuracy: 0.4629 - command_output_accuracy: 0.2871 - command_output_1_accuracy: 0.0100 - participant_output_1_accuracy: 0.2557 - val_loss: 3.3709 - val_participant_output_loss: 1.3888 - val_command_output_loss: 1.9813 - val_command_output_1_loss: 1.7256e-04 - val_participant_output_1_loss: 5.9971e-04 - val_participant_output_accuracy: 0.3886 - val_command_output_accuracy: 0.5009 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2044\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 2.8965 - participant_output_loss: 1.0240 - command_output_loss: 1.8720 - command_output_1_loss: 1.7325e-04 - participant_output_1_loss: 2.6757e-04 - participant_output_accuracy: 0.6329 - command_output_accuracy: 0.6871 - command_output_1_accuracy: 0.0029 - participant_output_1_accuracy: 0.2557 - val_loss: 2.9251 - val_participant_output_loss: 1.0997 - val_command_output_loss: 1.8252 - val_command_output_1_loss: 6.0080e-05 - val_participant_output_1_loss: 1.5097e-04 - val_participant_output_accuracy: 0.6262 - val_command_output_accuracy: 0.7330 - val_command_output_1_accuracy: 0.0313 - val_participant_output_1_accuracy: 0.2505\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 2.5279 - participant_output_loss: 0.8044 - command_output_loss: 1.7233 - command_output_1_loss: 6.0578e-05 - participant_output_1_loss: 9.2395e-05 - participant_output_accuracy: 0.8043 - command_output_accuracy: 0.8529 - command_output_1_accuracy: 0.0271 - participant_output_1_accuracy: 0.2900 - val_loss: 2.5872 - val_participant_output_loss: 0.8964 - val_command_output_loss: 1.6906 - val_command_output_1_loss: 8.9691e-05 - val_participant_output_1_loss: 9.7149e-05 - val_participant_output_accuracy: 0.7808 - val_command_output_accuracy: 0.8306 - val_command_output_1_accuracy: 0.0239 - val_participant_output_1_accuracy: 0.0792\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 219ms/step - loss: 2.2390 - participant_output_loss: 0.6519 - command_output_loss: 1.5870 - command_output_1_loss: 7.6245e-05 - participant_output_1_loss: 5.4703e-05 - participant_output_accuracy: 0.8957 - command_output_accuracy: 0.9129 - command_output_1_accuracy: 0.0429 - participant_output_1_accuracy: 0.3100 - val_loss: 2.3266 - val_participant_output_loss: 0.7693 - val_command_output_loss: 1.5572 - val_command_output_1_loss: 6.7529e-05 - val_participant_output_1_loss: 4.1778e-05 - val_participant_output_accuracy: 0.8435 - val_command_output_accuracy: 0.8932 - val_command_output_1_accuracy: 0.1676 - val_participant_output_1_accuracy: 0.1289\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 2.0019 - participant_output_loss: 0.5386 - command_output_loss: 1.4632 - command_output_1_loss: 5.0401e-05 - participant_output_1_loss: 3.5173e-05 - participant_output_accuracy: 0.9271 - command_output_accuracy: 0.9414 - command_output_1_accuracy: 0.2057 - participant_output_1_accuracy: 0.1943 - val_loss: 2.0771 - val_participant_output_loss: 0.6261 - val_command_output_loss: 1.4509 - val_command_output_1_loss: 4.8408e-05 - val_participant_output_1_loss: 3.4762e-05 - val_participant_output_accuracy: 0.9171 - val_command_output_accuracy: 0.8987 - val_command_output_1_accuracy: 0.5709 - val_participant_output_1_accuracy: 0.3186\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 1.7778 - participant_output_loss: 0.4334 - command_output_loss: 1.3444 - command_output_1_loss: 6.2134e-05 - participant_output_1_loss: 2.9413e-05 - participant_output_accuracy: 0.9629 - command_output_accuracy: 0.9514 - command_output_1_accuracy: 0.6429 - participant_output_1_accuracy: 0.2029 - val_loss: 1.8809 - val_participant_output_loss: 0.5440 - val_command_output_loss: 1.3368 - val_command_output_1_loss: 4.5268e-05 - val_participant_output_1_loss: 3.1895e-05 - val_participant_output_accuracy: 0.9282 - val_command_output_accuracy: 0.9227 - val_command_output_1_accuracy: 0.4512 - val_participant_output_1_accuracy: 0.3591\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 213ms/step - loss: 1.5833 - participant_output_loss: 0.3646 - command_output_loss: 1.2187 - command_output_1_loss: 4.0911e-05 - participant_output_1_loss: 2.7181e-05 - participant_output_accuracy: 0.9800 - command_output_accuracy: 0.9771 - command_output_1_accuracy: 0.1057 - participant_output_1_accuracy: 0.2443 - val_loss: 1.6998 - val_participant_output_loss: 0.4752 - val_command_output_loss: 1.2244 - val_command_output_1_loss: 5.4866e-05 - val_participant_output_1_loss: 3.0809e-05 - val_participant_output_accuracy: 0.9466 - val_command_output_accuracy: 0.9411 - val_command_output_1_accuracy: 0.0055 - val_participant_output_1_accuracy: 0.1786\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 218ms/step - loss: 1.4170 - participant_output_loss: 0.3131 - command_output_loss: 1.1037 - command_output_1_loss: 5.1347e-05 - participant_output_1_loss: 2.5846e-05 - participant_output_accuracy: 0.9857 - command_output_accuracy: 0.9829 - command_output_1_accuracy: 0.0900 - participant_output_1_accuracy: 0.2357 - val_loss: 1.5839 - val_participant_output_loss: 0.4701 - val_command_output_loss: 1.1137 - val_command_output_1_loss: 3.8711e-05 - val_participant_output_1_loss: 3.0636e-05 - val_participant_output_accuracy: 0.9392 - val_command_output_accuracy: 0.9540 - val_command_output_1_accuracy: 0.2560 - val_participant_output_1_accuracy: 0.1584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "11/11 [==============================] - 3s 234ms/step - loss: 1.2722 - participant_output_loss: 0.2810 - command_output_loss: 0.9911 - command_output_1_loss: 5.4498e-05 - participant_output_1_loss: 2.4936e-05 - participant_output_accuracy: 0.9857 - command_output_accuracy: 0.9929 - command_output_1_accuracy: 0.1357 - participant_output_1_accuracy: 0.2086 - val_loss: 1.4135 - val_participant_output_loss: 0.3935 - val_command_output_loss: 1.0200 - val_command_output_1_loss: 4.6329e-05 - val_participant_output_1_loss: 3.0346e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9521 - val_command_output_1_accuracy: 0.0902 - val_participant_output_1_accuracy: 0.1786\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 1.1317 - participant_output_loss: 0.2454 - command_output_loss: 0.8862 - command_output_1_loss: 4.3994e-05 - participant_output_1_loss: 2.4377e-05 - participant_output_accuracy: 0.9929 - command_output_accuracy: 0.9929 - command_output_1_accuracy: 0.1100 - participant_output_1_accuracy: 0.1986 - val_loss: 1.2921 - val_participant_output_loss: 0.3605 - val_command_output_loss: 0.9316 - val_command_output_1_loss: 6.4100e-05 - val_participant_output_1_loss: 2.9955e-05 - val_participant_output_accuracy: 0.9576 - val_command_output_accuracy: 0.9503 - val_command_output_1_accuracy: 0.0755 - val_participant_output_1_accuracy: 0.2044\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 266ms/step - loss: 1.0079 - participant_output_loss: 0.2164 - command_output_loss: 0.7914 - command_output_1_loss: 5.9874e-05 - participant_output_1_loss: 2.3091e-05 - participant_output_accuracy: 0.9929 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.0814 - participant_output_1_accuracy: 0.2286 - val_loss: 1.1741 - val_participant_output_loss: 0.3336 - val_command_output_loss: 0.8404 - val_command_output_1_loss: 4.8417e-05 - val_participant_output_1_loss: 2.9709e-05 - val_participant_output_accuracy: 0.9595 - val_command_output_accuracy: 0.9595 - val_command_output_1_accuracy: 0.1013 - val_participant_output_1_accuracy: 0.1860\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 0.8901 - participant_output_loss: 0.1875 - command_output_loss: 0.7026 - command_output_1_loss: 4.1776e-05 - participant_output_1_loss: 2.2815e-05 - participant_output_accuracy: 0.9957 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.1343 - participant_output_1_accuracy: 0.1929 - val_loss: 1.0681 - val_participant_output_loss: 0.3113 - val_command_output_loss: 0.7568 - val_command_output_1_loss: 3.8489e-05 - val_participant_output_1_loss: 3.0747e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9687 - val_command_output_1_accuracy: 0.1105 - val_participant_output_1_accuracy: 0.2928\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 184ms/step - loss: 0.7892 - participant_output_loss: 0.1643 - command_output_loss: 0.6248 - command_output_1_loss: 3.8359e-05 - participant_output_1_loss: 2.2531e-05 - participant_output_accuracy: 0.9957 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0414 - participant_output_1_accuracy: 0.2200 - val_loss: 0.9831 - val_participant_output_loss: 0.2878 - val_command_output_loss: 0.6953 - val_command_output_1_loss: 4.2847e-05 - val_participant_output_1_loss: 3.0701e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.0184 - val_participant_output_1_accuracy: 0.2855\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 143ms/step - loss: 0.7023 - participant_output_loss: 0.1440 - command_output_loss: 0.5582 - command_output_1_loss: 3.5033e-05 - participant_output_1_loss: 2.1881e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1043 - participant_output_1_accuracy: 0.2143 - val_loss: 0.9062 - val_participant_output_loss: 0.2723 - val_command_output_loss: 0.6338 - val_command_output_1_loss: 3.6366e-05 - val_participant_output_1_loss: 3.0743e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9687 - val_command_output_1_accuracy: 0.1952 - val_participant_output_1_accuracy: 0.3131\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 150ms/step - loss: 0.6267 - participant_output_loss: 0.1289 - command_output_loss: 0.4977 - command_output_1_loss: 2.8991e-05 - participant_output_1_loss: 2.1260e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1429 - participant_output_1_accuracy: 0.2343 - val_loss: 0.8366 - val_participant_output_loss: 0.2538 - val_command_output_loss: 0.5827 - val_command_output_1_loss: 3.4062e-05 - val_participant_output_1_loss: 3.0710e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.0921 - val_participant_output_1_accuracy: 0.2873\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 149ms/step - loss: 0.5575 - participant_output_loss: 0.1148 - command_output_loss: 0.4426 - command_output_1_loss: 2.8994e-05 - participant_output_1_loss: 2.0817e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0429 - participant_output_1_accuracy: 0.2286 - val_loss: 0.7808 - val_participant_output_loss: 0.2425 - val_command_output_loss: 0.5383 - val_command_output_1_loss: 3.0256e-05 - val_participant_output_1_loss: 3.0984e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9595 - val_command_output_1_accuracy: 0.1473 - val_participant_output_1_accuracy: 0.2339\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 180ms/step - loss: 0.4983 - participant_output_loss: 0.1028 - command_output_loss: 0.3954 - command_output_1_loss: 2.8523e-05 - participant_output_1_loss: 2.0449e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.2914 - participant_output_1_accuracy: 0.2343 - val_loss: 0.7139 - val_participant_output_loss: 0.2218 - val_command_output_loss: 0.4920 - val_command_output_1_loss: 3.1092e-05 - val_participant_output_1_loss: 3.0787e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.4144 - val_participant_output_1_accuracy: 0.1897\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 209ms/step - loss: 0.4456 - participant_output_loss: 0.0929 - command_output_loss: 0.3527 - command_output_1_loss: 2.2381e-05 - participant_output_1_loss: 1.9097e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.2086 - participant_output_1_accuracy: 0.2243 - val_loss: 0.6561 - val_participant_output_loss: 0.2053 - val_command_output_loss: 0.4508 - val_command_output_1_loss: 2.3280e-05 - val_participant_output_1_loss: 2.9629e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9687 - val_command_output_1_accuracy: 0.0460 - val_participant_output_1_accuracy: 0.2339\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 337ms/step - loss: 3.6875 - participant_output_loss: 1.4018 - command_output_loss: 2.2826 - command_output_1_loss: 0.0017 - participant_output_1_loss: 0.0013 - participant_output_accuracy: 0.4514 - command_output_accuracy: 0.2671 - command_output_1_accuracy: 0.0900 - participant_output_1_accuracy: 0.2543 - val_loss: 3.4006 - val_participant_output_loss: 1.3133 - val_command_output_loss: 2.0869 - val_command_output_1_loss: 2.3558e-04 - val_participant_output_1_loss: 1.7674e-04 - val_participant_output_accuracy: 0.4549 - val_command_output_accuracy: 0.4659 - val_command_output_1_accuracy: 0.0166 - val_participant_output_1_accuracy: 0.1105\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 3.0214 - participant_output_loss: 1.0123 - command_output_loss: 2.0088 - command_output_1_loss: 1.4619e-04 - participant_output_1_loss: 1.8504e-04 - participant_output_accuracy: 0.6586 - command_output_accuracy: 0.6957 - command_output_1_accuracy: 0.2800 - participant_output_1_accuracy: 0.2943 - val_loss: 3.0755 - val_participant_output_loss: 1.0991 - val_command_output_loss: 1.9762 - val_command_output_1_loss: 8.8308e-05 - val_participant_output_1_loss: 1.3808e-04 - val_participant_output_accuracy: 0.6427 - val_command_output_accuracy: 0.7661 - val_command_output_1_accuracy: 0.8379 - val_participant_output_1_accuracy: 0.0424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 2.7544 - participant_output_loss: 0.8462 - command_output_loss: 1.9080 - command_output_1_loss: 6.8000e-05 - participant_output_1_loss: 8.8078e-05 - participant_output_accuracy: 0.7671 - command_output_accuracy: 0.8586 - command_output_1_accuracy: 0.5186 - participant_output_1_accuracy: 0.2300 - val_loss: 2.8386 - val_participant_output_loss: 0.9526 - val_command_output_loss: 1.8859 - val_command_output_1_loss: 5.2040e-05 - val_participant_output_1_loss: 7.3137e-05 - val_participant_output_accuracy: 0.7090 - val_command_output_accuracy: 0.7937 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1786\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 2.4859 - participant_output_loss: 0.6770 - command_output_loss: 1.8088 - command_output_1_loss: 6.7775e-05 - participant_output_1_loss: 5.4512e-05 - participant_output_accuracy: 0.8700 - command_output_accuracy: 0.8800 - command_output_1_accuracy: 0.0071 - participant_output_1_accuracy: 0.1757 - val_loss: 2.5699 - val_participant_output_loss: 0.7759 - val_command_output_loss: 1.7940 - val_command_output_1_loss: 6.2848e-05 - val_participant_output_1_loss: 5.2398e-05 - val_participant_output_accuracy: 0.8435 - val_command_output_accuracy: 0.8692 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1786\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 2.2569 - participant_output_loss: 0.5469 - command_output_loss: 1.7099 - command_output_1_loss: 6.1584e-05 - participant_output_1_loss: 4.2723e-05 - participant_output_accuracy: 0.9243 - command_output_accuracy: 0.9300 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1714 - val_loss: 2.3478 - val_participant_output_loss: 0.6459 - val_command_output_loss: 1.7018 - val_command_output_1_loss: 6.5776e-05 - val_participant_output_1_loss: 4.7238e-05 - val_participant_output_accuracy: 0.9116 - val_command_output_accuracy: 0.8913 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1381\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 216ms/step - loss: 2.0566 - participant_output_loss: 0.4447 - command_output_loss: 1.6117 - command_output_1_loss: 6.4773e-05 - participant_output_1_loss: 3.7762e-05 - participant_output_accuracy: 0.9671 - command_output_accuracy: 0.9629 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2071 - val_loss: 2.1500 - val_participant_output_loss: 0.5460 - val_command_output_loss: 1.6039 - val_command_output_1_loss: 4.8276e-05 - val_participant_output_1_loss: 4.0136e-05 - val_participant_output_accuracy: 0.9466 - val_command_output_accuracy: 0.9208 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1547\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 209ms/step - loss: 1.8710 - participant_output_loss: 0.3593 - command_output_loss: 1.5117 - command_output_1_loss: 4.8013e-05 - participant_output_1_loss: 3.1408e-05 - participant_output_accuracy: 0.9771 - command_output_accuracy: 0.9729 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1857 - val_loss: 1.9768 - val_participant_output_loss: 0.4622 - val_command_output_loss: 1.5145 - val_command_output_1_loss: 3.8117e-05 - val_participant_output_1_loss: 3.8161e-05 - val_participant_output_accuracy: 0.9448 - val_command_output_accuracy: 0.9282 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.2818\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 211ms/step - loss: 1.7020 - participant_output_loss: 0.2870 - command_output_loss: 1.4150 - command_output_1_loss: 4.0624e-05 - participant_output_1_loss: 2.7833e-05 - participant_output_accuracy: 0.9857 - command_output_accuracy: 0.9843 - command_output_1_accuracy: 0.1957 - participant_output_1_accuracy: 0.2171 - val_loss: 1.8367 - val_participant_output_loss: 0.4000 - val_command_output_loss: 1.4366 - val_command_output_1_loss: 3.9728e-05 - val_participant_output_1_loss: 3.7536e-05 - val_participant_output_accuracy: 0.9540 - val_command_output_accuracy: 0.9319 - val_command_output_1_accuracy: 0.4549 - val_participant_output_1_accuracy: 0.1234\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 208ms/step - loss: 1.5526 - participant_output_loss: 0.2344 - command_output_loss: 1.3182 - command_output_1_loss: 3.1189e-05 - participant_output_1_loss: 2.6843e-05 - participant_output_accuracy: 0.9886 - command_output_accuracy: 0.9886 - command_output_1_accuracy: 0.3257 - participant_output_1_accuracy: 0.1471 - val_loss: 1.7013 - val_participant_output_loss: 0.3533 - val_command_output_loss: 1.3479 - val_command_output_1_loss: 4.1551e-05 - val_participant_output_1_loss: 3.4531e-05 - val_participant_output_accuracy: 0.9595 - val_command_output_accuracy: 0.9411 - val_command_output_1_accuracy: 0.0166 - val_participant_output_1_accuracy: 0.1694\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.4263 - participant_output_loss: 0.1967 - command_output_loss: 1.2295 - command_output_1_loss: 4.7167e-05 - participant_output_1_loss: 2.4322e-05 - participant_output_accuracy: 0.9900 - command_output_accuracy: 0.9929 - command_output_1_accuracy: 0.0029 - participant_output_1_accuracy: 0.2057 - val_loss: 1.5613 - val_participant_output_loss: 0.2978 - val_command_output_loss: 1.2634 - val_command_output_1_loss: 3.3284e-05 - val_participant_output_1_loss: 3.4693e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9448 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1750\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 1.3039 - participant_output_loss: 0.1629 - command_output_loss: 1.1409 - command_output_1_loss: 7.4119e-05 - participant_output_1_loss: 2.2912e-05 - participant_output_accuracy: 0.9929 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.1386 - participant_output_1_accuracy: 0.2029 - val_loss: 1.4535 - val_participant_output_loss: 0.2646 - val_command_output_loss: 1.1888 - val_command_output_1_loss: 7.0711e-05 - val_participant_output_1_loss: 3.4731e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9466 - val_command_output_1_accuracy: 0.4936 - val_participant_output_1_accuracy: 0.1271\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 1.1941 - participant_output_loss: 0.1379 - command_output_loss: 1.0561 - command_output_1_loss: 5.7832e-05 - participant_output_1_loss: 2.2198e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.2329 - participant_output_1_accuracy: 0.1843 - val_loss: 1.3464 - val_participant_output_loss: 0.2383 - val_command_output_loss: 1.1080 - val_command_output_1_loss: 7.5783e-05 - val_participant_output_1_loss: 3.4334e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9466 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1897\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 1.0909 - participant_output_loss: 0.1176 - command_output_loss: 0.9732 - command_output_1_loss: 7.4532e-05 - participant_output_1_loss: 2.1275e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1757 - val_loss: 1.2495 - val_participant_output_loss: 0.2140 - val_command_output_loss: 1.0355 - val_command_output_1_loss: 4.2175e-05 - val_participant_output_1_loss: 3.2531e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9576 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1989\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 147ms/step - loss: 0.9995 - participant_output_loss: 0.1027 - command_output_loss: 0.8968 - command_output_1_loss: 4.6614e-05 - participant_output_1_loss: 2.0211e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.0757 - participant_output_1_accuracy: 0.1771 - val_loss: 1.1715 - val_participant_output_loss: 0.1988 - val_command_output_loss: 0.9726 - val_command_output_1_loss: 5.5433e-05 - val_participant_output_1_loss: 3.2732e-05 - val_participant_output_accuracy: 0.9724 - val_command_output_accuracy: 0.9484 - val_command_output_1_accuracy: 0.2560 - val_participant_output_1_accuracy: 0.2578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.9176 - participant_output_loss: 0.0901 - command_output_loss: 0.8274 - command_output_1_loss: 6.3797e-05 - participant_output_1_loss: 1.9279e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.5829 - participant_output_1_accuracy: 0.1843 - val_loss: 1.1012 - val_participant_output_loss: 0.1913 - val_command_output_loss: 0.9099 - val_command_output_1_loss: 5.3091e-05 - val_participant_output_1_loss: 3.2115e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9558 - val_command_output_1_accuracy: 0.4328 - val_participant_output_1_accuracy: 0.1952\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 173ms/step - loss: 0.8414 - participant_output_loss: 0.0792 - command_output_loss: 0.7622 - command_output_1_loss: 5.0380e-05 - participant_output_1_loss: 1.8313e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0571 - participant_output_1_accuracy: 0.2129 - val_loss: 1.0502 - val_participant_output_loss: 0.1925 - val_command_output_loss: 0.8576 - val_command_output_1_loss: 4.8891e-05 - val_participant_output_1_loss: 3.1963e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9558 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1326\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 203ms/step - loss: 0.7719 - participant_output_loss: 0.0712 - command_output_loss: 0.7006 - command_output_1_loss: 3.9195e-05 - participant_output_1_loss: 1.7096e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0086 - participant_output_1_accuracy: 0.2129 - val_loss: 0.9719 - val_participant_output_loss: 0.1727 - val_command_output_loss: 0.7991 - val_command_output_1_loss: 3.1964e-05 - val_participant_output_1_loss: 3.0776e-05 - val_participant_output_accuracy: 0.9724 - val_command_output_accuracy: 0.9576 - val_command_output_1_accuracy: 0.0847 - val_participant_output_1_accuracy: 0.2063\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 0.7091 - participant_output_loss: 0.0656 - command_output_loss: 0.6434 - command_output_1_loss: 3.2225e-05 - participant_output_1_loss: 1.6155e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0614 - participant_output_1_accuracy: 0.1671 - val_loss: 0.9145 - val_participant_output_loss: 0.1673 - val_command_output_loss: 0.7472 - val_command_output_1_loss: 3.7776e-05 - val_participant_output_1_loss: 3.0449e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.0110 - val_participant_output_1_accuracy: 0.2302\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 178ms/step - loss: 0.6515 - participant_output_loss: 0.0605 - command_output_loss: 0.5910 - command_output_1_loss: 3.6319e-05 - participant_output_1_loss: 1.5290e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0800 - participant_output_1_accuracy: 0.2200 - val_loss: 0.8672 - val_participant_output_loss: 0.1674 - val_command_output_loss: 0.6997 - val_command_output_1_loss: 3.4084e-05 - val_participant_output_1_loss: 2.9735e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9558 - val_command_output_1_accuracy: 0.1418 - val_participant_output_1_accuracy: 0.1602\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 170ms/step - loss: 0.6003 - participant_output_loss: 0.0556 - command_output_loss: 0.5446 - command_output_1_loss: 3.3523e-05 - participant_output_1_loss: 1.4637e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0529 - participant_output_1_accuracy: 0.2157 - val_loss: 0.8151 - val_participant_output_loss: 0.1569 - val_command_output_loss: 0.6582 - val_command_output_1_loss: 2.6609e-05 - val_participant_output_1_loss: 2.9285e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9558 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2320\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 350ms/step - loss: 3.6009 - participant_output_loss: 1.4320 - command_output_loss: 2.1569 - command_output_1_loss: 6.6666e-04 - participant_output_1_loss: 0.0113 - participant_output_accuracy: 0.4457 - command_output_accuracy: 0.3343 - command_output_1_accuracy: 0.0271 - participant_output_1_accuracy: 0.2843 - val_loss: 3.3597 - val_participant_output_loss: 1.3930 - val_command_output_loss: 1.9658 - val_command_output_1_loss: 1.2565e-04 - val_participant_output_1_loss: 8.4514e-04 - val_participant_output_accuracy: 0.3112 - val_command_output_accuracy: 0.5691 - val_command_output_1_accuracy: 0.0810 - val_participant_output_1_accuracy: 0.0018\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 199ms/step - loss: 2.9605 - participant_output_loss: 1.0860 - command_output_loss: 1.8741 - command_output_1_loss: 1.3392e-04 - participant_output_1_loss: 3.3138e-04 - participant_output_accuracy: 0.5657 - command_output_accuracy: 0.7000 - command_output_1_accuracy: 0.0143 - participant_output_1_accuracy: 0.4457 - val_loss: 2.9827 - val_participant_output_loss: 1.1634 - val_command_output_loss: 1.8190 - val_command_output_1_loss: 9.4971e-05 - val_participant_output_1_loss: 1.5741e-04 - val_participant_output_accuracy: 0.6077 - val_command_output_accuracy: 0.7422 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0884\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 2.6421 - participant_output_loss: 0.9105 - command_output_loss: 1.7314 - command_output_1_loss: 8.1776e-05 - participant_output_1_loss: 1.1387e-04 - participant_output_accuracy: 0.7400 - command_output_accuracy: 0.8300 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.0357 - val_loss: 2.6876 - val_participant_output_loss: 1.0078 - val_command_output_loss: 1.6797 - val_command_output_1_loss: 6.5387e-05 - val_participant_output_1_loss: 7.6712e-05 - val_participant_output_accuracy: 0.7293 - val_command_output_accuracy: 0.8361 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0166\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 2.3562 - participant_output_loss: 0.7534 - command_output_loss: 1.6026 - command_output_1_loss: 8.2508e-05 - participant_output_1_loss: 6.0559e-05 - participant_output_accuracy: 0.8529 - command_output_accuracy: 0.9057 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.4171 - val_loss: 2.4334 - val_participant_output_loss: 0.8626 - val_command_output_loss: 1.5707 - val_command_output_1_loss: 8.6516e-05 - val_participant_output_1_loss: 5.0393e-05 - val_participant_output_accuracy: 0.7882 - val_command_output_accuracy: 0.8840 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.4807\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 2.0801 - participant_output_loss: 0.5967 - command_output_loss: 1.4833 - command_output_1_loss: 6.8898e-05 - participant_output_1_loss: 4.1598e-05 - participant_output_accuracy: 0.8971 - command_output_accuracy: 0.9429 - command_output_1_accuracy: 0.0057 - participant_output_1_accuracy: 0.2286 - val_loss: 2.1521 - val_participant_output_loss: 0.6846 - val_command_output_loss: 1.4674 - val_command_output_1_loss: 4.8315e-05 - val_participant_output_1_loss: 3.6361e-05 - val_participant_output_accuracy: 0.8821 - val_command_output_accuracy: 0.8877 - val_command_output_1_accuracy: 0.1492 - val_participant_output_1_accuracy: 0.1565\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 216ms/step - loss: 1.8479 - participant_output_loss: 0.4807 - command_output_loss: 1.3671 - command_output_1_loss: 5.5751e-05 - participant_output_1_loss: 3.3221e-05 - participant_output_accuracy: 0.9286 - command_output_accuracy: 0.9614 - command_output_1_accuracy: 0.5829 - participant_output_1_accuracy: 0.2129 - val_loss: 1.9373 - val_participant_output_loss: 0.5858 - val_command_output_loss: 1.3514 - val_command_output_1_loss: 5.6055e-05 - val_participant_output_1_loss: 3.6119e-05 - val_participant_output_accuracy: 0.8932 - val_command_output_accuracy: 0.9079 - val_command_output_1_accuracy: 0.4899 - val_participant_output_1_accuracy: 0.2136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 1.6173 - participant_output_loss: 0.3725 - command_output_loss: 1.2447 - command_output_1_loss: 4.9637e-05 - participant_output_1_loss: 2.9717e-05 - participant_output_accuracy: 0.9657 - command_output_accuracy: 0.9814 - command_output_1_accuracy: 0.1229 - participant_output_1_accuracy: 0.2443 - val_loss: 1.7508 - val_participant_output_loss: 0.5093 - val_command_output_loss: 1.2414 - val_command_output_1_loss: 6.9033e-05 - val_participant_output_1_loss: 3.0465e-05 - val_participant_output_accuracy: 0.9190 - val_command_output_accuracy: 0.9282 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1418\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 210ms/step - loss: 1.4320 - participant_output_loss: 0.3037 - command_output_loss: 1.1282 - command_output_1_loss: 6.5752e-05 - participant_output_1_loss: 2.7370e-05 - participant_output_accuracy: 0.9800 - command_output_accuracy: 0.9871 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1729 - val_loss: 1.5907 - val_participant_output_loss: 0.4518 - val_command_output_loss: 1.1388 - val_command_output_1_loss: 4.7354e-05 - val_participant_output_1_loss: 2.9596e-05 - val_participant_output_accuracy: 0.9392 - val_command_output_accuracy: 0.9374 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1602\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 210ms/step - loss: 1.2789 - participant_output_loss: 0.2571 - command_output_loss: 1.0217 - command_output_1_loss: 5.7143e-05 - participant_output_1_loss: 2.7362e-05 - participant_output_accuracy: 0.9900 - command_output_accuracy: 0.9929 - command_output_1_accuracy: 0.1000 - participant_output_1_accuracy: 0.2400 - val_loss: 1.4357 - val_participant_output_loss: 0.3886 - val_command_output_loss: 1.0470 - val_command_output_1_loss: 5.2792e-05 - val_participant_output_1_loss: 2.8853e-05 - val_participant_output_accuracy: 0.9448 - val_command_output_accuracy: 0.9411 - val_command_output_1_accuracy: 0.3886 - val_participant_output_1_accuracy: 0.1584\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 205ms/step - loss: 1.1402 - participant_output_loss: 0.2198 - command_output_loss: 0.9203 - command_output_1_loss: 5.1245e-05 - participant_output_1_loss: 2.6062e-05 - participant_output_accuracy: 0.9900 - command_output_accuracy: 0.9886 - command_output_1_accuracy: 0.4100 - participant_output_1_accuracy: 0.1929 - val_loss: 1.3008 - val_participant_output_loss: 0.3392 - val_command_output_loss: 0.9615 - val_command_output_1_loss: 5.3956e-05 - val_participant_output_1_loss: 2.7929e-05 - val_participant_output_accuracy: 0.9576 - val_command_output_accuracy: 0.9429 - val_command_output_1_accuracy: 0.1105 - val_participant_output_1_accuracy: 0.1713\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 1.0174 - participant_output_loss: 0.1879 - command_output_loss: 0.8294 - command_output_1_loss: 5.4624e-05 - participant_output_1_loss: 2.5766e-05 - participant_output_accuracy: 0.9986 - command_output_accuracy: 0.9929 - command_output_1_accuracy: 0.0300 - participant_output_1_accuracy: 0.2114 - val_loss: 1.1998 - val_participant_output_loss: 0.3236 - val_command_output_loss: 0.8761 - val_command_output_1_loss: 5.2414e-05 - val_participant_output_1_loss: 2.7512e-05 - val_participant_output_accuracy: 0.9540 - val_command_output_accuracy: 0.9558 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1971\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 0.9036 - participant_output_loss: 0.1616 - command_output_loss: 0.7420 - command_output_1_loss: 5.5494e-05 - participant_output_1_loss: 2.4328e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.0029 - participant_output_1_accuracy: 0.1843 - val_loss: 1.0757 - val_participant_output_loss: 0.2764 - val_command_output_loss: 0.7992 - val_command_output_1_loss: 5.0045e-05 - val_participant_output_1_loss: 2.7257e-05 - val_participant_output_accuracy: 0.9613 - val_command_output_accuracy: 0.9558 - val_command_output_1_accuracy: 0.0184 - val_participant_output_1_accuracy: 0.1768\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 0.8031 - participant_output_loss: 0.1379 - command_output_loss: 0.6651 - command_output_1_loss: 3.6384e-05 - participant_output_1_loss: 2.3968e-05 - participant_output_accuracy: 0.9986 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.0843 - participant_output_1_accuracy: 0.2029 - val_loss: 0.9837 - val_participant_output_loss: 0.2532 - val_command_output_loss: 0.7304 - val_command_output_1_loss: 3.4138e-05 - val_participant_output_1_loss: 2.6718e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.1381 - val_participant_output_1_accuracy: 0.1731\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 168ms/step - loss: 0.7193 - participant_output_loss: 0.1224 - command_output_loss: 0.5969 - command_output_1_loss: 4.1476e-05 - participant_output_1_loss: 2.3113e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.2457 - participant_output_1_accuracy: 0.1857 - val_loss: 0.9125 - val_participant_output_loss: 0.2396 - val_command_output_loss: 0.6728 - val_command_output_1_loss: 3.8868e-05 - val_participant_output_1_loss: 2.6324e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.3517 - val_participant_output_1_accuracy: 0.1786\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 213ms/step - loss: 0.6430 - participant_output_loss: 0.1090 - command_output_loss: 0.5339 - command_output_1_loss: 3.0232e-05 - participant_output_1_loss: 2.2490e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.1214 - participant_output_1_accuracy: 0.2000 - val_loss: 0.8471 - val_participant_output_loss: 0.2300 - val_command_output_loss: 0.6170 - val_command_output_1_loss: 3.4543e-05 - val_participant_output_1_loss: 2.5733e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.0184 - val_participant_output_1_accuracy: 0.1400\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 173ms/step - loss: 0.5807 - participant_output_loss: 0.0990 - command_output_loss: 0.4816 - command_output_1_loss: 2.7982e-05 - participant_output_1_loss: 2.1770e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0143 - participant_output_1_accuracy: 0.1729 - val_loss: 0.7821 - val_participant_output_loss: 0.2140 - val_command_output_loss: 0.5680 - val_command_output_1_loss: 2.5796e-05 - val_participant_output_1_loss: 2.5127e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.0295 - val_participant_output_1_accuracy: 0.1823\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 203ms/step - loss: 0.5238 - participant_output_loss: 0.0902 - command_output_loss: 0.4335 - command_output_1_loss: 3.0309e-05 - participant_output_1_loss: 2.0926e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0343 - participant_output_1_accuracy: 0.1957 - val_loss: 0.7287 - val_participant_output_loss: 0.2036 - val_command_output_loss: 0.5251 - val_command_output_1_loss: 3.4356e-05 - val_participant_output_1_loss: 2.4569e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.0700 - val_participant_output_1_accuracy: 0.1492\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 174ms/step - loss: 0.4735 - participant_output_loss: 0.0829 - command_output_loss: 0.3906 - command_output_1_loss: 2.6617e-05 - participant_output_1_loss: 2.0215e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1657 - participant_output_1_accuracy: 0.1771 - val_loss: 0.6905 - val_participant_output_loss: 0.2050 - val_command_output_loss: 0.4855 - val_command_output_1_loss: 2.6973e-05 - val_participant_output_1_loss: 2.3817e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.0516 - val_participant_output_1_accuracy: 0.1786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 211ms/step - loss: 0.4262 - participant_output_loss: 0.0769 - command_output_loss: 0.3492 - command_output_1_loss: 2.5315e-05 - participant_output_1_loss: 1.9473e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0229 - participant_output_1_accuracy: 0.1857 - val_loss: 0.6431 - val_participant_output_loss: 0.1921 - val_command_output_loss: 0.4510 - val_command_output_1_loss: 2.0039e-05 - val_participant_output_1_loss: 2.3369e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.0147 - val_participant_output_1_accuracy: 0.1842\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 0.3880 - participant_output_loss: 0.0721 - command_output_loss: 0.3158 - command_output_1_loss: 2.5634e-05 - participant_output_1_loss: 1.8878e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0086 - participant_output_1_accuracy: 0.1786 - val_loss: 0.6029 - val_participant_output_loss: 0.1819 - val_command_output_loss: 0.4209 - val_command_output_1_loss: 3.1986e-05 - val_participant_output_1_loss: 2.3028e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.0037 - val_participant_output_1_accuracy: 0.1786\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 345ms/step - loss: 3.5019 - participant_output_loss: 1.2608 - command_output_loss: 2.2332 - command_output_1_loss: 0.0016 - participant_output_1_loss: 0.0063 - participant_output_accuracy: 0.5214 - command_output_accuracy: 0.3329 - command_output_1_accuracy: 0.0143 - participant_output_1_accuracy: 0.3557 - val_loss: 3.1857 - val_participant_output_loss: 1.1115 - val_command_output_loss: 2.0733 - val_command_output_1_loss: 1.6152e-04 - val_participant_output_1_loss: 7.0266e-04 - val_participant_output_accuracy: 0.6427 - val_command_output_accuracy: 0.5967 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.8306\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 2.8236 - participant_output_loss: 0.8307 - command_output_loss: 1.9923 - command_output_1_loss: 1.7028e-04 - participant_output_1_loss: 4.2719e-04 - participant_output_accuracy: 0.7614 - command_output_accuracy: 0.6643 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2029 - val_loss: 2.7945 - val_participant_output_loss: 0.8567 - val_command_output_loss: 1.9375 - val_command_output_1_loss: 2.1558e-04 - val_participant_output_1_loss: 9.2550e-05 - val_participant_output_accuracy: 0.8214 - val_command_output_accuracy: 0.6575 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0147\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 204ms/step - loss: 2.5140 - participant_output_loss: 0.6417 - command_output_loss: 1.8720 - command_output_1_loss: 1.9445e-04 - participant_output_1_loss: 9.9672e-05 - participant_output_accuracy: 0.8786 - command_output_accuracy: 0.8043 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.0743 - val_loss: 2.6013 - val_participant_output_loss: 0.7617 - val_command_output_loss: 1.8394 - val_command_output_1_loss: 1.2304e-04 - val_participant_output_1_loss: 3.7563e-05 - val_participant_output_accuracy: 0.8122 - val_command_output_accuracy: 0.8250 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1142\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 208ms/step - loss: 2.2589 - participant_output_loss: 0.4864 - command_output_loss: 1.7724 - command_output_1_loss: 8.7000e-05 - participant_output_1_loss: 4.1296e-05 - participant_output_accuracy: 0.9300 - command_output_accuracy: 0.8671 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1686 - val_loss: 2.3245 - val_participant_output_loss: 0.5716 - val_command_output_loss: 1.7528 - val_command_output_1_loss: 5.9317e-05 - val_participant_output_1_loss: 3.0083e-05 - val_participant_output_accuracy: 0.9208 - val_command_output_accuracy: 0.8158 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2357\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 212ms/step - loss: 2.0372 - participant_output_loss: 0.3620 - command_output_loss: 1.6751 - command_output_1_loss: 5.9564e-05 - participant_output_1_loss: 2.6601e-05 - participant_output_accuracy: 0.9714 - command_output_accuracy: 0.8543 - command_output_1_accuracy: 0.3057 - participant_output_1_accuracy: 0.2043 - val_loss: 2.1278 - val_participant_output_loss: 0.4677 - val_command_output_loss: 1.6600 - val_command_output_1_loss: 5.3288e-05 - val_participant_output_1_loss: 3.0869e-05 - val_participant_output_accuracy: 0.9282 - val_command_output_accuracy: 0.8379 - val_command_output_1_accuracy: 0.7330 - val_participant_output_1_accuracy: 0.1013\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 1.8735 - participant_output_loss: 0.2980 - command_output_loss: 1.5755 - command_output_1_loss: 4.8712e-05 - participant_output_1_loss: 2.2649e-05 - participant_output_accuracy: 0.9771 - command_output_accuracy: 0.9129 - command_output_1_accuracy: 0.1443 - participant_output_1_accuracy: 0.1914 - val_loss: 1.9664 - val_participant_output_loss: 0.3997 - val_command_output_loss: 1.5666 - val_command_output_1_loss: 4.1904e-05 - val_participant_output_1_loss: 2.3720e-05 - val_participant_output_accuracy: 0.9521 - val_command_output_accuracy: 0.9079 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2560\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 1.7142 - participant_output_loss: 0.2372 - command_output_loss: 1.4770 - command_output_1_loss: 3.7862e-05 - participant_output_1_loss: 2.0341e-05 - participant_output_accuracy: 0.9871 - command_output_accuracy: 0.9529 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2557 - val_loss: 1.8296 - val_participant_output_loss: 0.3483 - val_command_output_loss: 1.4812 - val_command_output_1_loss: 5.1043e-05 - val_participant_output_1_loss: 2.2896e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9190 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1289\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 210ms/step - loss: 1.5824 - participant_output_loss: 0.1980 - command_output_loss: 1.3843 - command_output_1_loss: 5.1524e-05 - participant_output_1_loss: 1.8925e-05 - participant_output_accuracy: 0.9914 - command_output_accuracy: 0.9657 - command_output_1_accuracy: 0.0100 - participant_output_1_accuracy: 0.1786 - val_loss: 1.7215 - val_participant_output_loss: 0.3222 - val_command_output_loss: 1.3993 - val_command_output_1_loss: 3.5942e-05 - val_participant_output_1_loss: 2.1697e-05 - val_participant_output_accuracy: 0.9576 - val_command_output_accuracy: 0.9466 - val_command_output_1_accuracy: 0.1179 - val_participant_output_1_accuracy: 0.2007\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 176ms/step - loss: 1.4545 - participant_output_loss: 0.1672 - command_output_loss: 1.2873 - command_output_1_loss: 4.6229e-05 - participant_output_1_loss: 1.8570e-05 - participant_output_accuracy: 0.9929 - command_output_accuracy: 0.9886 - command_output_1_accuracy: 0.4271 - participant_output_1_accuracy: 0.2543 - val_loss: 1.6251 - val_participant_output_loss: 0.3154 - val_command_output_loss: 1.3096 - val_command_output_1_loss: 6.3409e-05 - val_participant_output_1_loss: 2.1671e-05 - val_participant_output_accuracy: 0.9319 - val_command_output_accuracy: 0.9411 - val_command_output_1_accuracy: 0.1123 - val_participant_output_1_accuracy: 0.1326\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 1.3418 - participant_output_loss: 0.1454 - command_output_loss: 1.1964 - command_output_1_loss: 5.5196e-05 - participant_output_1_loss: 1.8062e-05 - participant_output_accuracy: 0.9957 - command_output_accuracy: 0.9929 - command_output_1_accuracy: 0.1043 - participant_output_1_accuracy: 0.2343 - val_loss: 1.5006 - val_participant_output_loss: 0.2698 - val_command_output_loss: 1.2307 - val_command_output_1_loss: 4.2041e-05 - val_participant_output_1_loss: 2.1356e-05 - val_participant_output_accuracy: 0.9558 - val_command_output_accuracy: 0.9411 - val_command_output_1_accuracy: 0.0626 - val_participant_output_1_accuracy: 0.1639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 263ms/step - loss: 1.2238 - participant_output_loss: 0.1192 - command_output_loss: 1.1045 - command_output_1_loss: 4.6217e-05 - participant_output_1_loss: 1.7992e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9943 - command_output_1_accuracy: 0.1600 - participant_output_1_accuracy: 0.2357 - val_loss: 1.3773 - val_participant_output_loss: 0.2333 - val_command_output_loss: 1.1439 - val_command_output_1_loss: 4.1275e-05 - val_participant_output_1_loss: 2.1966e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9503 - val_command_output_1_accuracy: 0.1271 - val_participant_output_1_accuracy: 0.1934\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 1.1160 - participant_output_loss: 0.1019 - command_output_loss: 1.0141 - command_output_1_loss: 4.4013e-05 - participant_output_1_loss: 1.7686e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.0157 - participant_output_1_accuracy: 0.2329 - val_loss: 1.3041 - val_participant_output_loss: 0.2373 - val_command_output_loss: 1.0668 - val_command_output_1_loss: 3.8698e-05 - val_participant_output_1_loss: 2.1847e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9558 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1713\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 159ms/step - loss: 1.0186 - participant_output_loss: 0.0907 - command_output_loss: 0.9278 - command_output_1_loss: 4.6048e-05 - participant_output_1_loss: 1.7991e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.0014 - participant_output_1_accuracy: 0.2186 - val_loss: 1.2093 - val_participant_output_loss: 0.2221 - val_command_output_loss: 0.9872 - val_command_output_1_loss: 5.8307e-05 - val_participant_output_1_loss: 2.2395e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9503 - val_command_output_1_accuracy: 0.0497 - val_participant_output_1_accuracy: 0.1657\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 161ms/step - loss: 0.9262 - participant_output_loss: 0.0781 - command_output_loss: 0.8480 - command_output_1_loss: 6.1557e-05 - participant_output_1_loss: 1.7740e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.6157 - participant_output_1_accuracy: 0.2257 - val_loss: 1.1066 - val_participant_output_loss: 0.1926 - val_command_output_loss: 0.9139 - val_command_output_1_loss: 5.1829e-05 - val_participant_output_1_loss: 2.2580e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9540 - val_command_output_1_accuracy: 0.7606 - val_participant_output_1_accuracy: 0.1510\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 3s 231ms/step - loss: 0.8422 - participant_output_loss: 0.0698 - command_output_loss: 0.7723 - command_output_1_loss: 4.9560e-05 - participant_output_1_loss: 1.7054e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.2429 - participant_output_1_accuracy: 0.2514 - val_loss: 1.0249 - val_participant_output_loss: 0.1780 - val_command_output_loss: 0.8469 - val_command_output_1_loss: 2.9851e-05 - val_participant_output_1_loss: 2.2342e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9558 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1657\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 203ms/step - loss: 0.7678 - participant_output_loss: 0.0621 - command_output_loss: 0.7057 - command_output_1_loss: 2.9347e-05 - participant_output_1_loss: 1.6291e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2029 - val_loss: 0.9568 - val_participant_output_loss: 0.1685 - val_command_output_loss: 0.7882 - val_command_output_1_loss: 3.1604e-05 - val_participant_output_1_loss: 2.2024e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9595 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2099\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 0.6982 - participant_output_loss: 0.0564 - command_output_loss: 0.6418 - command_output_1_loss: 2.5742e-05 - participant_output_1_loss: 1.5849e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1957 - participant_output_1_accuracy: 0.2343 - val_loss: 0.8873 - val_participant_output_loss: 0.1565 - val_command_output_loss: 0.7308 - val_command_output_1_loss: 2.6402e-05 - val_participant_output_1_loss: 2.1753e-05 - val_participant_output_accuracy: 0.9724 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.6851 - val_participant_output_1_accuracy: 0.2192\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 176ms/step - loss: 0.6360 - participant_output_loss: 0.0511 - command_output_loss: 0.5849 - command_output_1_loss: 2.4581e-05 - participant_output_1_loss: 1.5190e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.2814 - participant_output_1_accuracy: 0.2229 - val_loss: 0.8305 - val_participant_output_loss: 0.1549 - val_command_output_loss: 0.6755 - val_command_output_1_loss: 2.5544e-05 - val_participant_output_1_loss: 2.1285e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9576 - val_command_output_1_accuracy: 0.0350 - val_participant_output_1_accuracy: 0.1897\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 185ms/step - loss: 0.5794 - participant_output_loss: 0.0463 - command_output_loss: 0.5331 - command_output_1_loss: 1.8012e-05 - participant_output_1_loss: 1.4731e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0486 - participant_output_1_accuracy: 0.2343 - val_loss: 0.7791 - val_participant_output_loss: 0.1471 - val_command_output_loss: 0.6320 - val_command_output_1_loss: 1.9955e-05 - val_participant_output_1_loss: 2.1333e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1731\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 171ms/step - loss: 0.5286 - participant_output_loss: 0.0422 - command_output_loss: 0.4864 - command_output_1_loss: 2.3148e-05 - participant_output_1_loss: 1.4099e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0014 - participant_output_1_accuracy: 0.2229 - val_loss: 0.7344 - val_participant_output_loss: 0.1439 - val_command_output_loss: 0.5904 - val_command_output_1_loss: 2.0941e-05 - val_participant_output_1_loss: 2.0270e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1823\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 349ms/step - loss: 3.5884 - participant_output_loss: 1.2720 - command_output_loss: 2.3054 - command_output_1_loss: 0.0023 - participant_output_1_loss: 0.0087 - participant_output_accuracy: 0.4943 - command_output_accuracy: 0.2129 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.0914 - val_loss: 3.3263 - val_participant_output_loss: 1.1811 - val_command_output_loss: 2.1445 - val_command_output_1_loss: 2.9748e-04 - val_participant_output_1_loss: 4.1100e-04 - val_participant_output_accuracy: 0.5212 - val_command_output_accuracy: 0.5028 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0092\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 2.9455 - participant_output_loss: 0.8595 - command_output_loss: 2.0854 - command_output_1_loss: 3.8311e-04 - participant_output_1_loss: 2.3488e-04 - participant_output_accuracy: 0.7500 - command_output_accuracy: 0.6143 - command_output_1_accuracy: 0.0929 - participant_output_1_accuracy: 0.2300 - val_loss: 2.9973 - val_participant_output_loss: 0.9400 - val_command_output_loss: 2.0568 - val_command_output_1_loss: 3.7349e-04 - val_participant_output_1_loss: 1.4796e-04 - val_participant_output_accuracy: 0.7459 - val_command_output_accuracy: 0.5838 - val_command_output_1_accuracy: 0.0718 - val_participant_output_1_accuracy: 0.6114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 2.6656 - participant_output_loss: 0.6572 - command_output_loss: 2.0080 - command_output_1_loss: 2.4832e-04 - participant_output_1_loss: 8.7696e-05 - participant_output_accuracy: 0.8714 - command_output_accuracy: 0.7629 - command_output_1_accuracy: 0.0043 - participant_output_1_accuracy: 0.3000 - val_loss: 2.7103 - val_participant_output_loss: 0.7225 - val_command_output_loss: 1.9876 - val_command_output_1_loss: 1.0494e-04 - val_participant_output_1_loss: 8.8213e-05 - val_participant_output_accuracy: 0.8821 - val_command_output_accuracy: 0.7219 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0295\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 183ms/step - loss: 2.4362 - participant_output_loss: 0.4965 - command_output_loss: 1.9396 - command_output_1_loss: 9.4871e-05 - participant_output_1_loss: 5.7597e-05 - participant_output_accuracy: 0.9271 - command_output_accuracy: 0.8400 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1671 - val_loss: 2.5018 - val_participant_output_loss: 0.5867 - val_command_output_loss: 1.9150 - val_command_output_1_loss: 7.8361e-05 - val_participant_output_1_loss: 5.8582e-05 - val_participant_output_accuracy: 0.8932 - val_command_output_accuracy: 0.8527 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.3260\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 2.2461 - participant_output_loss: 0.3819 - command_output_loss: 1.8641 - command_output_1_loss: 4.6589e-05 - participant_output_1_loss: 4.8660e-05 - participant_output_accuracy: 0.9543 - command_output_accuracy: 0.8914 - command_output_1_accuracy: 0.0014 - participant_output_1_accuracy: 0.3286 - val_loss: 2.3409 - val_participant_output_loss: 0.4874 - val_command_output_loss: 1.8535 - val_command_output_1_loss: 3.4309e-05 - val_participant_output_1_loss: 5.3172e-05 - val_participant_output_accuracy: 0.9282 - val_command_output_accuracy: 0.8508 - val_command_output_1_accuracy: 0.0829 - val_participant_output_1_accuracy: 0.1400\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 155ms/step - loss: 2.1040 - participant_output_loss: 0.3045 - command_output_loss: 1.7994 - command_output_1_loss: 3.9188e-05 - participant_output_1_loss: 4.4292e-05 - participant_output_accuracy: 0.9700 - command_output_accuracy: 0.9143 - command_output_1_accuracy: 0.0629 - participant_output_1_accuracy: 0.1200 - val_loss: 2.2413 - val_participant_output_loss: 0.4452 - val_command_output_loss: 1.7960 - val_command_output_1_loss: 2.6345e-05 - val_participant_output_1_loss: 4.8194e-05 - val_participant_output_accuracy: 0.9208 - val_command_output_accuracy: 0.8564 - val_command_output_1_accuracy: 0.0718 - val_participant_output_1_accuracy: 0.2947\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 1.9754 - participant_output_loss: 0.2414 - command_output_loss: 1.7340 - command_output_1_loss: 1.9688e-05 - participant_output_1_loss: 3.9350e-05 - participant_output_accuracy: 0.9857 - command_output_accuracy: 0.9457 - command_output_1_accuracy: 0.1900 - participant_output_1_accuracy: 0.2429 - val_loss: 2.0964 - val_participant_output_loss: 0.3629 - val_command_output_loss: 1.7334 - val_command_output_1_loss: 2.3243e-05 - val_participant_output_1_loss: 4.9956e-05 - val_participant_output_accuracy: 0.9558 - val_command_output_accuracy: 0.9024 - val_command_output_1_accuracy: 0.0037 - val_participant_output_1_accuracy: 0.2947\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 216ms/step - loss: 1.8625 - participant_output_loss: 0.1956 - command_output_loss: 1.6668 - command_output_1_loss: 1.7680e-05 - participant_output_1_loss: 3.7570e-05 - participant_output_accuracy: 0.9929 - command_output_accuracy: 0.9657 - command_output_1_accuracy: 0.0114 - participant_output_1_accuracy: 0.1986 - val_loss: 1.9801 - val_participant_output_loss: 0.3135 - val_command_output_loss: 1.6666 - val_command_output_1_loss: 2.3100e-05 - val_participant_output_1_loss: 4.2564e-05 - val_participant_output_accuracy: 0.9613 - val_command_output_accuracy: 0.9134 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1971\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 160ms/step - loss: 1.7635 - participant_output_loss: 0.1623 - command_output_loss: 1.6011 - command_output_1_loss: 3.2408e-05 - participant_output_1_loss: 3.4952e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9586 - command_output_1_accuracy: 0.0943 - participant_output_1_accuracy: 0.2243 - val_loss: 1.8908 - val_participant_output_loss: 0.2853 - val_command_output_loss: 1.6054 - val_command_output_1_loss: 2.7458e-05 - val_participant_output_1_loss: 4.2744e-05 - val_participant_output_accuracy: 0.9595 - val_command_output_accuracy: 0.9116 - val_command_output_1_accuracy: 0.2044 - val_participant_output_1_accuracy: 0.1952\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 205ms/step - loss: 1.6676 - participant_output_loss: 0.1378 - command_output_loss: 1.5298 - command_output_1_loss: 2.5718e-05 - participant_output_1_loss: 3.3704e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9729 - command_output_1_accuracy: 0.0757 - participant_output_1_accuracy: 0.2200 - val_loss: 1.7979 - val_participant_output_loss: 0.2511 - val_command_output_loss: 1.5467 - val_command_output_1_loss: 2.5467e-05 - val_participant_output_1_loss: 4.1427e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9171 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2081\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 1.5797 - participant_output_loss: 0.1158 - command_output_loss: 1.4639 - command_output_1_loss: 4.4311e-05 - participant_output_1_loss: 3.2670e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9886 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2314 - val_loss: 1.7199 - val_participant_output_loss: 0.2420 - val_command_output_loss: 1.4779 - val_command_output_1_loss: 3.2675e-05 - val_participant_output_1_loss: 4.1101e-05 - val_participant_output_accuracy: 0.9613 - val_command_output_accuracy: 0.9355 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.2523\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 154ms/step - loss: 1.4900 - participant_output_loss: 0.1021 - command_output_loss: 1.3879 - command_output_1_loss: 4.6107e-05 - participant_output_1_loss: 3.2038e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9914 - command_output_1_accuracy: 0.0971 - participant_output_1_accuracy: 0.1900 - val_loss: 1.6373 - val_participant_output_loss: 0.2261 - val_command_output_loss: 1.4111 - val_command_output_1_loss: 4.5017e-05 - val_participant_output_1_loss: 4.4730e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9282 - val_command_output_1_accuracy: 0.1381 - val_participant_output_1_accuracy: 0.3683\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 180ms/step - loss: 1.4065 - participant_output_loss: 0.0881 - command_output_loss: 1.3183 - command_output_1_loss: 2.8943e-05 - participant_output_1_loss: 3.1240e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9914 - command_output_1_accuracy: 0.0786 - participant_output_1_accuracy: 0.2200 - val_loss: 1.5452 - val_participant_output_loss: 0.1993 - val_command_output_loss: 1.3459 - val_command_output_1_loss: 3.7612e-05 - val_participant_output_1_loss: 3.9083e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9429 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2799\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 1.3246 - participant_output_loss: 0.0785 - command_output_loss: 1.2461 - command_output_1_loss: 3.0774e-05 - participant_output_1_loss: 2.9185e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9943 - command_output_1_accuracy: 0.0014 - participant_output_1_accuracy: 0.2186 - val_loss: 1.4779 - val_participant_output_loss: 0.1998 - val_command_output_loss: 1.2780 - val_command_output_1_loss: 2.3902e-05 - val_participant_output_1_loss: 3.7821e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9503 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 179ms/step - loss: 1.2497 - participant_output_loss: 0.0702 - command_output_loss: 1.1795 - command_output_1_loss: 2.0027e-05 - participant_output_1_loss: 2.7942e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.1229 - participant_output_1_accuracy: 0.2114 - val_loss: 1.4034 - val_participant_output_loss: 0.1857 - val_command_output_loss: 1.2176 - val_command_output_1_loss: 2.6139e-05 - val_participant_output_1_loss: 3.6917e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9521 - val_command_output_1_accuracy: 0.7808 - val_participant_output_1_accuracy: 0.3149\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 1.1743 - participant_output_loss: 0.0628 - command_output_loss: 1.1115 - command_output_1_loss: 3.4970e-05 - participant_output_1_loss: 2.6142e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.4443 - participant_output_1_accuracy: 0.2086 - val_loss: 1.3465 - val_participant_output_loss: 0.1840 - val_command_output_loss: 1.1624 - val_command_output_1_loss: 4.1231e-05 - val_participant_output_1_loss: 3.5133e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9540 - val_command_output_1_accuracy: 0.2597 - val_participant_output_1_accuracy: 0.2726\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 1.1041 - participant_output_loss: 0.0569 - command_output_loss: 1.0472 - command_output_1_loss: 3.6525e-05 - participant_output_1_loss: 2.4979e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.5443 - participant_output_1_accuracy: 0.2257 - val_loss: 1.2793 - val_participant_output_loss: 0.1800 - val_command_output_loss: 1.0992 - val_command_output_1_loss: 2.1825e-05 - val_participant_output_1_loss: 3.4387e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9576 - val_command_output_1_accuracy: 0.1455 - val_participant_output_1_accuracy: 0.2891\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 145ms/step - loss: 1.0363 - participant_output_loss: 0.0525 - command_output_loss: 0.9837 - command_output_1_loss: 2.1507e-05 - participant_output_1_loss: 2.4026e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0229 - participant_output_1_accuracy: 0.2214 - val_loss: 1.2116 - val_participant_output_loss: 0.1729 - val_command_output_loss: 1.0386 - val_command_output_1_loss: 2.4930e-05 - val_participant_output_1_loss: 3.4403e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9576 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.3297\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 0.9739 - participant_output_loss: 0.0484 - command_output_loss: 0.9254 - command_output_1_loss: 3.3571e-05 - participant_output_1_loss: 2.2983e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2157 - val_loss: 1.1479 - val_participant_output_loss: 0.1615 - val_command_output_loss: 0.9864 - val_command_output_1_loss: 2.4356e-05 - val_participant_output_1_loss: 3.2392e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2652\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 171ms/step - loss: 0.9139 - participant_output_loss: 0.0450 - command_output_loss: 0.8688 - command_output_1_loss: 2.0900e-05 - participant_output_1_loss: 2.2520e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0500 - participant_output_1_accuracy: 0.2443 - val_loss: 1.0925 - val_participant_output_loss: 0.1572 - val_command_output_loss: 0.9352 - val_command_output_1_loss: 2.2515e-05 - val_participant_output_1_loss: 3.1609e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.1381 - val_participant_output_1_accuracy: 0.2357\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 334ms/step - loss: 3.6012 - participant_output_loss: 1.3994 - command_output_loss: 2.1959 - command_output_1_loss: 5.7196e-04 - participant_output_1_loss: 0.0053 - participant_output_accuracy: 0.4357 - command_output_accuracy: 0.3486 - command_output_1_accuracy: 0.1386 - participant_output_1_accuracy: 0.1500 - val_loss: 3.3152 - val_participant_output_loss: 1.3146 - val_command_output_loss: 1.9998 - val_command_output_1_loss: 5.3082e-05 - val_participant_output_1_loss: 7.3061e-04 - val_participant_output_accuracy: 0.4199 - val_command_output_accuracy: 0.6759 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 2.9419 - participant_output_loss: 1.0276 - command_output_loss: 1.9138 - command_output_1_loss: 5.6335e-05 - participant_output_1_loss: 4.8668e-04 - participant_output_accuracy: 0.6329 - command_output_accuracy: 0.7400 - command_output_1_accuracy: 0.1271 - participant_output_1_accuracy: 0.2029 - val_loss: 2.9498 - val_participant_output_loss: 1.0968 - val_command_output_loss: 1.8528 - val_command_output_1_loss: 8.6201e-05 - val_participant_output_1_loss: 1.4040e-04 - val_participant_output_accuracy: 0.6133 - val_command_output_accuracy: 0.7053 - val_command_output_1_accuracy: 0.8029 - val_participant_output_1_accuracy: 0.4678\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 2.5897 - participant_output_loss: 0.8120 - command_output_loss: 1.7775 - command_output_1_loss: 7.8474e-05 - participant_output_1_loss: 1.1222e-04 - participant_output_accuracy: 0.7814 - command_output_accuracy: 0.7729 - command_output_1_accuracy: 0.5829 - participant_output_1_accuracy: 0.1557 - val_loss: 2.6487 - val_participant_output_loss: 0.9158 - val_command_output_loss: 1.7327 - val_command_output_1_loss: 3.3373e-05 - val_participant_output_1_loss: 1.2503e-04 - val_participant_output_accuracy: 0.7201 - val_command_output_accuracy: 0.7937 - val_command_output_1_accuracy: 0.2007 - val_participant_output_1_accuracy: 0.0331\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 2.3027 - participant_output_loss: 0.6499 - command_output_loss: 1.6527 - command_output_1_loss: 3.3775e-05 - participant_output_1_loss: 6.8927e-05 - participant_output_accuracy: 0.8714 - command_output_accuracy: 0.8771 - command_output_1_accuracy: 0.0571 - participant_output_1_accuracy: 0.2600 - val_loss: 2.3677 - val_participant_output_loss: 0.7447 - val_command_output_loss: 1.6229 - val_command_output_1_loss: 3.8509e-05 - val_participant_output_1_loss: 5.0215e-05 - val_participant_output_accuracy: 0.8619 - val_command_output_accuracy: 0.8692 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.3278\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 211ms/step - loss: 2.0539 - participant_output_loss: 0.5165 - command_output_loss: 1.5372 - command_output_1_loss: 6.4954e-05 - participant_output_1_loss: 4.2518e-05 - participant_output_accuracy: 0.9229 - command_output_accuracy: 0.9371 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2486 - val_loss: 2.1592 - val_participant_output_loss: 0.6424 - val_command_output_loss: 1.5167 - val_command_output_1_loss: 6.4391e-05 - val_participant_output_1_loss: 4.3357e-05 - val_participant_output_accuracy: 0.9024 - val_command_output_accuracy: 0.9061 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2689\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 213ms/step - loss: 1.8520 - participant_output_loss: 0.4241 - command_output_loss: 1.4278 - command_output_1_loss: 5.7172e-05 - participant_output_1_loss: 3.6132e-05 - participant_output_accuracy: 0.9600 - command_output_accuracy: 0.9529 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2529 - val_loss: 1.9567 - val_participant_output_loss: 0.5402 - val_command_output_loss: 1.4163 - val_command_output_1_loss: 4.2444e-05 - val_participant_output_1_loss: 4.3054e-05 - val_participant_output_accuracy: 0.9355 - val_command_output_accuracy: 0.9171 - val_command_output_1_accuracy: 0.0129 - val_participant_output_1_accuracy: 0.2762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 1.6731 - participant_output_loss: 0.3546 - command_output_loss: 1.3184 - command_output_1_loss: 3.3874e-05 - participant_output_1_loss: 3.4197e-05 - participant_output_accuracy: 0.9686 - command_output_accuracy: 0.9771 - command_output_1_accuracy: 0.0900 - participant_output_1_accuracy: 0.2486 - val_loss: 1.7872 - val_participant_output_loss: 0.4760 - val_command_output_loss: 1.3111 - val_command_output_1_loss: 2.9836e-05 - val_participant_output_1_loss: 4.1234e-05 - val_participant_output_accuracy: 0.9503 - val_command_output_accuracy: 0.9319 - val_command_output_1_accuracy: 0.1215 - val_participant_output_1_accuracy: 0.2873\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 1.5193 - participant_output_loss: 0.3047 - command_output_loss: 1.2145 - command_output_1_loss: 3.3866e-05 - participant_output_1_loss: 3.2278e-05 - participant_output_accuracy: 0.9843 - command_output_accuracy: 0.9800 - command_output_1_accuracy: 0.0371 - participant_output_1_accuracy: 0.2686 - val_loss: 1.6423 - val_participant_output_loss: 0.4240 - val_command_output_loss: 1.2182 - val_command_output_1_loss: 3.2911e-05 - val_participant_output_1_loss: 4.3024e-05 - val_participant_output_accuracy: 0.9595 - val_command_output_accuracy: 0.9503 - val_command_output_1_accuracy: 0.0497 - val_participant_output_1_accuracy: 0.2615\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 215ms/step - loss: 1.3798 - participant_output_loss: 0.2669 - command_output_loss: 1.1127 - command_output_1_loss: 4.0120e-05 - participant_output_1_loss: 3.2177e-05 - participant_output_accuracy: 0.9886 - command_output_accuracy: 0.9843 - command_output_1_accuracy: 0.0457 - participant_output_1_accuracy: 0.2771 - val_loss: 1.5169 - val_participant_output_loss: 0.3886 - val_command_output_loss: 1.1282 - val_command_output_1_loss: 3.9891e-05 - val_participant_output_1_loss: 4.1503e-05 - val_participant_output_accuracy: 0.9576 - val_command_output_accuracy: 0.9540 - val_command_output_1_accuracy: 0.0497 - val_participant_output_1_accuracy: 0.2265\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 1.2397 - participant_output_loss: 0.2232 - command_output_loss: 1.0165 - command_output_1_loss: 4.0146e-05 - participant_output_1_loss: 3.0913e-05 - participant_output_accuracy: 0.9929 - command_output_accuracy: 0.9914 - command_output_1_accuracy: 0.0386 - participant_output_1_accuracy: 0.2629 - val_loss: 1.3932 - val_participant_output_loss: 0.3573 - val_command_output_loss: 1.0359 - val_command_output_1_loss: 4.6911e-05 - val_participant_output_1_loss: 4.2123e-05 - val_participant_output_accuracy: 0.9576 - val_command_output_accuracy: 0.9521 - val_command_output_1_accuracy: 0.0350 - val_participant_output_1_accuracy: 0.2781\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 272ms/step - loss: 1.1078 - participant_output_loss: 0.1879 - command_output_loss: 0.9198 - command_output_1_loss: 3.8122e-05 - participant_output_1_loss: 2.9640e-05 - participant_output_accuracy: 0.9943 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.0386 - participant_output_1_accuracy: 0.2714 - val_loss: 1.2623 - val_participant_output_loss: 0.3063 - val_command_output_loss: 0.9560 - val_command_output_1_loss: 4.2206e-05 - val_participant_output_1_loss: 4.0030e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9521 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.2339\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 0.9931 - participant_output_loss: 0.1587 - command_output_loss: 0.8343 - command_output_1_loss: 4.6259e-05 - participant_output_1_loss: 2.8512e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2429 - val_loss: 1.1523 - val_participant_output_loss: 0.2762 - val_command_output_loss: 0.8760 - val_command_output_1_loss: 4.6366e-05 - val_participant_output_1_loss: 3.9489e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.0037 - val_participant_output_1_accuracy: 0.2726\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 146ms/step - loss: 0.8949 - participant_output_loss: 0.1396 - command_output_loss: 0.7552 - command_output_1_loss: 4.0415e-05 - participant_output_1_loss: 2.7553e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1500 - participant_output_1_accuracy: 0.2986 - val_loss: 1.0700 - val_participant_output_loss: 0.2586 - val_command_output_loss: 0.8113 - val_command_output_1_loss: 3.9656e-05 - val_participant_output_1_loss: 3.8976e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.2413 - val_participant_output_1_accuracy: 0.2192\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 147ms/step - loss: 0.8059 - participant_output_loss: 0.1239 - command_output_loss: 0.6820 - command_output_1_loss: 3.5760e-05 - participant_output_1_loss: 2.7061e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1500 - participant_output_1_accuracy: 0.2443 - val_loss: 0.9848 - val_participant_output_loss: 0.2429 - val_command_output_loss: 0.7418 - val_command_output_1_loss: 3.4684e-05 - val_participant_output_1_loss: 3.8841e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.0129 - val_participant_output_1_accuracy: 0.2670\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 149ms/step - loss: 0.7268 - participant_output_loss: 0.1103 - command_output_loss: 0.6164 - command_output_1_loss: 3.5820e-05 - participant_output_1_loss: 2.5674e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0043 - participant_output_1_accuracy: 0.2586 - val_loss: 0.9200 - val_participant_output_loss: 0.2303 - val_command_output_loss: 0.6896 - val_command_output_1_loss: 4.0581e-05 - val_participant_output_1_loss: 3.7297e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9687 - val_command_output_1_accuracy: 0.0092 - val_participant_output_1_accuracy: 0.2376\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 149ms/step - loss: 0.6606 - participant_output_loss: 0.1010 - command_output_loss: 0.5596 - command_output_1_loss: 4.4167e-05 - participant_output_1_loss: 2.4263e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.3543 - participant_output_1_accuracy: 0.2643 - val_loss: 0.8579 - val_participant_output_loss: 0.2212 - val_command_output_loss: 0.6366 - val_command_output_1_loss: 3.9147e-05 - val_participant_output_1_loss: 3.7256e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.3186 - val_participant_output_1_accuracy: 0.2063\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 0.5990 - participant_output_loss: 0.0920 - command_output_loss: 0.5070 - command_output_1_loss: 3.2379e-05 - participant_output_1_loss: 2.3403e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0557 - participant_output_1_accuracy: 0.2686 - val_loss: 0.7932 - val_participant_output_loss: 0.2054 - val_command_output_loss: 0.5877 - val_command_output_1_loss: 3.7073e-05 - val_participant_output_1_loss: 3.7270e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.2210\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 333ms/step - loss: 3.7255 - participant_output_loss: 1.3766 - command_output_loss: 2.3425 - command_output_1_loss: 0.0032 - participant_output_1_loss: 0.0032 - participant_output_accuracy: 0.4657 - command_output_accuracy: 0.1529 - command_output_1_accuracy: 0.0343 - participant_output_1_accuracy: 0.1686 - val_loss: 3.5376 - val_participant_output_loss: 1.3482 - val_command_output_loss: 2.1887 - val_command_output_1_loss: 4.7678e-04 - val_participant_output_1_loss: 1.5757e-04 - val_participant_output_accuracy: 0.3941 - val_command_output_accuracy: 0.3996 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.6501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 3.1216 - participant_output_loss: 0.9847 - command_output_loss: 2.1364 - command_output_1_loss: 4.6188e-04 - participant_output_1_loss: 1.2179e-04 - participant_output_accuracy: 0.6557 - command_output_accuracy: 0.5829 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2186 - val_loss: 3.1441 - val_participant_output_loss: 1.0426 - val_command_output_loss: 2.1011 - val_command_output_1_loss: 3.4536e-04 - val_participant_output_1_loss: 6.7071e-05 - val_participant_output_accuracy: 0.6667 - val_command_output_accuracy: 0.6169 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1473\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 2.8369 - participant_output_loss: 0.7717 - command_output_loss: 2.0649 - command_output_1_loss: 2.4085e-04 - participant_output_1_loss: 5.7331e-05 - participant_output_accuracy: 0.8257 - command_output_accuracy: 0.7243 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1243 - val_loss: 2.9173 - val_participant_output_loss: 0.8730 - val_command_output_loss: 2.0441 - val_command_output_1_loss: 1.6576e-04 - val_participant_output_1_loss: 4.4401e-05 - val_participant_output_accuracy: 0.7882 - val_command_output_accuracy: 0.7238 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2836\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 180ms/step - loss: 2.6190 - participant_output_loss: 0.6182 - command_output_loss: 2.0006 - command_output_1_loss: 1.3141e-04 - participant_output_1_loss: 3.8926e-05 - participant_output_accuracy: 0.9057 - command_output_accuracy: 0.8171 - command_output_1_accuracy: 0.0257 - participant_output_1_accuracy: 0.2443 - val_loss: 2.7050 - val_participant_output_loss: 0.7196 - val_command_output_loss: 1.9852 - val_command_output_1_loss: 8.0494e-05 - val_participant_output_1_loss: 4.2422e-05 - val_participant_output_accuracy: 0.8692 - val_command_output_accuracy: 0.7477 - val_command_output_1_accuracy: 0.2118 - val_participant_output_1_accuracy: 0.3204\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 2.4438 - participant_output_loss: 0.5029 - command_output_loss: 1.9407 - command_output_1_loss: 6.2989e-05 - participant_output_1_loss: 3.0388e-05 - participant_output_accuracy: 0.9371 - command_output_accuracy: 0.8329 - command_output_1_accuracy: 0.0586 - participant_output_1_accuracy: 0.2643 - val_loss: 2.5176 - val_participant_output_loss: 0.5903 - val_command_output_loss: 1.9273 - val_command_output_1_loss: 4.5908e-05 - val_participant_output_1_loss: 3.4142e-05 - val_participant_output_accuracy: 0.9319 - val_command_output_accuracy: 0.8122 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1842\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 180ms/step - loss: 2.2923 - participant_output_loss: 0.4137 - command_output_loss: 1.8785 - command_output_1_loss: 4.6516e-05 - participant_output_1_loss: 2.6567e-05 - participant_output_accuracy: 0.9757 - command_output_accuracy: 0.8829 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2343 - val_loss: 2.3974 - val_participant_output_loss: 0.5284 - val_command_output_loss: 1.8689 - val_command_output_1_loss: 3.9435e-05 - val_participant_output_1_loss: 3.2851e-05 - val_participant_output_accuracy: 0.9355 - val_command_output_accuracy: 0.8619 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2910\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 2.1737 - participant_output_loss: 0.3513 - command_output_loss: 1.8224 - command_output_1_loss: 4.0108e-05 - participant_output_1_loss: 2.3621e-05 - participant_output_accuracy: 0.9814 - command_output_accuracy: 0.9071 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2300 - val_loss: 2.2755 - val_participant_output_loss: 0.4601 - val_command_output_loss: 1.8154 - val_command_output_1_loss: 2.1484e-05 - val_participant_output_1_loss: 2.8606e-05 - val_participant_output_accuracy: 0.9466 - val_command_output_accuracy: 0.9134 - val_command_output_1_accuracy: 0.0037 - val_participant_output_1_accuracy: 0.2320\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 2.0587 - participant_output_loss: 0.3008 - command_output_loss: 1.7578 - command_output_1_loss: 1.5345e-05 - participant_output_1_loss: 2.1199e-05 - participant_output_accuracy: 0.9871 - command_output_accuracy: 0.9671 - command_output_1_accuracy: 0.1929 - participant_output_1_accuracy: 0.2086 - val_loss: 2.1839 - val_participant_output_loss: 0.4243 - val_command_output_loss: 1.7596 - val_command_output_1_loss: 2.0164e-05 - val_participant_output_1_loss: 2.8503e-05 - val_participant_output_accuracy: 0.9558 - val_command_output_accuracy: 0.9116 - val_command_output_1_accuracy: 0.0645 - val_participant_output_1_accuracy: 0.2689\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 186ms/step - loss: 1.9539 - participant_output_loss: 0.2569 - command_output_loss: 1.6969 - command_output_1_loss: 1.7543e-05 - participant_output_1_loss: 1.9982e-05 - participant_output_accuracy: 0.9886 - command_output_accuracy: 0.9743 - command_output_1_accuracy: 0.1200 - participant_output_1_accuracy: 0.1957 - val_loss: 2.0782 - val_participant_output_loss: 0.3792 - val_command_output_loss: 1.6989 - val_command_output_1_loss: 1.3596e-05 - val_participant_output_1_loss: 2.5103e-05 - val_participant_output_accuracy: 0.9595 - val_command_output_accuracy: 0.9282 - val_command_output_1_accuracy: 0.0350 - val_participant_output_1_accuracy: 0.2449\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 1.8549 - participant_output_loss: 0.2209 - command_output_loss: 1.6340 - command_output_1_loss: 3.5452e-05 - participant_output_1_loss: 1.8172e-05 - participant_output_accuracy: 0.9914 - command_output_accuracy: 0.9857 - command_output_1_accuracy: 0.0014 - participant_output_1_accuracy: 0.1957 - val_loss: 1.9978 - val_participant_output_loss: 0.3537 - val_command_output_loss: 1.6441 - val_command_output_1_loss: 3.6832e-05 - val_participant_output_1_loss: 2.4038e-05 - val_participant_output_accuracy: 0.9558 - val_command_output_accuracy: 0.9355 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2081\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 250ms/step - loss: 1.7654 - participant_output_loss: 0.1926 - command_output_loss: 1.5728 - command_output_1_loss: 3.3927e-05 - participant_output_1_loss: 1.6877e-05 - participant_output_accuracy: 0.9957 - command_output_accuracy: 0.9857 - command_output_1_accuracy: 0.0057 - participant_output_1_accuracy: 0.1786 - val_loss: 1.9140 - val_participant_output_loss: 0.3179 - val_command_output_loss: 1.5961 - val_command_output_1_loss: 3.7119e-05 - val_participant_output_1_loss: 2.3589e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9337 - val_command_output_1_accuracy: 0.1860 - val_participant_output_1_accuracy: 0.2855\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 188ms/step - loss: 1.6787 - participant_output_loss: 0.1711 - command_output_loss: 1.5076 - command_output_1_loss: 2.7962e-05 - participant_output_1_loss: 1.5801e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9929 - command_output_1_accuracy: 0.2529 - participant_output_1_accuracy: 0.1914 - val_loss: 1.8358 - val_participant_output_loss: 0.3046 - val_command_output_loss: 1.5312 - val_command_output_1_loss: 2.6767e-05 - val_participant_output_1_loss: 2.2027e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9392 - val_command_output_1_accuracy: 0.2597 - val_participant_output_1_accuracy: 0.2873\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 181ms/step - loss: 1.6017 - participant_output_loss: 0.1579 - command_output_loss: 1.4437 - command_output_1_loss: 3.6690e-05 - participant_output_1_loss: 1.4857e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.0229 - participant_output_1_accuracy: 0.2300 - val_loss: 1.7509 - val_participant_output_loss: 0.2846 - val_command_output_loss: 1.4663 - val_command_output_1_loss: 2.1564e-05 - val_participant_output_1_loss: 2.1718e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9411 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 203ms/step - loss: 1.5204 - participant_output_loss: 0.1421 - command_output_loss: 1.3783 - command_output_1_loss: 2.9126e-05 - participant_output_1_loss: 1.4113e-05 - participant_output_accuracy: 0.9986 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.1300 - participant_output_1_accuracy: 0.1914 - val_loss: 1.6815 - val_participant_output_loss: 0.2758 - val_command_output_loss: 1.4057 - val_command_output_1_loss: 4.3086e-05 - val_participant_output_1_loss: 2.0820e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9540 - val_command_output_1_accuracy: 0.0276 - val_participant_output_1_accuracy: 0.2541\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 144ms/step - loss: 1.4447 - participant_output_loss: 0.1331 - command_output_loss: 1.3115 - command_output_1_loss: 3.5213e-05 - participant_output_1_loss: 1.3386e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0029 - participant_output_1_accuracy: 0.2186 - val_loss: 1.5993 - val_participant_output_loss: 0.2469 - val_command_output_loss: 1.3524 - val_command_output_1_loss: 3.3577e-05 - val_participant_output_1_loss: 2.0229e-05 - val_participant_output_accuracy: 0.9724 - val_command_output_accuracy: 0.9466 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2155\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 155ms/step - loss: 1.3681 - participant_output_loss: 0.1209 - command_output_loss: 1.2471 - command_output_1_loss: 2.4491e-05 - participant_output_1_loss: 1.2854e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0014 - participant_output_1_accuracy: 0.1957 - val_loss: 1.5306 - val_participant_output_loss: 0.2404 - val_command_output_loss: 1.2902 - val_command_output_1_loss: 1.7396e-05 - val_participant_output_1_loss: 2.0112e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9503 - val_command_output_1_accuracy: 0.1657 - val_participant_output_1_accuracy: 0.2228\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 182ms/step - loss: 1.2971 - participant_output_loss: 0.1103 - command_output_loss: 1.1867 - command_output_1_loss: 3.6782e-05 - participant_output_1_loss: 1.2397e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.2414 - participant_output_1_accuracy: 0.2171 - val_loss: 1.4661 - val_participant_output_loss: 0.2291 - val_command_output_loss: 1.2369 - val_command_output_1_loss: 2.7600e-05 - val_participant_output_1_loss: 1.9716e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9540 - val_command_output_1_accuracy: 0.4880 - val_participant_output_1_accuracy: 0.2155\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 145ms/step - loss: 1.2310 - participant_output_loss: 0.1026 - command_output_loss: 1.1284 - command_output_1_loss: 3.7786e-05 - participant_output_1_loss: 1.2023e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.5171 - participant_output_1_accuracy: 0.2214 - val_loss: 1.4005 - val_participant_output_loss: 0.2211 - val_command_output_loss: 1.1793 - val_command_output_1_loss: 4.4001e-05 - val_participant_output_1_loss: 1.9420e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9558 - val_command_output_1_accuracy: 0.1381 - val_participant_output_1_accuracy: 0.2192\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 182ms/step - loss: 1.1647 - participant_output_loss: 0.0960 - command_output_loss: 1.0687 - command_output_1_loss: 3.3776e-05 - participant_output_1_loss: 1.1541e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0229 - participant_output_1_accuracy: 0.2100 - val_loss: 1.3393 - val_participant_output_loss: 0.2116 - val_command_output_loss: 1.1276 - val_command_output_1_loss: 2.9197e-05 - val_participant_output_1_loss: 1.9176e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9576 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2265\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 182ms/step - loss: 1.1072 - participant_output_loss: 0.0902 - command_output_loss: 1.0170 - command_output_1_loss: 3.4612e-05 - participant_output_1_loss: 1.1109e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2329 - val_loss: 1.2886 - val_participant_output_loss: 0.2104 - val_command_output_loss: 1.0781 - val_command_output_1_loss: 2.4684e-05 - val_participant_output_1_loss: 1.8861e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2136\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 336ms/step - loss: 3.5959 - participant_output_loss: 1.3921 - command_output_loss: 2.1990 - command_output_1_loss: 8.8355e-04 - participant_output_1_loss: 0.0039 - participant_output_accuracy: 0.4471 - command_output_accuracy: 0.3057 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.0743 - val_loss: 3.3958 - val_participant_output_loss: 1.4039 - val_command_output_loss: 1.9913 - val_command_output_1_loss: 2.9695e-04 - val_participant_output_1_loss: 3.4894e-04 - val_participant_output_accuracy: 0.3444 - val_command_output_accuracy: 0.5359 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0037\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 2.9111 - participant_output_loss: 1.0119 - command_output_loss: 1.8989 - command_output_1_loss: 1.8463e-04 - participant_output_1_loss: 2.1185e-04 - participant_output_accuracy: 0.6371 - command_output_accuracy: 0.6529 - command_output_1_accuracy: 0.1443 - participant_output_1_accuracy: 0.4143 - val_loss: 2.9504 - val_participant_output_loss: 1.1036 - val_command_output_loss: 1.8466 - val_command_output_1_loss: 9.2318e-05 - val_participant_output_1_loss: 1.0253e-04 - val_participant_output_accuracy: 0.6059 - val_command_output_accuracy: 0.6427 - val_command_output_1_accuracy: 0.0055 - val_participant_output_1_accuracy: 0.2136\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 2.5526 - participant_output_loss: 0.7898 - command_output_loss: 1.7626 - command_output_1_loss: 7.2150e-05 - participant_output_1_loss: 7.5127e-05 - participant_output_accuracy: 0.7586 - command_output_accuracy: 0.7229 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1057 - val_loss: 2.6020 - val_participant_output_loss: 0.8648 - val_command_output_loss: 1.7371 - val_command_output_1_loss: 6.4390e-05 - val_participant_output_1_loss: 3.8433e-05 - val_participant_output_accuracy: 0.8085 - val_command_output_accuracy: 0.7145 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1565\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 2.2617 - participant_output_loss: 0.6277 - command_output_loss: 1.6340 - command_output_1_loss: 5.3721e-05 - participant_output_1_loss: 4.3809e-05 - participant_output_accuracy: 0.8800 - command_output_accuracy: 0.8043 - command_output_1_accuracy: 0.0014 - participant_output_1_accuracy: 0.3514 - val_loss: 2.3560 - val_participant_output_loss: 0.7462 - val_command_output_loss: 1.6098 - val_command_output_1_loss: 5.6694e-05 - val_participant_output_1_loss: 3.1556e-05 - val_participant_output_accuracy: 0.8343 - val_command_output_accuracy: 0.7993 - val_command_output_1_accuracy: 0.0055 - val_participant_output_1_accuracy: 0.3720\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 2.0123 - participant_output_loss: 0.4945 - command_output_loss: 1.5177 - command_output_1_loss: 6.5779e-05 - participant_output_1_loss: 2.7454e-05 - participant_output_accuracy: 0.9314 - command_output_accuracy: 0.9029 - command_output_1_accuracy: 0.0743 - participant_output_1_accuracy: 0.1900 - val_loss: 2.1095 - val_participant_output_loss: 0.6101 - val_command_output_loss: 1.4993 - val_command_output_1_loss: 6.9875e-05 - val_participant_output_1_loss: 2.7491e-05 - val_participant_output_accuracy: 0.9042 - val_command_output_accuracy: 0.8932 - val_command_output_1_accuracy: 0.2652 - val_participant_output_1_accuracy: 0.2799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 153ms/step - loss: 1.8075 - participant_output_loss: 0.4073 - command_output_loss: 1.4002 - command_output_1_loss: 4.9966e-05 - participant_output_1_loss: 2.1555e-05 - participant_output_accuracy: 0.9457 - command_output_accuracy: 0.9257 - command_output_1_accuracy: 0.3843 - participant_output_1_accuracy: 0.2943 - val_loss: 1.9363 - val_participant_output_loss: 0.5453 - val_command_output_loss: 1.3909 - val_command_output_1_loss: 3.8232e-05 - val_participant_output_1_loss: 2.4357e-05 - val_participant_output_accuracy: 0.9116 - val_command_output_accuracy: 0.8803 - val_command_output_1_accuracy: 0.1529 - val_participant_output_1_accuracy: 0.2707\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 185ms/step - loss: 1.6232 - participant_output_loss: 0.3325 - command_output_loss: 1.2907 - command_output_1_loss: 4.0056e-05 - participant_output_1_loss: 2.0002e-05 - participant_output_accuracy: 0.9800 - command_output_accuracy: 0.9314 - command_output_1_accuracy: 0.0657 - participant_output_1_accuracy: 0.2500 - val_loss: 1.7319 - val_participant_output_loss: 0.4352 - val_command_output_loss: 1.2966 - val_command_output_1_loss: 3.5002e-05 - val_participant_output_1_loss: 2.4389e-05 - val_participant_output_accuracy: 0.9503 - val_command_output_accuracy: 0.9042 - val_command_output_1_accuracy: 0.0055 - val_participant_output_1_accuracy: 0.2136\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 163ms/step - loss: 1.4544 - participant_output_loss: 0.2711 - command_output_loss: 1.1832 - command_output_1_loss: 3.4212e-05 - participant_output_1_loss: 1.9368e-05 - participant_output_accuracy: 0.9871 - command_output_accuracy: 0.9514 - command_output_1_accuracy: 0.0143 - participant_output_1_accuracy: 0.2600 - val_loss: 1.6108 - val_participant_output_loss: 0.4138 - val_command_output_loss: 1.1969 - val_command_output_1_loss: 3.7659e-05 - val_participant_output_1_loss: 2.4348e-05 - val_participant_output_accuracy: 0.9411 - val_command_output_accuracy: 0.9042 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.2118\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 185ms/step - loss: 1.3124 - participant_output_loss: 0.2261 - command_output_loss: 1.0862 - command_output_1_loss: 4.4702e-05 - participant_output_1_loss: 1.8489e-05 - participant_output_accuracy: 0.9929 - command_output_accuracy: 0.9543 - command_output_1_accuracy: 0.0214 - participant_output_1_accuracy: 0.2329 - val_loss: 1.4700 - val_participant_output_loss: 0.3589 - val_command_output_loss: 1.1110 - val_command_output_1_loss: 3.5089e-05 - val_participant_output_1_loss: 2.4679e-05 - val_participant_output_accuracy: 0.9540 - val_command_output_accuracy: 0.9282 - val_command_output_1_accuracy: 0.1381 - val_participant_output_1_accuracy: 0.2689\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 1.1785 - participant_output_loss: 0.1928 - command_output_loss: 0.9857 - command_output_1_loss: 4.0568e-05 - participant_output_1_loss: 1.7895e-05 - participant_output_accuracy: 0.9900 - command_output_accuracy: 0.9871 - command_output_1_accuracy: 0.1686 - participant_output_1_accuracy: 0.2086 - val_loss: 1.3285 - val_participant_output_loss: 0.3099 - val_command_output_loss: 1.0185 - val_command_output_1_loss: 6.0751e-05 - val_participant_output_1_loss: 2.4303e-05 - val_participant_output_accuracy: 0.9521 - val_command_output_accuracy: 0.9540 - val_command_output_1_accuracy: 0.2026 - val_participant_output_1_accuracy: 0.2468\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 1.0498 - participant_output_loss: 0.1575 - command_output_loss: 0.8923 - command_output_1_loss: 5.3039e-05 - participant_output_1_loss: 1.7253e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9929 - command_output_1_accuracy: 0.1314 - participant_output_1_accuracy: 0.2300 - val_loss: 1.2118 - val_participant_output_loss: 0.2730 - val_command_output_loss: 0.9387 - val_command_output_1_loss: 3.5280e-05 - val_participant_output_1_loss: 2.4154e-05 - val_participant_output_accuracy: 0.9613 - val_command_output_accuracy: 0.9540 - val_command_output_1_accuracy: 0.0608 - val_participant_output_1_accuracy: 0.2910\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 147ms/step - loss: 0.9353 - participant_output_loss: 0.1283 - command_output_loss: 0.8069 - command_output_1_loss: 3.4006e-05 - participant_output_1_loss: 1.6952e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9943 - command_output_1_accuracy: 0.0643 - participant_output_1_accuracy: 0.2314 - val_loss: 1.1074 - val_participant_output_loss: 0.2459 - val_command_output_loss: 0.8614 - val_command_output_1_loss: 3.6581e-05 - val_participant_output_1_loss: 2.3309e-05 - val_participant_output_accuracy: 0.9595 - val_command_output_accuracy: 0.9448 - val_command_output_1_accuracy: 0.0626 - val_participant_output_1_accuracy: 0.2357\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 185ms/step - loss: 0.8342 - participant_output_loss: 0.1084 - command_output_loss: 0.7258 - command_output_1_loss: 3.8363e-05 - participant_output_1_loss: 1.6161e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.0443 - participant_output_1_accuracy: 0.1900 - val_loss: 1.0158 - val_participant_output_loss: 0.2262 - val_command_output_loss: 0.7895 - val_command_output_1_loss: 3.4647e-05 - val_participant_output_1_loss: 2.3593e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9595 - val_command_output_1_accuracy: 0.0074 - val_participant_output_1_accuracy: 0.2910\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 148ms/step - loss: 0.7442 - participant_output_loss: 0.0928 - command_output_loss: 0.6514 - command_output_1_loss: 3.4010e-05 - participant_output_1_loss: 1.5479e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.0057 - participant_output_1_accuracy: 0.1871 - val_loss: 0.9526 - val_participant_output_loss: 0.2264 - val_command_output_loss: 0.7261 - val_command_output_1_loss: 3.9565e-05 - val_participant_output_1_loss: 2.2639e-05 - val_participant_output_accuracy: 0.9558 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.0608 - val_participant_output_1_accuracy: 0.2781\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 141ms/step - loss: 0.6699 - participant_output_loss: 0.0812 - command_output_loss: 0.5886 - command_output_1_loss: 4.0465e-05 - participant_output_1_loss: 1.4834e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.3743 - participant_output_1_accuracy: 0.2071 - val_loss: 0.8691 - val_participant_output_loss: 0.1964 - val_command_output_loss: 0.6727 - val_command_output_1_loss: 3.1124e-05 - val_participant_output_1_loss: 2.1879e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9540 - val_command_output_1_accuracy: 0.2284 - val_participant_output_1_accuracy: 0.2284\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 0.5987 - participant_output_loss: 0.0703 - command_output_loss: 0.5284 - command_output_1_loss: 2.7146e-05 - participant_output_1_loss: 1.4232e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0386 - participant_output_1_accuracy: 0.2000 - val_loss: 0.7916 - val_participant_output_loss: 0.1755 - val_command_output_loss: 0.6161 - val_command_output_1_loss: 2.6141e-05 - val_participant_output_1_loss: 2.1420e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2597\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 179ms/step - loss: 0.5395 - participant_output_loss: 0.0633 - command_output_loss: 0.4762 - command_output_1_loss: 2.5475e-05 - participant_output_1_loss: 1.3405e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0043 - participant_output_1_accuracy: 0.1943 - val_loss: 0.7402 - val_participant_output_loss: 0.1713 - val_command_output_loss: 0.5688 - val_command_output_1_loss: 2.6546e-05 - val_participant_output_1_loss: 2.1148e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.0645 - val_participant_output_1_accuracy: 0.2560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 0.4854 - participant_output_loss: 0.0576 - command_output_loss: 0.4278 - command_output_1_loss: 2.3275e-05 - participant_output_1_loss: 1.2650e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1843 - participant_output_1_accuracy: 0.1886 - val_loss: 0.6917 - val_participant_output_loss: 0.1656 - val_command_output_loss: 0.5261 - val_command_output_1_loss: 2.3059e-05 - val_participant_output_1_loss: 2.0092e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.3407 - val_participant_output_1_accuracy: 0.2468\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 145ms/step - loss: 0.4396 - participant_output_loss: 0.0527 - command_output_loss: 0.3869 - command_output_1_loss: 2.3216e-05 - participant_output_1_loss: 1.1950e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.3200 - participant_output_1_accuracy: 0.1886 - val_loss: 0.6408 - val_participant_output_loss: 0.1512 - val_command_output_loss: 0.4896 - val_command_output_1_loss: 2.3786e-05 - val_participant_output_1_loss: 1.9726e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9595 - val_command_output_1_accuracy: 0.1197 - val_participant_output_1_accuracy: 0.2523\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 163ms/step - loss: 0.3973 - participant_output_loss: 0.0479 - command_output_loss: 0.3494 - command_output_1_loss: 2.0623e-05 - participant_output_1_loss: 1.1437e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0229 - participant_output_1_accuracy: 0.1800 - val_loss: 0.6008 - val_participant_output_loss: 0.1474 - val_command_output_loss: 0.4534 - val_command_output_1_loss: 2.1256e-05 - val_participant_output_1_loss: 1.9124e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2468\n"
     ]
    }
   ],
   "source": [
    "high_acc = 0\n",
    "for run in range(0,10):\n",
    "    # feature extraction layers\n",
    "    resnet_model = ResNet50(input_shape=(224, 224,3), include_top=False, weights=\"imagenet\")\n",
    "    for layer in resnet_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    d = resnet_model.output.shape[-1] # dimension of last layer\n",
    "\n",
    "    ###################### model 1 ###################### \n",
    "    layer_1_0 = tf.keras.layers.Dense(d,name=\"weight_1\")(resnet_model.output) #times weight before flatten\n",
    "    layer_1_1 = tf.keras.layers.Flatten(name='flatten_1')(layer_1_0)\n",
    "\n",
    "    Dense_1_1 = tf.keras.layers.Dense(shape_1_1, activation=actv_fun_1_1,name='fc1_1')\n",
    "    layer_1_2 = Dense_1_1(layer_1_1)\n",
    "    Dense_1_2 = tf.keras.layers.Dense(shape_1_2, activation=actv_fun_1_2,name='fc1_2')\n",
    "    layer_1_3 = Dense_1_2(layer_1_2)\n",
    "\n",
    "    Dense_1_3 = tf.keras.layers.Dense(train_number, activation='softmax' ,name='participant_output')\n",
    "    out_layer_1 = Dense_1_3(layer_1_3)\n",
    "\n",
    "    ###################### model 2 ###################### \n",
    "    layer_2_0 = tf.keras.layers.Dense(d,name=\"weight_2\")(resnet_model.output) #times weight before flatten\n",
    "    layer_2_1 = tf.keras.layers.Flatten(name='flatten_2')(layer_2_0)\n",
    "\n",
    "    Dense_2_1 = tf.keras.layers.Dense(shape_2_1, activation=actv_fun_2_1,name='fc2_1')\n",
    "    layer_2_2  = Dense_2_1(layer_2_1)\n",
    "    Dense_2_2 = tf.keras.layers.Dense(shape_2_2, activation=actv_fun_2_2,name='fc2_2')\n",
    "    layer_2_3  = Dense_2_2(layer_2_2)\n",
    "\n",
    "    Dense_2_3 = tf.keras.layers.Dense(10, activation='softmax',name='command_output')\n",
    "    out_layer_2 = Dense_2_3(layer_2_3)\n",
    "\n",
    "    ###################### model 1' ###################### \n",
    "    layer_1_2_  = Dense_2_1(layer_1_1)\n",
    "    layer_1_3_  = Dense_2_2(layer_1_2_)\n",
    "    out_layer_1_ = Dense_2_3(layer_1_3_)\n",
    "\n",
    "    ###################### model 1' ###################### \n",
    "    layer_2_2_  = Dense_1_1(layer_2_1)\n",
    "    layer_2_3_  = Dense_1_2(layer_2_2_)\n",
    "    out_layer_2_ = Dense_1_3(layer_2_3_)\n",
    "\n",
    "    resnet_model = tf.keras.Model(resnet_model.input, [out_layer_1,out_layer_2,out_layer_1_,out_layer_2_])\n",
    "\n",
    "\n",
    "    # resnet_model.summary() \n",
    "\n",
    "    w_1, w_2, w_1_, w_2_ = 1,1,1,1\n",
    "    ##################### training ############################\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    callbacks = [MyEarlyStopping(monitor1 = 'val_' + resnet_model.layers[-1].name+'_accuracy',\n",
    "                                  monitor2 = 'val_' + resnet_model.layers[-2].name+'_accuracy',\n",
    "                                  patience=5,restore_best_weights=True)]\n",
    "    resnet_model.compile(optimizer=opt, loss=[\"categorical_crossentropy\",\"categorical_crossentropy\",\"mse\",\"mse\"],\n",
    "                         loss_weights=[w_1, w_2, w_1_, w_2_], metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "       \n",
    "    history1 = resnet_model.fit(Train_Inputs, \n",
    "                           {resnet_model.layers[-2].name:Train_participant_class, \n",
    "                            resnet_model.layers[-1].name:Train_command_class,\n",
    "                            resnet_model.layers[-1].name+\"_1\":Train_command_uniform, \n",
    "                            resnet_model.layers[-2].name+\"_1\":Train_participant_uniform}, \n",
    "                            validation_data=(Val_Inputs,\n",
    "                                             {resnet_model.layers[-2].name:Val_participant_class,\n",
    "                                              resnet_model.layers[-1].name:Val_command_class,\n",
    "                                              resnet_model.layers[-1].name+\"_1\":Val_command_uniform,\n",
    "                                              resnet_model.layers[-2].name+\"_1\":Val_participant_uniform}), \n",
    "                           callbacks=callbacks,\n",
    "                           batch_size=64,\n",
    "                           epochs=10)\n",
    "    \n",
    "    for layer in resnet_model.layers[0:175]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    history2 = resnet_model.fit(Train_Inputs, \n",
    "                               {resnet_model.layers[-2].name:Train_participant_class, \n",
    "                                resnet_model.layers[-1].name:Train_command_class,\n",
    "                                resnet_model.layers[-1].name+\"_1\":Train_command_uniform, \n",
    "                                resnet_model.layers[-2].name+\"_1\":Train_participant_uniform}, \n",
    "                                validation_data=(Val_Inputs,\n",
    "                                                 {resnet_model.layers[-2].name:Val_participant_class,\n",
    "                                                  resnet_model.layers[-1].name:Val_command_class,\n",
    "                                                  resnet_model.layers[-1].name+\"_1\":Val_command_uniform,\n",
    "                                                  resnet_model.layers[-2].name+\"_1\":Val_participant_uniform}), \n",
    "                               callbacks=callbacks,\n",
    "                               batch_size=64,\n",
    "                               epochs=10)\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    run_time = stop - start\n",
    "    \n",
    "    ##################### test performance ############################\n",
    "    predictions = resnet_model.predict(Test_Inputs)[0]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)+1\n",
    "    true_classes = test_unit_participant_class\n",
    "    acc_p15_s = metrics.accuracy_score(true_classes, predicted_classes).round(4)\n",
    "\n",
    "    predictions = resnet_model.predict(Test_Inputs)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class\n",
    "    acc_p15_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)\n",
    "\n",
    "\n",
    "    # test on p1\n",
    "    ## speaker classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_1)[0]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)+1\n",
    "    true_classes = np.array([1]*len(predicted_classes))\n",
    "    acc_p1_s = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_1)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_1, axis=-1)\n",
    "    acc_p1_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p2\n",
    "    ## speaker classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_2)[0]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)+1\n",
    "    true_classes = np.array([2]*len(predicted_classes))\n",
    "    acc_p2_s = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_2)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_2, axis=-1)\n",
    "    acc_p2_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p3\n",
    "    ## speaker classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_3)[0]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)+1\n",
    "    true_classes = np.array([3]*len(predicted_classes))\n",
    "    acc_p3_s = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_3)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_3, axis=-1)\n",
    "    acc_p3_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p4\n",
    "    ## speaker classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_4)[0]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)+1\n",
    "    true_classes = np.array([4]*len(predicted_classes))\n",
    "    acc_p4_s = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_4)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_4, axis=-1)\n",
    "    acc_p4_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p5\n",
    "    ## speaker classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_5)[0]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)+1\n",
    "    true_classes = np.array([5]*len(predicted_classes))\n",
    "    acc_p5_s = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_5)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_5, axis=-1)\n",
    "    acc_p5_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p6 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_6)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class_6\n",
    "    acc_p6 = metrics.accuracy_score(true_classes, predicted_classes).round(4)                \n",
    "\n",
    "    # test on p7 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_7)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class_7\n",
    "    acc_p7 = metrics.accuracy_score(true_classes, predicted_classes).round(4)    \n",
    "\n",
    "    # test on p8 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_8)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class_8\n",
    "    acc_p8 = metrics.accuracy_score(true_classes, predicted_classes).round(4)    \n",
    "\n",
    "    Perfomance = Perfomance.append({'Model': \"Group\",'Size':'mix_20%&s2_60%','Time':run_time,\n",
    "                                    'Partcp_Acc_p15':acc_p15_s,'Command_Acc_p15':acc_p15_c,'Partcp_Acc_p1':acc_p1_s,\n",
    "                                    'Command_Acc_p1':acc_p1_c,'Partcp_Acc_p2':acc_p2_s,'Command_Acc_p2':acc_p2_c,\n",
    "                                    'Partcp_Acc_p3':acc_p3_s,'Command_Acc_p3':acc_p3_c,'Partcp_Acc_p4':acc_p4_s,\n",
    "                                    'Command_Acc_p4':acc_p4_c,'Partcp_Acc_p5':acc_p5_s,'Command_Acc_p5':acc_p5_c,\n",
    "                                    'Acc_p6':acc_p6,'Acc_p7':acc_p7,'Acc_p8':acc_p8}, ignore_index=True)\n",
    "\n",
    "\n",
    "    if high_acc < acc_p15_c + acc_p15_s:\n",
    "        resnet_model.save('Initial_group_model_mix_20p_s2(60p)_0608.h5')\n",
    "        high_acc = acc_p15_c + acc_p15_s\n",
    "        best_index = run\n",
    "        pd.DataFrame.from_dict(history1.history).to_csv('mix_20p_s2(60p)_history1_0608.csv',index=False)\n",
    "        pd.DataFrame.from_dict(history2.history).to_csv('mix_20p_s2(60p)_history2_0608.csv',index=False)\n",
    "        \n",
    "    del resnet_model\n",
    "    run = run + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "0.9926 0.9705\n"
     ]
    }
   ],
   "source": [
    "resnet_model = tf.keras.models.load_model('Initial_group_model_mix_20p_s2(60p)_0608.h5')\n",
    "predictions = resnet_model.predict(Test_Inputs)[0]\n",
    "predicted_classes = np.argmax(predictions, axis=1)+1\n",
    "acc_c = round(sum(x == y for x, y in zip(test_unit_participant_class, predicted_classes)) / len(test_unit_participant_class),4)\n",
    "\n",
    "predictions = resnet_model.predict(Test_Inputs)[1]\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "acc_s = round(sum(x == y for x, y in zip(test_unit_command_class, predicted_classes)) / len(test_unit_command_class),4)\n",
    "overall_acc = acc_c + acc_s\n",
    "print(acc_c,acc_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Size</th>\n",
       "      <th>Time</th>\n",
       "      <th>Partcp_Acc_p15</th>\n",
       "      <th>Command_Acc_p15</th>\n",
       "      <th>Partcp_Acc_p1</th>\n",
       "      <th>Command_Acc_p1</th>\n",
       "      <th>Partcp_Acc_p2</th>\n",
       "      <th>Command_Acc_p2</th>\n",
       "      <th>Partcp_Acc_p3</th>\n",
       "      <th>Command_Acc_p3</th>\n",
       "      <th>Partcp_Acc_p4</th>\n",
       "      <th>Command_Acc_p4</th>\n",
       "      <th>Partcp_Acc_p5</th>\n",
       "      <th>Command_Acc_p5</th>\n",
       "      <th>Acc_p6</th>\n",
       "      <th>Acc_p7</th>\n",
       "      <th>Acc_p8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>47.540156</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.9484</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.3366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>48.307249</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>46.201615</td>\n",
       "      <td>0.9613</td>\n",
       "      <td>0.9448</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9358</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>43.692362</td>\n",
       "      <td>0.9650</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9358</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>44.504580</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>0.9484</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>44.229166</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>44.535541</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>42.948731</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.2574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>45.444911</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.9503</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>42.707019</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.4059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>67.289367</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9640</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>64.328163</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>61.978212</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>60.036626</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>58.215136</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>49.347837</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>59.879732</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.3366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>57.413757</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9640</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>50.219540</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>53.871787</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>80.966223</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>77.376384</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.2178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>79.340686</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>63.791425</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>78.678261</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.2376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>71.382334</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>76.130331</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9853</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>70.832085</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>73.439023</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.2277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>72.148432</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>47.769887</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>49.705597</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.9576</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>44.363904</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9576</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9478</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.2376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>46.206470</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9650</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>47.192904</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>47.685570</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.9558</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>44.867667</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>44.795298</td>\n",
       "      <td>0.9853</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>40.871670</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.9558</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.9391</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>42.479896</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>51.959552</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>52.339809</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9478</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9358</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>53.653035</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.8899</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>49.510542</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9640</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9304</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>49.548754</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>53.723940</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9391</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>50.595989</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9174</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>48.617480</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.2475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>47.476377</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>48.240296</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_60%</td>\n",
       "      <td>50.357350</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_60%</td>\n",
       "      <td>52.448013</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.3465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_60%</td>\n",
       "      <td>46.477153</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.9650</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9358</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.3663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_60%</td>\n",
       "      <td>50.288148</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.9503</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9099</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9391</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_60%</td>\n",
       "      <td>51.380808</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9189</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_60%</td>\n",
       "      <td>51.368167</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9099</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_60%</td>\n",
       "      <td>49.387344</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9478</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_60%</td>\n",
       "      <td>43.176742</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9358</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_60%</td>\n",
       "      <td>48.327896</td>\n",
       "      <td>0.9853</td>\n",
       "      <td>0.9558</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9391</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_60%</td>\n",
       "      <td>47.509030</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model             Size       Time  Partcp_Acc_p15  Command_Acc_p15  \\\n",
       "0   Group              20%  47.540156          0.9724           0.9484   \n",
       "1   Group              20%  48.307249          0.9779           0.9521   \n",
       "2   Group              20%  46.201615          0.9613           0.9448   \n",
       "3   Group              20%  43.692362          0.9650           0.9521   \n",
       "4   Group              20%  44.504580          0.9687           0.9484   \n",
       "5   Group              20%  44.229166          0.9705           0.9521   \n",
       "6   Group              20%  44.535541          0.9632           0.9540   \n",
       "7   Group              20%  42.948731          0.9724           0.9595   \n",
       "8   Group              20%  45.444911          0.9724           0.9503   \n",
       "9   Group              20%  42.707019          0.9669           0.9540   \n",
       "10  Group              40%  67.289367          0.9945           0.9761   \n",
       "11  Group              40%  64.328163          0.9890           0.9761   \n",
       "12  Group              40%  61.978212          0.9926           0.9797   \n",
       "13  Group              40%  60.036626          0.9926           0.9779   \n",
       "14  Group              40%  58.215136          0.9890           0.9761   \n",
       "15  Group              40%  49.347837          0.9871           0.9761   \n",
       "16  Group              40%  59.879732          0.9908           0.9779   \n",
       "17  Group              40%  57.413757          0.9908           0.9761   \n",
       "18  Group              40%  50.219540          0.9908           0.9761   \n",
       "19  Group              40%  53.871787          0.9945           0.9742   \n",
       "20  Group              60%  80.966223          0.9926           0.9871   \n",
       "21  Group              60%  77.376384          0.9926           0.9797   \n",
       "22  Group              60%  79.340686          0.9945           0.9816   \n",
       "23  Group              60%  63.791425          0.9890           0.9871   \n",
       "24  Group              60%  78.678261          0.9926           0.9797   \n",
       "25  Group              60%  71.382334          0.9945           0.9834   \n",
       "26  Group              60%  76.130331          0.9908           0.9853   \n",
       "27  Group              60%  70.832085          0.9945           0.9871   \n",
       "28  Group              60%  73.439023          0.9945           0.9890   \n",
       "29  Group              60%  72.148432          0.9945           0.9834   \n",
       "30  Group   mix_20%&s2_40%  47.769887          0.9834           0.9669   \n",
       "31  Group   mix_20%&s2_40%  49.705597          0.9816           0.9576   \n",
       "32  Group   mix_20%&s2_40%  44.363904          0.9797           0.9576   \n",
       "33  Group   mix_20%&s2_40%  46.206470          0.9945           0.9650   \n",
       "34  Group   mix_20%&s2_40%  47.192904          0.9797           0.9632   \n",
       "35  Group   mix_20%&s2_40%  47.685570          0.9761           0.9558   \n",
       "36  Group   mix_20%&s2_40%  44.867667          0.9816           0.9687   \n",
       "37  Group   mix_20%&s2_40%  44.795298          0.9853           0.9724   \n",
       "38  Group   mix_20%&s2_40%  40.871670          0.9742           0.9558   \n",
       "39  Group   mix_20%&s2_40%  42.479896          0.9834           0.9687   \n",
       "40  Group  mix_20%&s12_40%  51.959552          0.9834           0.9687   \n",
       "41  Group  mix_20%&s12_40%  52.339809          0.9705           0.9669   \n",
       "42  Group  mix_20%&s12_40%  53.653035          0.9724           0.9705   \n",
       "43  Group  mix_20%&s12_40%  49.510542          0.9705           0.9669   \n",
       "44  Group  mix_20%&s12_40%  49.548754          0.9834           0.9669   \n",
       "45  Group  mix_20%&s12_40%  53.723940          0.9761           0.9669   \n",
       "46  Group  mix_20%&s12_40%  50.595989          0.9761           0.9595   \n",
       "47  Group  mix_20%&s12_40%  48.617480          0.9797           0.9632   \n",
       "48  Group  mix_20%&s12_40%  47.476377          0.9834           0.9705   \n",
       "49  Group  mix_20%&s12_40%  48.240296          0.9797           0.9687   \n",
       "50  Group   mix_20%&s2_60%  50.357350          0.9926           0.9705   \n",
       "51  Group   mix_20%&s2_60%  52.448013          0.9871           0.9669   \n",
       "52  Group   mix_20%&s2_60%  46.477153          0.9705           0.9650   \n",
       "53  Group   mix_20%&s2_60%  50.288148          0.9871           0.9503   \n",
       "54  Group   mix_20%&s2_60%  51.380808          0.9890           0.9595   \n",
       "55  Group   mix_20%&s2_60%  51.368167          0.9908           0.9595   \n",
       "56  Group   mix_20%&s2_60%  49.387344          0.9816           0.9595   \n",
       "57  Group   mix_20%&s2_60%  43.176742          0.9816           0.9669   \n",
       "58  Group   mix_20%&s2_60%  48.327896          0.9853           0.9558   \n",
       "59  Group   mix_20%&s2_60%  47.509030          0.9816           0.9669   \n",
       "\n",
       "    Partcp_Acc_p1  Command_Acc_p1  Partcp_Acc_p2  Command_Acc_p2  \\\n",
       "0           0.991          0.9550         0.9259          0.8796   \n",
       "1           1.000          0.9369         0.9352          0.8796   \n",
       "2           0.991          0.9369         0.9074          0.8611   \n",
       "3           1.000          0.9459         0.9074          0.8704   \n",
       "4           0.991          0.9550         0.9259          0.8796   \n",
       "5           1.000          0.9550         0.9074          0.8796   \n",
       "6           0.991          0.9369         0.9259          0.8981   \n",
       "7           1.000          0.9550         0.9074          0.8889   \n",
       "8           0.991          0.9459         0.9167          0.8796   \n",
       "9           0.991          0.9459         0.9167          0.8704   \n",
       "10          0.991          0.9640         0.9907          0.9352   \n",
       "11          0.982          0.9730         0.9815          0.9259   \n",
       "12          0.991          0.9730         0.9907          0.9444   \n",
       "13          0.991          0.9730         0.9907          0.9352   \n",
       "14          0.982          0.9730         0.9815          0.9352   \n",
       "15          0.982          0.9730         0.9815          0.9352   \n",
       "16          0.991          0.9730         0.9815          0.9444   \n",
       "17          0.991          0.9640         0.9815          0.9444   \n",
       "18          0.991          0.9730         0.9815          0.9352   \n",
       "19          1.000          0.9730         0.9907          0.9259   \n",
       "20          0.991          0.9820         0.9815          0.9722   \n",
       "21          0.982          0.9820         1.0000          0.9537   \n",
       "22          0.982          0.9820         1.0000          0.9444   \n",
       "23          0.973          0.9910         0.9815          0.9630   \n",
       "24          0.982          0.9820         1.0000          0.9537   \n",
       "25          0.982          0.9820         1.0000          0.9537   \n",
       "26          0.982          0.9910         0.9907          0.9537   \n",
       "27          0.991          0.9820         0.9907          0.9722   \n",
       "28          0.982          0.9910         1.0000          0.9722   \n",
       "29          0.982          0.9910         1.0000          0.9444   \n",
       "30          0.991          0.9459         0.9815          0.9352   \n",
       "31          0.991          0.9369         1.0000          0.9259   \n",
       "32          0.991          0.9550         0.9815          0.9259   \n",
       "33          0.991          0.9550         1.0000          0.9259   \n",
       "34          0.982          0.9369         0.9907          0.9444   \n",
       "35          0.991          0.9279         0.9907          0.9352   \n",
       "36          0.991          0.9550         1.0000          0.9537   \n",
       "37          1.000          0.9550         1.0000          0.9630   \n",
       "38          1.000          0.9459         0.9815          0.9167   \n",
       "39          0.982          0.9550         1.0000          0.9352   \n",
       "40          1.000          0.9730         0.9907          0.9444   \n",
       "41          1.000          0.9730         0.9907          0.9444   \n",
       "42          1.000          0.9910         0.9907          0.9259   \n",
       "43          0.991          0.9640         0.9907          0.9444   \n",
       "44          1.000          0.9550         0.9907          0.9444   \n",
       "45          0.991          0.9820         0.9907          0.9444   \n",
       "46          1.000          0.9550         0.9815          0.9352   \n",
       "47          0.991          0.9730         0.9907          0.9352   \n",
       "48          0.991          0.9910         0.9815          0.9259   \n",
       "49          1.000          0.9730         0.9907          0.9444   \n",
       "50          0.991          0.9369         1.0000          0.9722   \n",
       "51          0.991          0.9459         1.0000          0.9630   \n",
       "52          0.991          0.9459         1.0000          0.9537   \n",
       "53          1.000          0.9099         1.0000          0.9537   \n",
       "54          1.000          0.9189         1.0000          0.9537   \n",
       "55          0.991          0.9099         1.0000          0.9537   \n",
       "56          0.991          0.9369         1.0000          0.9630   \n",
       "57          1.000          0.9459         1.0000          0.9537   \n",
       "58          0.991          0.9279         1.0000          0.9537   \n",
       "59          0.991          0.9459         1.0000          0.9630   \n",
       "\n",
       "    Partcp_Acc_p3  Command_Acc_p3  Partcp_Acc_p4  Command_Acc_p4  \\\n",
       "0          0.9652          0.9565         0.9817          0.9633   \n",
       "1          0.9739          0.9652         0.9908          0.9908   \n",
       "2          0.9826          0.9652         0.9358          0.9725   \n",
       "3          0.9826          0.9652         0.9358          0.9908   \n",
       "4          0.9739          0.9652         0.9541          0.9541   \n",
       "5          0.9826          0.9739         0.9633          0.9633   \n",
       "6          0.9565          0.9652         0.9541          0.9817   \n",
       "7          0.9913          0.9739         0.9633          0.9908   \n",
       "8          0.9913          0.9652         0.9633          0.9725   \n",
       "9          0.9739          0.9652         0.9541          0.9908   \n",
       "10         1.0000          0.9913         0.9908          0.9908   \n",
       "11         0.9913          0.9913         0.9908          0.9908   \n",
       "12         0.9913          0.9913         0.9908          0.9908   \n",
       "13         0.9913          0.9913         0.9908          0.9908   \n",
       "14         0.9913          0.9826         0.9908          0.9908   \n",
       "15         0.9913          0.9913         0.9817          0.9908   \n",
       "16         0.9913          0.9826         0.9908          0.9908   \n",
       "17         0.9913          0.9826         0.9908          0.9908   \n",
       "18         0.9913          0.9913         0.9908          0.9908   \n",
       "19         0.9913          0.9913         0.9908          0.9817   \n",
       "20         1.0000          0.9913         0.9908          0.9908   \n",
       "21         0.9913          0.9826         0.9908          0.9817   \n",
       "22         1.0000          0.9913         0.9908          0.9908   \n",
       "23         1.0000          0.9913         0.9908          0.9908   \n",
       "24         0.9913          0.9826         0.9908          0.9817   \n",
       "25         1.0000          0.9913         0.9908          0.9908   \n",
       "26         0.9913          0.9913         0.9908          0.9908   \n",
       "27         1.0000          0.9913         0.9908          0.9908   \n",
       "28         1.0000          0.9913         0.9908          0.9908   \n",
       "29         1.0000          0.9913         0.9908          0.9908   \n",
       "30         0.9739          0.9826         0.9725          0.9817   \n",
       "31         0.9739          0.9652         0.9450          0.9817   \n",
       "32         0.9652          0.9478         0.9908          0.9725   \n",
       "33         0.9913          0.9652         0.9908          0.9908   \n",
       "34         0.9739          0.9652         0.9541          0.9817   \n",
       "35         0.9565          0.9565         0.9633          0.9817   \n",
       "36         0.9652          0.9652         0.9541          0.9817   \n",
       "37         0.9652          0.9739         0.9633          0.9817   \n",
       "38         0.9391          0.9565         0.9541          0.9725   \n",
       "39         0.9739          0.9826         0.9633          0.9817   \n",
       "40         0.9826          0.9565         0.9541          0.9817   \n",
       "41         0.9478          0.9652         0.9358          0.9725   \n",
       "42         0.9826          0.9652         0.8899          0.9817   \n",
       "43         0.9304          0.9565         0.9541          0.9725   \n",
       "44         0.9826          0.9652         0.9450          0.9817   \n",
       "45         0.9739          0.9391         0.9450          0.9817   \n",
       "46         0.9826          0.9565         0.9174          0.9633   \n",
       "47         0.9652          0.9565         0.9541          0.9725   \n",
       "48         0.9739          0.9739         0.9725          0.9817   \n",
       "49         0.9652          0.9565         0.9450          0.9817   \n",
       "50         1.0000          0.9652         0.9725          0.9908   \n",
       "51         0.9913          0.9652         0.9633          0.9817   \n",
       "52         0.9652          0.9652         0.9358          0.9725   \n",
       "53         0.9826          0.9391         0.9633          0.9725   \n",
       "54         0.9739          0.9652         0.9817          0.9817   \n",
       "55         0.9913          0.9652         0.9817          0.9817   \n",
       "56         0.9652          0.9478         0.9633          0.9725   \n",
       "57         0.9739          0.9652         0.9358          0.9817   \n",
       "58         0.9826          0.9391         0.9633          0.9817   \n",
       "59         0.9739          0.9652         0.9450          0.9817   \n",
       "\n",
       "    Partcp_Acc_p5  Command_Acc_p5  Acc_p6  Acc_p7  Acc_p8  \n",
       "0            1.00            0.99    0.67   0.528  0.3366  \n",
       "1            0.99            0.99    0.59   0.528  0.2970  \n",
       "2            0.99            0.99    0.67   0.432  0.2673  \n",
       "3            1.00            0.99    0.62   0.504  0.2673  \n",
       "4            1.00            0.99    0.59   0.472  0.3267  \n",
       "5            1.00            0.99    0.62   0.456  0.3069  \n",
       "6            0.99            0.99    0.67   0.496  0.3267  \n",
       "7            1.00            0.99    0.67   0.496  0.2574  \n",
       "8            1.00            0.99    0.60   0.552  0.2673  \n",
       "9            1.00            1.00    0.66   0.480  0.4059  \n",
       "10           1.00            1.00    0.65   0.472  0.2772  \n",
       "11           1.00            1.00    0.63   0.480  0.2871  \n",
       "12           1.00            1.00    0.69   0.400  0.3168  \n",
       "13           1.00            1.00    0.70   0.456  0.3267  \n",
       "14           1.00            1.00    0.70   0.448  0.2871  \n",
       "15           1.00            0.99    0.70   0.472  0.2772  \n",
       "16           1.00            1.00    0.70   0.440  0.3366  \n",
       "17           1.00            1.00    0.67   0.440  0.3168  \n",
       "18           1.00            0.99    0.63   0.456  0.2772  \n",
       "19           1.00            1.00    0.65   0.424  0.2673  \n",
       "20           1.00            1.00    0.69   0.440  0.2772  \n",
       "21           1.00            1.00    0.72   0.456  0.2178  \n",
       "22           1.00            1.00    0.68   0.440  0.2970  \n",
       "23           1.00            1.00    0.68   0.424  0.3069  \n",
       "24           1.00            1.00    0.68   0.424  0.2376  \n",
       "25           1.00            1.00    0.66   0.408  0.2475  \n",
       "26           1.00            1.00    0.73   0.416  0.2673  \n",
       "27           1.00            1.00    0.70   0.440  0.2673  \n",
       "28           1.00            1.00    0.70   0.424  0.2277  \n",
       "29           1.00            1.00    0.70   0.416  0.2772  \n",
       "30           1.00            0.99    0.69   0.552  0.3267  \n",
       "31           1.00            0.98    0.69   0.512  0.3168  \n",
       "32           0.97            0.99    0.72   0.536  0.2376  \n",
       "33           1.00            0.99    0.70   0.480  0.2673  \n",
       "34           1.00            0.99    0.69   0.504  0.2871  \n",
       "35           0.98            0.98    0.72   0.472  0.2871  \n",
       "36           1.00            0.99    0.66   0.496  0.2673  \n",
       "37           1.00            0.99    0.68   0.544  0.3069  \n",
       "38           1.00            0.99    0.71   0.480  0.2970  \n",
       "39           1.00            0.99    0.73   0.536  0.2673  \n",
       "40           0.99            0.99    0.66   0.504  0.3168  \n",
       "41           0.98            0.98    0.67   0.424  0.2673  \n",
       "42           1.00            0.99    0.67   0.544  0.3168  \n",
       "43           0.99            1.00    0.67   0.464  0.3168  \n",
       "44           1.00            0.99    0.65   0.488  0.2970  \n",
       "45           0.98            0.99    0.66   0.488  0.3069  \n",
       "46           1.00            0.99    0.64   0.456  0.3069  \n",
       "47           1.00            0.98    0.65   0.480  0.2475  \n",
       "48           1.00            0.98    0.59   0.472  0.3168  \n",
       "49           1.00            0.99    0.69   0.496  0.3069  \n",
       "50           1.00            0.99    0.65   0.512  0.3069  \n",
       "51           0.99            0.98    0.68   0.496  0.3465  \n",
       "52           0.96            0.99    0.63   0.496  0.3663  \n",
       "53           0.99            0.98    0.66   0.560  0.3168  \n",
       "54           0.99            0.98    0.74   0.520  0.2772  \n",
       "55           0.99            0.99    0.68   0.504  0.2871  \n",
       "56           0.99            0.98    0.70   0.536  0.2970  \n",
       "57           1.00            0.99    0.67   0.480  0.2871  \n",
       "58           0.99            0.98    0.65   0.512  0.2970  \n",
       "59           1.00            0.98    0.70   0.512  0.2772  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perfomance.to_csv('Performance_0608_training_Data_Size.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
