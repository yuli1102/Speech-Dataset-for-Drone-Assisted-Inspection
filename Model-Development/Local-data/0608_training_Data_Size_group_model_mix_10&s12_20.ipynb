{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"   #(xxxx is your specific GPU ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from MyEarlyStopping import MyEarlyStopping\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# from keras.optimizers import adam\n",
    "\n",
    "import timeit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import LabelEncoder  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_number = 5\n",
    "train_image = 10 #10:20%, 20: 40%, 30:60%\n",
    "train_image_s12 = 20 #10:20%, 20: 40%, 30:60%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training dataset (40%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1714 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "train_generator = train_data.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_Data/train',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = train_generator.filenames\n",
    "image_no = [i.split(\"/\")[1].split(\"_\")[2].split(\".\")[0] for i in image_names]\n",
    "image_no = np.array(list(map(int, image_no)))\n",
    "ALL_participant_class = [i.split(\"/\")[1].split(\"_\")[1] for i in image_names]\n",
    "ALL_participant_class = np.array(list(map(int, ALL_participant_class)))\n",
    "command_class = train_generator.classes\n",
    "All_participant_class = tf.keras.utils.to_categorical(ALL_participant_class-1, num_classes=train_number)\n",
    "All_command_class = tf.keras.utils.to_categorical(command_class, num_classes=10)\n",
    "All_command_uniform = All_command_class*0+1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Inputs = [next(train_generator)[0][0] for _ in range(len(train_generator))]\n",
    "All_Inputs = np.array(All_Inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([200., 200., 100., 100., 100.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_indexs_train = ((image_no<train_image)&(ALL_participant_class>2))|((image_no<train_image_s12)&(ALL_participant_class<=2))\n",
    "Train_Inputs = All_Inputs[select_indexs_train]\n",
    "Train_participant_class = All_participant_class[select_indexs_train]\n",
    "Train_participant_uniform = Train_participant_class*0+1/train_number\n",
    "Train_command_class = All_command_class[select_indexs_train]\n",
    "Train_command_uniform = Train_command_class*0+1/10\n",
    "sum(Train_participant_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 543 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_generator = val_data.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_Data/validation',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = val_generator.filenames\n",
    "participant_class = [i.split(\"/\", 1)[1].split(\"_\")[1] for i in image_names]\n",
    "participant_class = np.array(list(map(int, participant_class)))\n",
    "command_class = val_generator.classes\n",
    "Val_participant_class = tf.keras.utils.to_categorical(participant_class-1, num_classes=train_number)\n",
    "Val_participant_uniform = Val_participant_class*0+1/train_number\n",
    "Val_command_class = tf.keras.utils.to_categorical(command_class, num_classes=10)\n",
    "Val_command_uniform = Val_command_class*0+1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Val_Inputs = [next(val_generator)[0][0] for _ in range(len(val_generator))]\n",
    "Val_Inputs = np.array(Val_Inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 543 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator = test_data.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_Data/test',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = test_generator.filenames\n",
    "participant_class = [i.split(\"/\", 1)[1].split(\"_\")[1] for i in image_names]\n",
    "test_unit_participant_class = np.array(list(map(int, participant_class)))\n",
    "test_unit_command_class = test_generator.classes\n",
    "Test_participant_class = tf.keras.utils.to_categorical(test_unit_participant_class-1, num_classes=train_number)\n",
    "Test_participant_uniform = Test_participant_class*0+1/train_number\n",
    "Test_command_class = tf.keras.utils.to_categorical(test_unit_command_class, num_classes=10)\n",
    "Test_command_uniform = Test_command_class*0+1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Inputs = [next(test_generator)[0][0] for _ in range(len(test_generator))]\n",
    "Test_Inputs = np.array(Test_Inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Inputs_1 = Test_Inputs[np.where(test_unit_participant_class == 1)]\n",
    "Test_command_class_1 = Test_command_class[np.where(test_unit_participant_class == 1)]\n",
    "Test_Inputs_2 = Test_Inputs[np.where(test_unit_participant_class == 2)]\n",
    "Test_command_class_2 = Test_command_class[np.where(test_unit_participant_class == 2)]\n",
    "Test_Inputs_3 = Test_Inputs[np.where(test_unit_participant_class == 3)]\n",
    "Test_command_class_3 = Test_command_class[np.where(test_unit_participant_class == 3)]\n",
    "Test_Inputs_4 = Test_Inputs[np.where(test_unit_participant_class == 4)]\n",
    "Test_command_class_4 = Test_command_class[np.where(test_unit_participant_class == 4)]\n",
    "Test_Inputs_5 = Test_Inputs[np.where(test_unit_participant_class == 5)]\n",
    "Test_command_class_5 = Test_command_class[np.where(test_unit_participant_class == 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker 6 Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_6 = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator_6 = test_data_6.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_subject/p_6_split/test',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = test_generator_6.filenames\n",
    "command_class = [i.split(\"/\", 1)[0]for i in image_names]\n",
    "le = LabelEncoder()\n",
    "test_unit_command_class_6 = le.fit_transform(command_class)\n",
    "Test_command_class_6 = tf.keras.utils.to_categorical(test_unit_command_class_6, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Inputs_6 = [next(test_generator_6)[0][0] for _ in range(len(test_generator_6))]\n",
    "Test_Inputs_6 = np.array(Test_Inputs_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker 7 Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_7 = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator_7 = test_data_6.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_subject/p_7_split/test',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = test_generator_7.filenames\n",
    "command_class = [i.split(\"/\", 1)[0]for i in image_names]\n",
    "le = LabelEncoder()\n",
    "test_unit_command_class_7 = le.fit_transform(command_class)\n",
    "Test_command_class_7 = tf.keras.utils.to_categorical(test_unit_command_class_7, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Inputs_7 = [next(test_generator_7)[0][0] for _ in range(len(test_generator_7))]\n",
    "Test_Inputs_7 = np.array(Test_Inputs_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker 8 Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 101 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_8 = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator_8 = test_data_6.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_subject/p_8_split/test',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = test_generator_8.filenames\n",
    "command_class = [i.split(\"/\", 1)[0]for i in image_names]\n",
    "le = LabelEncoder()\n",
    "test_unit_command_class_8 = le.fit_transform(command_class)\n",
    "Test_command_class_8 = tf.keras.utils.to_categorical(test_unit_command_class_8, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Inputs_8 = [next(test_generator_8)[0][0] for _ in range(len(test_generator_8))]\n",
    "Test_Inputs_8 = np.array(Test_Inputs_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pd file store performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Size</th>\n",
       "      <th>Time</th>\n",
       "      <th>Partcp_Acc_p15</th>\n",
       "      <th>Command_Acc_p15</th>\n",
       "      <th>Partcp_Acc_p1</th>\n",
       "      <th>Command_Acc_p1</th>\n",
       "      <th>Partcp_Acc_p2</th>\n",
       "      <th>Command_Acc_p2</th>\n",
       "      <th>Partcp_Acc_p3</th>\n",
       "      <th>Command_Acc_p3</th>\n",
       "      <th>Partcp_Acc_p4</th>\n",
       "      <th>Command_Acc_p4</th>\n",
       "      <th>Partcp_Acc_p5</th>\n",
       "      <th>Command_Acc_p5</th>\n",
       "      <th>Acc_p6</th>\n",
       "      <th>Acc_p7</th>\n",
       "      <th>Acc_p8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>47.540156</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.9484</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.3366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>48.307249</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>46.201615</td>\n",
       "      <td>0.9613</td>\n",
       "      <td>0.9448</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9358</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>43.692362</td>\n",
       "      <td>0.9650</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9358</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>44.504580</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>0.9484</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>44.229166</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>44.535541</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>42.948731</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.2574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>45.444911</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.9503</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>42.707019</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.4059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>67.289367</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9640</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>64.328163</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>61.978212</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>60.036626</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>58.215136</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>49.347837</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>59.879732</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.3366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>57.413757</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9640</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>50.219540</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>53.871787</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>80.966223</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>77.376384</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.2178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>79.340686</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>63.791425</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>78.678261</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.2376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>71.382334</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>76.130331</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9853</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>70.832085</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>73.439023</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.2277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>72.148432</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>47.769887</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>49.705597</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.9576</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>44.363904</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9576</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9478</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.2376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>46.206470</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9650</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>47.192904</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>47.685570</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.9558</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>44.867667</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>44.795298</td>\n",
       "      <td>0.9853</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>40.871670</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.9558</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.9391</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>42.479896</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model            Size       Time  Partcp_Acc_p15  Command_Acc_p15  \\\n",
       "0   Group             20%  47.540156          0.9724           0.9484   \n",
       "1   Group             20%  48.307249          0.9779           0.9521   \n",
       "2   Group             20%  46.201615          0.9613           0.9448   \n",
       "3   Group             20%  43.692362          0.9650           0.9521   \n",
       "4   Group             20%  44.504580          0.9687           0.9484   \n",
       "5   Group             20%  44.229166          0.9705           0.9521   \n",
       "6   Group             20%  44.535541          0.9632           0.9540   \n",
       "7   Group             20%  42.948731          0.9724           0.9595   \n",
       "8   Group             20%  45.444911          0.9724           0.9503   \n",
       "9   Group             20%  42.707019          0.9669           0.9540   \n",
       "10  Group             40%  67.289367          0.9945           0.9761   \n",
       "11  Group             40%  64.328163          0.9890           0.9761   \n",
       "12  Group             40%  61.978212          0.9926           0.9797   \n",
       "13  Group             40%  60.036626          0.9926           0.9779   \n",
       "14  Group             40%  58.215136          0.9890           0.9761   \n",
       "15  Group             40%  49.347837          0.9871           0.9761   \n",
       "16  Group             40%  59.879732          0.9908           0.9779   \n",
       "17  Group             40%  57.413757          0.9908           0.9761   \n",
       "18  Group             40%  50.219540          0.9908           0.9761   \n",
       "19  Group             40%  53.871787          0.9945           0.9742   \n",
       "20  Group             60%  80.966223          0.9926           0.9871   \n",
       "21  Group             60%  77.376384          0.9926           0.9797   \n",
       "22  Group             60%  79.340686          0.9945           0.9816   \n",
       "23  Group             60%  63.791425          0.9890           0.9871   \n",
       "24  Group             60%  78.678261          0.9926           0.9797   \n",
       "25  Group             60%  71.382334          0.9945           0.9834   \n",
       "26  Group             60%  76.130331          0.9908           0.9853   \n",
       "27  Group             60%  70.832085          0.9945           0.9871   \n",
       "28  Group             60%  73.439023          0.9945           0.9890   \n",
       "29  Group             60%  72.148432          0.9945           0.9834   \n",
       "30  Group  mix_20%&s2_40%  47.769887          0.9834           0.9669   \n",
       "31  Group  mix_20%&s2_40%  49.705597          0.9816           0.9576   \n",
       "32  Group  mix_20%&s2_40%  44.363904          0.9797           0.9576   \n",
       "33  Group  mix_20%&s2_40%  46.206470          0.9945           0.9650   \n",
       "34  Group  mix_20%&s2_40%  47.192904          0.9797           0.9632   \n",
       "35  Group  mix_20%&s2_40%  47.685570          0.9761           0.9558   \n",
       "36  Group  mix_20%&s2_40%  44.867667          0.9816           0.9687   \n",
       "37  Group  mix_20%&s2_40%  44.795298          0.9853           0.9724   \n",
       "38  Group  mix_20%&s2_40%  40.871670          0.9742           0.9558   \n",
       "39  Group  mix_20%&s2_40%  42.479896          0.9834           0.9687   \n",
       "\n",
       "    Partcp_Acc_p1  Command_Acc_p1  Partcp_Acc_p2  Command_Acc_p2  \\\n",
       "0           0.991          0.9550         0.9259          0.8796   \n",
       "1           1.000          0.9369         0.9352          0.8796   \n",
       "2           0.991          0.9369         0.9074          0.8611   \n",
       "3           1.000          0.9459         0.9074          0.8704   \n",
       "4           0.991          0.9550         0.9259          0.8796   \n",
       "5           1.000          0.9550         0.9074          0.8796   \n",
       "6           0.991          0.9369         0.9259          0.8981   \n",
       "7           1.000          0.9550         0.9074          0.8889   \n",
       "8           0.991          0.9459         0.9167          0.8796   \n",
       "9           0.991          0.9459         0.9167          0.8704   \n",
       "10          0.991          0.9640         0.9907          0.9352   \n",
       "11          0.982          0.9730         0.9815          0.9259   \n",
       "12          0.991          0.9730         0.9907          0.9444   \n",
       "13          0.991          0.9730         0.9907          0.9352   \n",
       "14          0.982          0.9730         0.9815          0.9352   \n",
       "15          0.982          0.9730         0.9815          0.9352   \n",
       "16          0.991          0.9730         0.9815          0.9444   \n",
       "17          0.991          0.9640         0.9815          0.9444   \n",
       "18          0.991          0.9730         0.9815          0.9352   \n",
       "19          1.000          0.9730         0.9907          0.9259   \n",
       "20          0.991          0.9820         0.9815          0.9722   \n",
       "21          0.982          0.9820         1.0000          0.9537   \n",
       "22          0.982          0.9820         1.0000          0.9444   \n",
       "23          0.973          0.9910         0.9815          0.9630   \n",
       "24          0.982          0.9820         1.0000          0.9537   \n",
       "25          0.982          0.9820         1.0000          0.9537   \n",
       "26          0.982          0.9910         0.9907          0.9537   \n",
       "27          0.991          0.9820         0.9907          0.9722   \n",
       "28          0.982          0.9910         1.0000          0.9722   \n",
       "29          0.982          0.9910         1.0000          0.9444   \n",
       "30          0.991          0.9459         0.9815          0.9352   \n",
       "31          0.991          0.9369         1.0000          0.9259   \n",
       "32          0.991          0.9550         0.9815          0.9259   \n",
       "33          0.991          0.9550         1.0000          0.9259   \n",
       "34          0.982          0.9369         0.9907          0.9444   \n",
       "35          0.991          0.9279         0.9907          0.9352   \n",
       "36          0.991          0.9550         1.0000          0.9537   \n",
       "37          1.000          0.9550         1.0000          0.9630   \n",
       "38          1.000          0.9459         0.9815          0.9167   \n",
       "39          0.982          0.9550         1.0000          0.9352   \n",
       "\n",
       "    Partcp_Acc_p3  Command_Acc_p3  Partcp_Acc_p4  Command_Acc_p4  \\\n",
       "0          0.9652          0.9565         0.9817          0.9633   \n",
       "1          0.9739          0.9652         0.9908          0.9908   \n",
       "2          0.9826          0.9652         0.9358          0.9725   \n",
       "3          0.9826          0.9652         0.9358          0.9908   \n",
       "4          0.9739          0.9652         0.9541          0.9541   \n",
       "5          0.9826          0.9739         0.9633          0.9633   \n",
       "6          0.9565          0.9652         0.9541          0.9817   \n",
       "7          0.9913          0.9739         0.9633          0.9908   \n",
       "8          0.9913          0.9652         0.9633          0.9725   \n",
       "9          0.9739          0.9652         0.9541          0.9908   \n",
       "10         1.0000          0.9913         0.9908          0.9908   \n",
       "11         0.9913          0.9913         0.9908          0.9908   \n",
       "12         0.9913          0.9913         0.9908          0.9908   \n",
       "13         0.9913          0.9913         0.9908          0.9908   \n",
       "14         0.9913          0.9826         0.9908          0.9908   \n",
       "15         0.9913          0.9913         0.9817          0.9908   \n",
       "16         0.9913          0.9826         0.9908          0.9908   \n",
       "17         0.9913          0.9826         0.9908          0.9908   \n",
       "18         0.9913          0.9913         0.9908          0.9908   \n",
       "19         0.9913          0.9913         0.9908          0.9817   \n",
       "20         1.0000          0.9913         0.9908          0.9908   \n",
       "21         0.9913          0.9826         0.9908          0.9817   \n",
       "22         1.0000          0.9913         0.9908          0.9908   \n",
       "23         1.0000          0.9913         0.9908          0.9908   \n",
       "24         0.9913          0.9826         0.9908          0.9817   \n",
       "25         1.0000          0.9913         0.9908          0.9908   \n",
       "26         0.9913          0.9913         0.9908          0.9908   \n",
       "27         1.0000          0.9913         0.9908          0.9908   \n",
       "28         1.0000          0.9913         0.9908          0.9908   \n",
       "29         1.0000          0.9913         0.9908          0.9908   \n",
       "30         0.9739          0.9826         0.9725          0.9817   \n",
       "31         0.9739          0.9652         0.9450          0.9817   \n",
       "32         0.9652          0.9478         0.9908          0.9725   \n",
       "33         0.9913          0.9652         0.9908          0.9908   \n",
       "34         0.9739          0.9652         0.9541          0.9817   \n",
       "35         0.9565          0.9565         0.9633          0.9817   \n",
       "36         0.9652          0.9652         0.9541          0.9817   \n",
       "37         0.9652          0.9739         0.9633          0.9817   \n",
       "38         0.9391          0.9565         0.9541          0.9725   \n",
       "39         0.9739          0.9826         0.9633          0.9817   \n",
       "\n",
       "    Partcp_Acc_p5  Command_Acc_p5  Acc_p6  Acc_p7  Acc_p8  \n",
       "0            1.00            0.99    0.67   0.528  0.3366  \n",
       "1            0.99            0.99    0.59   0.528  0.2970  \n",
       "2            0.99            0.99    0.67   0.432  0.2673  \n",
       "3            1.00            0.99    0.62   0.504  0.2673  \n",
       "4            1.00            0.99    0.59   0.472  0.3267  \n",
       "5            1.00            0.99    0.62   0.456  0.3069  \n",
       "6            0.99            0.99    0.67   0.496  0.3267  \n",
       "7            1.00            0.99    0.67   0.496  0.2574  \n",
       "8            1.00            0.99    0.60   0.552  0.2673  \n",
       "9            1.00            1.00    0.66   0.480  0.4059  \n",
       "10           1.00            1.00    0.65   0.472  0.2772  \n",
       "11           1.00            1.00    0.63   0.480  0.2871  \n",
       "12           1.00            1.00    0.69   0.400  0.3168  \n",
       "13           1.00            1.00    0.70   0.456  0.3267  \n",
       "14           1.00            1.00    0.70   0.448  0.2871  \n",
       "15           1.00            0.99    0.70   0.472  0.2772  \n",
       "16           1.00            1.00    0.70   0.440  0.3366  \n",
       "17           1.00            1.00    0.67   0.440  0.3168  \n",
       "18           1.00            0.99    0.63   0.456  0.2772  \n",
       "19           1.00            1.00    0.65   0.424  0.2673  \n",
       "20           1.00            1.00    0.69   0.440  0.2772  \n",
       "21           1.00            1.00    0.72   0.456  0.2178  \n",
       "22           1.00            1.00    0.68   0.440  0.2970  \n",
       "23           1.00            1.00    0.68   0.424  0.3069  \n",
       "24           1.00            1.00    0.68   0.424  0.2376  \n",
       "25           1.00            1.00    0.66   0.408  0.2475  \n",
       "26           1.00            1.00    0.73   0.416  0.2673  \n",
       "27           1.00            1.00    0.70   0.440  0.2673  \n",
       "28           1.00            1.00    0.70   0.424  0.2277  \n",
       "29           1.00            1.00    0.70   0.416  0.2772  \n",
       "30           1.00            0.99    0.69   0.552  0.3267  \n",
       "31           1.00            0.98    0.69   0.512  0.3168  \n",
       "32           0.97            0.99    0.72   0.536  0.2376  \n",
       "33           1.00            0.99    0.70   0.480  0.2673  \n",
       "34           1.00            0.99    0.69   0.504  0.2871  \n",
       "35           0.98            0.98    0.72   0.472  0.2871  \n",
       "36           1.00            0.99    0.66   0.496  0.2673  \n",
       "37           1.00            0.99    0.68   0.544  0.3069  \n",
       "38           1.00            0.99    0.71   0.480  0.2970  \n",
       "39           1.00            0.99    0.73   0.536  0.2673  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd file store performance\n",
    "# Perfomance = pd.DataFrame(columns = ['Model','Time','Partcp_Acc_p15','Command_Acc_p15','Partcp_Acc_p1','Command_Acc_p1',\n",
    "#                                       'Partcp_Acc_p2','Command_Acc_p2','Partcp_Acc_p3','Command_Acc_p3',\n",
    "#                                       'Partcp_Acc_p4','Command_Acc_p4','Partcp_Acc_p5','Command_Acc_p5','Acc_p6','Acc_p7','Acc_p8'])\n",
    "\n",
    "Perfomance = pd.read_csv('Performance_0608_training_Data_Size.csv')\n",
    "# Perfomance\n",
    "Perfomance                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "actv_fun_1_1 = \"relu\" \n",
    "actv_fun_1_2 = \"sigmoid\"\n",
    "shape_1_1 = 128\n",
    "shape_1_2 = 256\n",
    "actv_fun_2_1 = \"sigmoid\" \n",
    "actv_fun_2_2 = \"sigmoid\"\n",
    "shape_2_1 = 512\n",
    "shape_2_2 = 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 386ms/step - loss: 3.7623 - participant_output_loss: 1.5291 - command_output_loss: 2.2274 - command_output_1_loss: 8.0474e-04 - participant_output_1_loss: 0.0050 - participant_output_accuracy: 0.3786 - command_output_accuracy: 0.2486 - command_output_1_accuracy: 0.2971 - participant_output_1_accuracy: 0.1086 - val_loss: 3.4046 - val_participant_output_loss: 1.3838 - val_command_output_loss: 2.0199 - val_command_output_1_loss: 2.7886e-04 - val_participant_output_1_loss: 6.9281e-04 - val_participant_output_accuracy: 0.4622 - val_command_output_accuracy: 0.5157 - val_command_output_1_accuracy: 0.8343 - val_participant_output_1_accuracy: 0.6796\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 187ms/step - loss: 3.0969 - participant_output_loss: 1.1818 - command_output_loss: 1.9145 - command_output_1_loss: 1.7534e-04 - participant_output_1_loss: 4.6747e-04 - participant_output_accuracy: 0.6629 - command_output_accuracy: 0.6700 - command_output_1_accuracy: 0.8157 - participant_output_1_accuracy: 0.4671 - val_loss: 3.0201 - val_participant_output_loss: 1.1631 - val_command_output_loss: 1.8567 - val_command_output_1_loss: 1.2357e-04 - val_participant_output_1_loss: 9.1262e-05 - val_participant_output_accuracy: 0.6593 - val_command_output_accuracy: 0.7422 - val_command_output_1_accuracy: 0.5912 - val_participant_output_1_accuracy: 0.1529\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 186ms/step - loss: 2.7806 - participant_output_loss: 0.9976 - command_output_loss: 1.7828 - command_output_1_loss: 1.1505e-04 - participant_output_1_loss: 1.2735e-04 - participant_output_accuracy: 0.7514 - command_output_accuracy: 0.8043 - command_output_1_accuracy: 0.1571 - participant_output_1_accuracy: 0.1257 - val_loss: 2.7612 - val_participant_output_loss: 1.0263 - val_command_output_loss: 1.7347 - val_command_output_1_loss: 8.7550e-05 - val_participant_output_1_loss: 7.4220e-05 - val_participant_output_accuracy: 0.7330 - val_command_output_accuracy: 0.8379 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0552\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 188ms/step - loss: 2.4776 - participant_output_loss: 0.8238 - command_output_loss: 1.6537 - command_output_1_loss: 7.1995e-05 - participant_output_1_loss: 6.5741e-05 - participant_output_accuracy: 0.8229 - command_output_accuracy: 0.8786 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2343 - val_loss: 2.5036 - val_participant_output_loss: 0.8823 - val_command_output_loss: 1.6212 - val_command_output_1_loss: 6.0306e-05 - val_participant_output_1_loss: 5.6613e-05 - val_participant_output_accuracy: 0.8048 - val_command_output_accuracy: 0.8895 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1602\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 2.2331 - participant_output_loss: 0.6989 - command_output_loss: 1.5341 - command_output_1_loss: 6.2401e-05 - participant_output_1_loss: 4.7021e-05 - participant_output_accuracy: 0.9014 - command_output_accuracy: 0.9229 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2829 - val_loss: 2.2940 - val_participant_output_loss: 0.7802 - val_command_output_loss: 1.5138 - val_command_output_1_loss: 5.0064e-05 - val_participant_output_1_loss: 4.9893e-05 - val_participant_output_accuracy: 0.8711 - val_command_output_accuracy: 0.9227 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1031\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 185ms/step - loss: 2.0283 - participant_output_loss: 0.6101 - command_output_loss: 1.4180 - command_output_1_loss: 4.5200e-05 - participant_output_1_loss: 3.9697e-05 - participant_output_accuracy: 0.9257 - command_output_accuracy: 0.9386 - command_output_1_accuracy: 0.0071 - participant_output_1_accuracy: 0.2071 - val_loss: 2.1084 - val_participant_output_loss: 0.7053 - val_command_output_loss: 1.4030 - val_command_output_1_loss: 4.0311e-05 - val_participant_output_1_loss: 4.2162e-05 - val_participant_output_accuracy: 0.8877 - val_command_output_accuracy: 0.9337 - val_command_output_1_accuracy: 0.1418 - val_participant_output_1_accuracy: 0.1676\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 146ms/step - loss: 1.8339 - participant_output_loss: 0.5301 - command_output_loss: 1.3036 - command_output_1_loss: 4.3524e-05 - participant_output_1_loss: 3.4389e-05 - participant_output_accuracy: 0.9571 - command_output_accuracy: 0.9686 - command_output_1_accuracy: 0.5686 - participant_output_1_accuracy: 0.2114 - val_loss: 1.9783 - val_participant_output_loss: 0.6761 - val_command_output_loss: 1.3021 - val_command_output_1_loss: 6.6281e-05 - val_participant_output_1_loss: 3.9349e-05 - val_participant_output_accuracy: 0.8748 - val_command_output_accuracy: 0.9392 - val_command_output_1_accuracy: 0.8122 - val_participant_output_1_accuracy: 0.1805\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 1.6616 - participant_output_loss: 0.4628 - command_output_loss: 1.1987 - command_output_1_loss: 4.7161e-05 - participant_output_1_loss: 3.2147e-05 - participant_output_accuracy: 0.9614 - command_output_accuracy: 0.9871 - command_output_1_accuracy: 0.6457 - participant_output_1_accuracy: 0.2100 - val_loss: 1.7309 - val_participant_output_loss: 0.5236 - val_command_output_loss: 1.2072 - val_command_output_1_loss: 3.0228e-05 - val_participant_output_1_loss: 3.9081e-05 - val_participant_output_accuracy: 0.9429 - val_command_output_accuracy: 0.9540 - val_command_output_1_accuracy: 0.0442 - val_participant_output_1_accuracy: 0.1363\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 187ms/step - loss: 1.4724 - participant_output_loss: 0.3782 - command_output_loss: 1.0941 - command_output_1_loss: 4.3050e-05 - participant_output_1_loss: 3.0586e-05 - participant_output_accuracy: 0.9829 - command_output_accuracy: 0.9914 - command_output_1_accuracy: 0.0157 - participant_output_1_accuracy: 0.2457 - val_loss: 1.5591 - val_participant_output_loss: 0.4488 - val_command_output_loss: 1.1102 - val_command_output_1_loss: 5.4965e-05 - val_participant_output_1_loss: 3.8448e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9558 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1215\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 162ms/step - loss: 1.3155 - participant_output_loss: 0.3228 - command_output_loss: 0.9926 - command_output_1_loss: 4.2727e-05 - participant_output_1_loss: 2.9474e-05 - participant_output_accuracy: 0.9871 - command_output_accuracy: 0.9943 - command_output_1_accuracy: 0.0186 - participant_output_1_accuracy: 0.2371 - val_loss: 1.4295 - val_participant_output_loss: 0.4135 - val_command_output_loss: 1.0159 - val_command_output_1_loss: 4.2737e-05 - val_participant_output_1_loss: 3.6836e-05 - val_participant_output_accuracy: 0.9484 - val_command_output_accuracy: 0.9576 - val_command_output_1_accuracy: 0.0552 - val_participant_output_1_accuracy: 0.1565\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 264ms/step - loss: 1.1699 - participant_output_loss: 0.2773 - command_output_loss: 0.8926 - command_output_1_loss: 4.2516e-05 - participant_output_1_loss: 2.8423e-05 - participant_output_accuracy: 0.9943 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.1400 - participant_output_1_accuracy: 0.2271 - val_loss: 1.3027 - val_participant_output_loss: 0.3781 - val_command_output_loss: 0.9244 - val_command_output_1_loss: 3.9433e-05 - val_participant_output_1_loss: 3.6226e-05 - val_participant_output_accuracy: 0.9595 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.2026 - val_participant_output_1_accuracy: 0.1750\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 145ms/step - loss: 1.0472 - participant_output_loss: 0.2469 - command_output_loss: 0.8003 - command_output_1_loss: 3.6204e-05 - participant_output_1_loss: 2.7255e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.1043 - participant_output_1_accuracy: 0.2414 - val_loss: 1.1985 - val_participant_output_loss: 0.3503 - val_command_output_loss: 0.8482 - val_command_output_1_loss: 3.2942e-05 - val_participant_output_1_loss: 3.5433e-05 - val_participant_output_accuracy: 0.9613 - val_command_output_accuracy: 0.9595 - val_command_output_1_accuracy: 0.0516 - val_participant_output_1_accuracy: 0.1768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 202ms/step - loss: 0.9384 - participant_output_loss: 0.2220 - command_output_loss: 0.7163 - command_output_1_loss: 3.5504e-05 - participant_output_1_loss: 2.7040e-05 - participant_output_accuracy: 0.9943 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0257 - participant_output_1_accuracy: 0.2314 - val_loss: 1.1052 - val_participant_output_loss: 0.3384 - val_command_output_loss: 0.7667 - val_command_output_1_loss: 4.1896e-05 - val_participant_output_1_loss: 3.5118e-05 - val_participant_output_accuracy: 0.9595 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.0276 - val_participant_output_1_accuracy: 0.1731\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 0.8360 - participant_output_loss: 0.1968 - command_output_loss: 0.6391 - command_output_1_loss: 3.7823e-05 - participant_output_1_loss: 2.6157e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.1429 - participant_output_1_accuracy: 0.2357 - val_loss: 1.0208 - val_participant_output_loss: 0.3245 - val_command_output_loss: 0.6962 - val_command_output_1_loss: 3.9639e-05 - val_participant_output_1_loss: 3.6201e-05 - val_participant_output_accuracy: 0.9613 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.0460 - val_participant_output_1_accuracy: 0.1565\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 0.7464 - participant_output_loss: 0.1770 - command_output_loss: 0.5693 - command_output_1_loss: 2.7999e-05 - participant_output_1_loss: 2.5732e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0529 - participant_output_1_accuracy: 0.2114 - val_loss: 0.9319 - val_participant_output_loss: 0.2888 - val_command_output_loss: 0.6430 - val_command_output_1_loss: 2.8282e-05 - val_participant_output_1_loss: 3.6601e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.0497 - val_participant_output_1_accuracy: 0.1529\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 215ms/step - loss: 0.6709 - participant_output_loss: 0.1601 - command_output_loss: 0.5108 - command_output_1_loss: 2.7395e-05 - participant_output_1_loss: 2.4804e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0143 - participant_output_1_accuracy: 0.2186 - val_loss: 0.8565 - val_participant_output_loss: 0.2680 - val_command_output_loss: 0.5885 - val_command_output_1_loss: 2.5054e-05 - val_participant_output_1_loss: 3.4312e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.0350 - val_participant_output_1_accuracy: 0.1436\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 208ms/step - loss: 0.6046 - participant_output_loss: 0.1474 - command_output_loss: 0.4572 - command_output_1_loss: 2.3328e-05 - participant_output_1_loss: 2.2976e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0814 - participant_output_1_accuracy: 0.2157 - val_loss: 0.7873 - val_participant_output_loss: 0.2507 - val_command_output_loss: 0.5365 - val_command_output_1_loss: 3.2050e-05 - val_participant_output_1_loss: 3.3434e-05 - val_participant_output_accuracy: 0.9742 - val_command_output_accuracy: 0.9742 - val_command_output_1_accuracy: 0.0424 - val_participant_output_1_accuracy: 0.1657\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 170ms/step - loss: 0.5501 - participant_output_loss: 0.1390 - command_output_loss: 0.4111 - command_output_1_loss: 3.0254e-05 - participant_output_1_loss: 2.2355e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0857 - participant_output_1_accuracy: 0.1971 - val_loss: 0.7388 - val_participant_output_loss: 0.2410 - val_command_output_loss: 0.4977 - val_command_output_1_loss: 2.5922e-05 - val_participant_output_1_loss: 3.2245e-05 - val_participant_output_accuracy: 0.9779 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.2965 - val_participant_output_1_accuracy: 0.1381\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 0.4972 - participant_output_loss: 0.1280 - command_output_loss: 0.3691 - command_output_1_loss: 2.6432e-05 - participant_output_1_loss: 2.0945e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.3429 - participant_output_1_accuracy: 0.2186 - val_loss: 0.6843 - val_participant_output_loss: 0.2282 - val_command_output_loss: 0.4560 - val_command_output_1_loss: 3.1810e-05 - val_participant_output_1_loss: 3.1116e-05 - val_participant_output_accuracy: 0.9724 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.0976 - val_participant_output_1_accuracy: 0.1676\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 168ms/step - loss: 0.4464 - participant_output_loss: 0.1139 - command_output_loss: 0.3324 - command_output_1_loss: 2.1642e-05 - participant_output_1_loss: 2.0257e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1171 - participant_output_1_accuracy: 0.1671 - val_loss: 0.6372 - val_participant_output_loss: 0.2131 - val_command_output_loss: 0.4241 - val_command_output_1_loss: 1.9642e-05 - val_participant_output_1_loss: 3.0659e-05 - val_participant_output_accuracy: 0.9724 - val_command_output_accuracy: 0.9742 - val_command_output_1_accuracy: 0.0571 - val_participant_output_1_accuracy: 0.1602\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 346ms/step - loss: 3.6755 - participant_output_loss: 1.4048 - command_output_loss: 2.2621 - command_output_1_loss: 0.0019 - participant_output_1_loss: 0.0068 - participant_output_accuracy: 0.4914 - command_output_accuracy: 0.2600 - command_output_1_accuracy: 0.1857 - participant_output_1_accuracy: 0.1700 - val_loss: 3.3629 - val_participant_output_loss: 1.2646 - val_command_output_loss: 2.0974 - val_command_output_1_loss: 3.4134e-04 - val_participant_output_1_loss: 5.6609e-04 - val_participant_output_accuracy: 0.5230 - val_command_output_accuracy: 0.5470 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.5230\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 3.0260 - participant_output_loss: 1.0050 - command_output_loss: 2.0205 - command_output_1_loss: 1.1958e-04 - participant_output_1_loss: 3.8106e-04 - participant_output_accuracy: 0.6800 - command_output_accuracy: 0.6514 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.0829 - val_loss: 2.9690 - val_participant_output_loss: 0.9966 - val_command_output_loss: 1.9721 - val_command_output_1_loss: 8.2341e-05 - val_participant_output_1_loss: 2.4474e-04 - val_participant_output_accuracy: 0.6740 - val_command_output_accuracy: 0.6685 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0110\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 215ms/step - loss: 2.7321 - participant_output_loss: 0.8166 - command_output_loss: 1.9154 - command_output_1_loss: 8.9031e-05 - participant_output_1_loss: 1.3271e-04 - participant_output_accuracy: 0.7643 - command_output_accuracy: 0.7986 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.0143 - val_loss: 2.7613 - val_participant_output_loss: 0.8820 - val_command_output_loss: 1.8792 - val_command_output_1_loss: 9.1914e-05 - val_participant_output_1_loss: 1.1243e-04 - val_participant_output_accuracy: 0.7495 - val_command_output_accuracy: 0.8250 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0074\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 213ms/step - loss: 2.4832 - participant_output_loss: 0.6589 - command_output_loss: 1.8242 - command_output_1_loss: 6.0875e-05 - participant_output_1_loss: 5.8214e-05 - participant_output_accuracy: 0.8700 - command_output_accuracy: 0.8714 - command_output_1_accuracy: 0.3314 - participant_output_1_accuracy: 0.1343 - val_loss: 2.5668 - val_participant_output_loss: 0.7703 - val_command_output_loss: 1.7964 - val_command_output_1_loss: 3.1833e-05 - val_participant_output_1_loss: 4.9680e-05 - val_participant_output_accuracy: 0.7753 - val_command_output_accuracy: 0.8545 - val_command_output_1_accuracy: 0.6427 - val_participant_output_1_accuracy: 0.2597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 219ms/step - loss: 2.2912 - participant_output_loss: 0.5591 - command_output_loss: 1.7321 - command_output_1_loss: 2.3982e-05 - participant_output_1_loss: 4.2704e-05 - participant_output_accuracy: 0.9071 - command_output_accuracy: 0.9086 - command_output_1_accuracy: 0.3557 - participant_output_1_accuracy: 0.1071 - val_loss: 2.3772 - val_participant_output_loss: 0.6621 - val_command_output_loss: 1.7150 - val_command_output_1_loss: 3.7964e-05 - val_participant_output_1_loss: 3.7977e-05 - val_participant_output_accuracy: 0.8564 - val_command_output_accuracy: 0.8913 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0368\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 3s 228ms/step - loss: 2.1220 - participant_output_loss: 0.4744 - command_output_loss: 1.6475 - command_output_1_loss: 5.5271e-05 - participant_output_1_loss: 3.0765e-05 - participant_output_accuracy: 0.9386 - command_output_accuracy: 0.9414 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.0900 - val_loss: 2.2076 - val_participant_output_loss: 0.5752 - val_command_output_loss: 1.6323 - val_command_output_1_loss: 4.7480e-05 - val_participant_output_1_loss: 2.7870e-05 - val_participant_output_accuracy: 0.9006 - val_command_output_accuracy: 0.9171 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2044\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 209ms/step - loss: 1.9502 - participant_output_loss: 0.3917 - command_output_loss: 1.5585 - command_output_1_loss: 4.5691e-05 - participant_output_1_loss: 2.4686e-05 - participant_output_accuracy: 0.9629 - command_output_accuracy: 0.9629 - command_output_1_accuracy: 0.2043 - participant_output_1_accuracy: 0.1629 - val_loss: 2.0531 - val_participant_output_loss: 0.5088 - val_command_output_loss: 1.5442 - val_command_output_1_loss: 3.9132e-05 - val_participant_output_1_loss: 2.5209e-05 - val_participant_output_accuracy: 0.9171 - val_command_output_accuracy: 0.9245 - val_command_output_1_accuracy: 0.4843 - val_participant_output_1_accuracy: 0.1344\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 203ms/step - loss: 1.7915 - participant_output_loss: 0.3257 - command_output_loss: 1.4657 - command_output_1_loss: 2.9708e-05 - participant_output_1_loss: 2.4309e-05 - participant_output_accuracy: 0.9786 - command_output_accuracy: 0.9843 - command_output_1_accuracy: 0.1343 - participant_output_1_accuracy: 0.1086 - val_loss: 1.9002 - val_participant_output_loss: 0.4411 - val_command_output_loss: 1.4590 - val_command_output_1_loss: 3.5946e-05 - val_participant_output_1_loss: 2.5340e-05 - val_participant_output_accuracy: 0.9300 - val_command_output_accuracy: 0.9245 - val_command_output_1_accuracy: 0.0276 - val_participant_output_1_accuracy: 0.1326\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 210ms/step - loss: 1.6429 - participant_output_loss: 0.2675 - command_output_loss: 1.3754 - command_output_1_loss: 5.7047e-05 - participant_output_1_loss: 2.3412e-05 - participant_output_accuracy: 0.9857 - command_output_accuracy: 0.9757 - command_output_1_accuracy: 0.0029 - participant_output_1_accuracy: 0.1543 - val_loss: 1.7551 - val_participant_output_loss: 0.3766 - val_command_output_loss: 1.3784 - val_command_output_1_loss: 4.5010e-05 - val_participant_output_1_loss: 2.4738e-05 - val_participant_output_accuracy: 0.9503 - val_command_output_accuracy: 0.9355 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1492\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 224ms/step - loss: 1.5056 - participant_output_loss: 0.2180 - command_output_loss: 1.2876 - command_output_1_loss: 3.7432e-05 - participant_output_1_loss: 2.2797e-05 - participant_output_accuracy: 0.9929 - command_output_accuracy: 0.9886 - command_output_1_accuracy: 0.0257 - participant_output_1_accuracy: 0.1100 - val_loss: 1.6401 - val_participant_output_loss: 0.3408 - val_command_output_loss: 1.2992 - val_command_output_1_loss: 3.3559e-05 - val_participant_output_1_loss: 2.4838e-05 - val_participant_output_accuracy: 0.9576 - val_command_output_accuracy: 0.9595 - val_command_output_1_accuracy: 0.1878 - val_participant_output_1_accuracy: 0.1031\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 1.3902 - participant_output_loss: 0.1855 - command_output_loss: 1.2047 - command_output_1_loss: 5.3460e-05 - participant_output_1_loss: 2.2445e-05 - participant_output_accuracy: 0.9957 - command_output_accuracy: 0.9943 - command_output_1_accuracy: 0.1786 - participant_output_1_accuracy: 0.1114 - val_loss: 1.5280 - val_participant_output_loss: 0.3050 - val_command_output_loss: 1.2229 - val_command_output_1_loss: 4.4204e-05 - val_participant_output_1_loss: 2.4537e-05 - val_participant_output_accuracy: 0.9613 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1271\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 187ms/step - loss: 1.2843 - participant_output_loss: 0.1590 - command_output_loss: 1.1251 - command_output_1_loss: 6.9112e-05 - participant_output_1_loss: 2.2619e-05 - participant_output_accuracy: 0.9986 - command_output_accuracy: 0.9943 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1029 - val_loss: 1.4104 - val_participant_output_loss: 0.2679 - val_command_output_loss: 1.1424 - val_command_output_1_loss: 6.5487e-05 - val_participant_output_1_loss: 2.5253e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1142\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 153ms/step - loss: 1.1741 - participant_output_loss: 0.1337 - command_output_loss: 1.0403 - command_output_1_loss: 6.6006e-05 - participant_output_1_loss: 2.1992e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1429 - val_loss: 1.3226 - val_participant_output_loss: 0.2475 - val_command_output_loss: 1.0750 - val_command_output_1_loss: 5.4060e-05 - val_participant_output_1_loss: 2.4632e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9576 - val_command_output_1_accuracy: 0.0129 - val_participant_output_1_accuracy: 0.1252\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 147ms/step - loss: 1.0753 - participant_output_loss: 0.1159 - command_output_loss: 0.9594 - command_output_1_loss: 6.0658e-05 - participant_output_1_loss: 2.1610e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.4200 - participant_output_1_accuracy: 0.1043 - val_loss: 1.2290 - val_participant_output_loss: 0.2350 - val_command_output_loss: 0.9939 - val_command_output_1_loss: 5.2002e-05 - val_participant_output_1_loss: 2.4174e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.2910 - val_participant_output_1_accuracy: 0.1179\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 0.9862 - participant_output_loss: 0.1034 - command_output_loss: 0.8827 - command_output_1_loss: 3.1032e-05 - participant_output_1_loss: 2.0725e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0500 - participant_output_1_accuracy: 0.1329 - val_loss: 1.1483 - val_participant_output_loss: 0.2186 - val_command_output_loss: 0.9297 - val_command_output_1_loss: 2.5364e-05 - val_participant_output_1_loss: 2.3646e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9724 - val_command_output_1_accuracy: 0.0037 - val_participant_output_1_accuracy: 0.1105\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 214ms/step - loss: 0.9052 - participant_output_loss: 0.0922 - command_output_loss: 0.8130 - command_output_1_loss: 2.9936e-05 - participant_output_1_loss: 2.0123e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1429 - val_loss: 1.0697 - val_participant_output_loss: 0.2048 - val_command_output_loss: 0.8648 - val_command_output_1_loss: 2.3935e-05 - val_participant_output_1_loss: 2.2806e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9742 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 168ms/step - loss: 0.8346 - participant_output_loss: 0.0830 - command_output_loss: 0.7516 - command_output_1_loss: 2.5440e-05 - participant_output_1_loss: 1.9508e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0971 - participant_output_1_accuracy: 0.1343 - val_loss: 1.0032 - val_participant_output_loss: 0.1914 - val_command_output_loss: 0.8117 - val_command_output_1_loss: 2.6180e-05 - val_participant_output_1_loss: 2.2208e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.5046 - val_participant_output_1_accuracy: 0.1326\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 0.7687 - participant_output_loss: 0.0762 - command_output_loss: 0.6924 - command_output_1_loss: 2.9730e-05 - participant_output_1_loss: 1.8743e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1657 - participant_output_1_accuracy: 0.1443 - val_loss: 0.9442 - val_participant_output_loss: 0.1894 - val_command_output_loss: 0.7547 - val_command_output_1_loss: 3.6781e-05 - val_participant_output_1_loss: 2.1955e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9761 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1326\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 169ms/step - loss: 0.7110 - participant_output_loss: 0.0705 - command_output_loss: 0.6405 - command_output_1_loss: 2.9532e-05 - participant_output_1_loss: 1.8154e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1586 - val_loss: 0.8916 - val_participant_output_loss: 0.1780 - val_command_output_loss: 0.7135 - val_command_output_1_loss: 1.3350e-05 - val_participant_output_1_loss: 2.1343e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9687 - val_command_output_1_accuracy: 0.0037 - val_participant_output_1_accuracy: 0.1179\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 216ms/step - loss: 0.6565 - participant_output_loss: 0.0651 - command_output_loss: 0.5914 - command_output_1_loss: 2.1600e-05 - participant_output_1_loss: 1.7277e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1343 - val_loss: 0.8376 - val_participant_output_loss: 0.1735 - val_command_output_loss: 0.6641 - val_command_output_1_loss: 2.5329e-05 - val_participant_output_1_loss: 2.0594e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9761 - val_command_output_1_accuracy: 0.0737 - val_participant_output_1_accuracy: 0.1547\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 341ms/step - loss: 3.8668 - participant_output_loss: 1.5286 - command_output_loss: 2.3341 - command_output_1_loss: 0.0024 - participant_output_1_loss: 0.0017 - participant_output_accuracy: 0.3714 - command_output_accuracy: 0.2486 - command_output_1_accuracy: 0.0814 - participant_output_1_accuracy: 0.1286 - val_loss: 3.5452 - val_participant_output_loss: 1.3868 - val_command_output_loss: 2.1578 - val_command_output_1_loss: 3.1517e-04 - val_participant_output_1_loss: 3.2693e-04 - val_participant_output_accuracy: 0.3794 - val_command_output_accuracy: 0.5948 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0387\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 3.1989 - participant_output_loss: 1.0941 - command_output_loss: 2.1043 - command_output_1_loss: 3.7022e-04 - participant_output_1_loss: 2.1273e-04 - participant_output_accuracy: 0.5871 - command_output_accuracy: 0.6057 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2029 - val_loss: 3.1866 - val_participant_output_loss: 1.1269 - val_command_output_loss: 2.0593 - val_command_output_1_loss: 3.0964e-04 - val_participant_output_1_loss: 1.1349e-04 - val_participant_output_accuracy: 0.5912 - val_command_output_accuracy: 0.5746 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0681\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 2.9050 - participant_output_loss: 0.8856 - command_output_loss: 2.0191 - command_output_1_loss: 2.1009e-04 - participant_output_1_loss: 1.0130e-04 - participant_output_accuracy: 0.7529 - command_output_accuracy: 0.7029 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1914 - val_loss: 2.9603 - val_participant_output_loss: 0.9730 - val_command_output_loss: 1.9871 - val_command_output_1_loss: 9.0795e-05 - val_participant_output_1_loss: 7.2963e-05 - val_participant_output_accuracy: 0.7274 - val_command_output_accuracy: 0.8103 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2302\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 3s 233ms/step - loss: 2.7022 - participant_output_loss: 0.7524 - command_output_loss: 1.9497 - command_output_1_loss: 7.0640e-05 - participant_output_1_loss: 6.2002e-05 - participant_output_accuracy: 0.8571 - command_output_accuracy: 0.8643 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1471 - val_loss: 2.7992 - val_participant_output_loss: 0.8697 - val_command_output_loss: 1.9294 - val_command_output_1_loss: 7.0230e-05 - val_participant_output_1_loss: 6.3077e-05 - val_participant_output_accuracy: 0.7587 - val_command_output_accuracy: 0.8453 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.3131\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 211ms/step - loss: 2.5362 - participant_output_loss: 0.6536 - command_output_loss: 1.8824 - command_output_1_loss: 3.7176e-05 - participant_output_1_loss: 4.7630e-05 - participant_output_accuracy: 0.8829 - command_output_accuracy: 0.8743 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2300 - val_loss: 2.6288 - val_participant_output_loss: 0.7685 - val_command_output_loss: 1.8602 - val_command_output_1_loss: 2.0524e-05 - val_participant_output_1_loss: 4.8076e-05 - val_participant_output_accuracy: 0.8453 - val_command_output_accuracy: 0.8379 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2634\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 204ms/step - loss: 2.3738 - participant_output_loss: 0.5640 - command_output_loss: 1.8098 - command_output_1_loss: 2.7603e-05 - participant_output_1_loss: 3.7570e-05 - participant_output_accuracy: 0.9371 - command_output_accuracy: 0.8843 - command_output_1_accuracy: 0.0014 - participant_output_1_accuracy: 0.2271 - val_loss: 2.4461 - val_participant_output_loss: 0.6490 - val_command_output_loss: 1.7970 - val_command_output_1_loss: 2.3318e-05 - val_participant_output_1_loss: 4.2547e-05 - val_participant_output_accuracy: 0.9208 - val_command_output_accuracy: 0.8950 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2210\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 218ms/step - loss: 2.2260 - participant_output_loss: 0.4858 - command_output_loss: 1.7401 - command_output_1_loss: 2.5413e-05 - participant_output_1_loss: 3.4724e-05 - participant_output_accuracy: 0.9700 - command_output_accuracy: 0.9471 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2457 - val_loss: 2.3014 - val_participant_output_loss: 0.5701 - val_command_output_loss: 1.7313 - val_command_output_1_loss: 2.2022e-05 - val_participant_output_1_loss: 4.2216e-05 - val_participant_output_accuracy: 0.9429 - val_command_output_accuracy: 0.9006 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2634\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 2s 221ms/step - loss: 2.0836 - participant_output_loss: 0.4121 - command_output_loss: 1.6715 - command_output_1_loss: 2.2781e-05 - participant_output_1_loss: 3.3047e-05 - participant_output_accuracy: 0.9800 - command_output_accuracy: 0.9571 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2086 - val_loss: 2.1786 - val_participant_output_loss: 0.5106 - val_command_output_loss: 1.6679 - val_command_output_1_loss: 2.5184e-05 - val_participant_output_1_loss: 4.0259e-05 - val_participant_output_accuracy: 0.9503 - val_command_output_accuracy: 0.9282 - val_command_output_1_accuracy: 0.0074 - val_participant_output_1_accuracy: 0.2449\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 168ms/step - loss: 1.9570 - participant_output_loss: 0.3555 - command_output_loss: 1.6014 - command_output_1_loss: 1.8285e-05 - participant_output_1_loss: 3.1636e-05 - participant_output_accuracy: 0.9900 - command_output_accuracy: 0.9814 - command_output_1_accuracy: 0.1614 - participant_output_1_accuracy: 0.2214 - val_loss: 2.0893 - val_participant_output_loss: 0.4829 - val_command_output_loss: 1.6063 - val_command_output_1_loss: 1.8639e-05 - val_participant_output_1_loss: 3.9729e-05 - val_participant_output_accuracy: 0.9429 - val_command_output_accuracy: 0.9300 - val_command_output_1_accuracy: 0.0368 - val_participant_output_1_accuracy: 0.2670\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 178ms/step - loss: 1.8529 - participant_output_loss: 0.3218 - command_output_loss: 1.5310 - command_output_1_loss: 2.2308e-05 - participant_output_1_loss: 3.0540e-05 - participant_output_accuracy: 0.9886 - command_output_accuracy: 0.9829 - command_output_1_accuracy: 0.0043 - participant_output_1_accuracy: 0.2271 - val_loss: 2.0047 - val_participant_output_loss: 0.4655 - val_command_output_loss: 1.5392 - val_command_output_1_loss: 1.7624e-05 - val_participant_output_1_loss: 3.9887e-05 - val_participant_output_accuracy: 0.9355 - val_command_output_accuracy: 0.9429 - val_command_output_1_accuracy: 0.0037 - val_participant_output_1_accuracy: 0.3260\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 272ms/step - loss: 1.7453 - participant_output_loss: 0.2828 - command_output_loss: 1.4624 - command_output_1_loss: 2.8515e-05 - participant_output_1_loss: 2.8532e-05 - participant_output_accuracy: 0.9914 - command_output_accuracy: 0.9871 - command_output_1_accuracy: 0.0014 - participant_output_1_accuracy: 0.2157 - val_loss: 1.9226 - val_participant_output_loss: 0.4427 - val_command_output_loss: 1.4799 - val_command_output_1_loss: 4.5042e-05 - val_participant_output_1_loss: 3.8392e-05 - val_participant_output_accuracy: 0.9355 - val_command_output_accuracy: 0.9466 - val_command_output_1_accuracy: 0.0092 - val_participant_output_1_accuracy: 0.3333\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 1.6467 - participant_output_loss: 0.2552 - command_output_loss: 1.3914 - command_output_1_loss: 3.1026e-05 - participant_output_1_loss: 2.7588e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9900 - command_output_1_accuracy: 0.0071 - participant_output_1_accuracy: 0.2486 - val_loss: 1.8218 - val_participant_output_loss: 0.4065 - val_command_output_loss: 1.4153 - val_command_output_1_loss: 3.2271e-05 - val_participant_output_1_loss: 3.7404e-05 - val_participant_output_accuracy: 0.9392 - val_command_output_accuracy: 0.9558 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2523\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 213ms/step - loss: 1.5500 - participant_output_loss: 0.2247 - command_output_loss: 1.3252 - command_output_1_loss: 4.6019e-05 - participant_output_1_loss: 2.7247e-05 - participant_output_accuracy: 0.9957 - command_output_accuracy: 0.9929 - command_output_1_accuracy: 0.0357 - participant_output_1_accuracy: 0.2443 - val_loss: 1.7231 - val_participant_output_loss: 0.3727 - val_command_output_loss: 1.3503 - val_command_output_1_loss: 3.5231e-05 - val_participant_output_1_loss: 3.7369e-05 - val_participant_output_accuracy: 0.9448 - val_command_output_accuracy: 0.9576 - val_command_output_1_accuracy: 0.0497 - val_participant_output_1_accuracy: 0.3204\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 1.4565 - participant_output_loss: 0.2034 - command_output_loss: 1.2531 - command_output_1_loss: 3.5513e-05 - participant_output_1_loss: 2.6969e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9943 - command_output_1_accuracy: 0.0114 - participant_output_1_accuracy: 0.2529 - val_loss: 1.6246 - val_participant_output_loss: 0.3444 - val_command_output_loss: 1.2801 - val_command_output_1_loss: 3.9407e-05 - val_participant_output_1_loss: 3.6762e-05 - val_participant_output_accuracy: 0.9503 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2965\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 224ms/step - loss: 1.3693 - participant_output_loss: 0.1842 - command_output_loss: 1.1850 - command_output_1_loss: 4.5298e-05 - participant_output_1_loss: 2.5795e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2357 - val_loss: 1.5291 - val_participant_output_loss: 0.3089 - val_command_output_loss: 1.2201 - val_command_output_1_loss: 4.9867e-05 - val_participant_output_1_loss: 3.6280e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2560\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 212ms/step - loss: 1.2891 - participant_output_loss: 0.1694 - command_output_loss: 1.1196 - command_output_1_loss: 3.4000e-05 - participant_output_1_loss: 2.4252e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0014 - participant_output_1_accuracy: 0.2057 - val_loss: 1.4465 - val_participant_output_loss: 0.2836 - val_command_output_loss: 1.1628 - val_command_output_1_loss: 2.3543e-05 - val_participant_output_1_loss: 3.5394e-05 - val_participant_output_accuracy: 0.9724 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.2873\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 181ms/step - loss: 1.2130 - participant_output_loss: 0.1577 - command_output_loss: 1.0553 - command_output_1_loss: 2.7526e-05 - participant_output_1_loss: 2.3317e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.0114 - participant_output_1_accuracy: 0.2186 - val_loss: 1.3822 - val_participant_output_loss: 0.2774 - val_command_output_loss: 1.1048 - val_command_output_1_loss: 3.4508e-05 - val_participant_output_1_loss: 3.5416e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.0147 - val_participant_output_1_accuracy: 0.3020\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 170ms/step - loss: 1.1427 - participant_output_loss: 0.1480 - command_output_loss: 0.9946 - command_output_1_loss: 2.4299e-05 - participant_output_1_loss: 2.2423e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0429 - participant_output_1_accuracy: 0.2200 - val_loss: 1.3143 - val_participant_output_loss: 0.2655 - val_command_output_loss: 1.0487 - val_command_output_1_loss: 2.3869e-05 - val_participant_output_1_loss: 3.4347e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.0092 - val_participant_output_1_accuracy: 0.2891\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 167ms/step - loss: 1.0750 - participant_output_loss: 0.1380 - command_output_loss: 0.9369 - command_output_1_loss: 3.4765e-05 - participant_output_1_loss: 2.1720e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0029 - participant_output_1_accuracy: 0.2443 - val_loss: 1.2517 - val_participant_output_loss: 0.2588 - val_command_output_loss: 0.9928 - val_command_output_1_loss: 3.1077e-05 - val_participant_output_1_loss: 3.3498e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.2118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 186ms/step - loss: 1.0120 - participant_output_loss: 0.1297 - command_output_loss: 0.8822 - command_output_1_loss: 2.4495e-05 - participant_output_1_loss: 2.0937e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0314 - participant_output_1_accuracy: 0.2129 - val_loss: 1.1975 - val_participant_output_loss: 0.2538 - val_command_output_loss: 0.9437 - val_command_output_1_loss: 3.1339e-05 - val_participant_output_1_loss: 3.2927e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.0976 - val_participant_output_1_accuracy: 0.2081\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 349ms/step - loss: 3.6662 - participant_output_loss: 1.4525 - command_output_loss: 2.2009 - command_output_1_loss: 9.7598e-04 - participant_output_1_loss: 0.0118 - participant_output_accuracy: 0.4500 - command_output_accuracy: 0.2829 - command_output_1_accuracy: 0.0257 - participant_output_1_accuracy: 0.0914 - val_loss: 3.2637 - val_participant_output_loss: 1.2879 - val_command_output_loss: 1.9751 - val_command_output_1_loss: 1.5125e-04 - val_participant_output_1_loss: 5.6730e-04 - val_participant_output_accuracy: 0.4991 - val_command_output_accuracy: 0.6519 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.4438\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 199ms/step - loss: 2.9354 - participant_output_loss: 1.0269 - command_output_loss: 1.9080 - command_output_1_loss: 9.9692e-05 - participant_output_1_loss: 4.5513e-04 - participant_output_accuracy: 0.6800 - command_output_accuracy: 0.8143 - command_output_1_accuracy: 0.1229 - participant_output_1_accuracy: 0.2814 - val_loss: 2.8983 - val_participant_output_loss: 1.0465 - val_command_output_loss: 1.8517 - val_command_output_1_loss: 5.3503e-05 - val_participant_output_1_loss: 1.0135e-04 - val_participant_output_accuracy: 0.6372 - val_command_output_accuracy: 0.8177 - val_command_output_1_accuracy: 0.6372 - val_participant_output_1_accuracy: 0.4530\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 2.5979 - participant_output_loss: 0.8130 - command_output_loss: 1.7847 - command_output_1_loss: 4.3757e-05 - participant_output_1_loss: 1.1214e-04 - participant_output_accuracy: 0.7914 - command_output_accuracy: 0.8743 - command_output_1_accuracy: 0.4129 - participant_output_1_accuracy: 0.1314 - val_loss: 2.5854 - val_participant_output_loss: 0.8382 - val_command_output_loss: 1.7471 - val_command_output_1_loss: 4.7274e-05 - val_participant_output_1_loss: 6.3706e-05 - val_participant_output_accuracy: 0.7993 - val_command_output_accuracy: 0.8527 - val_command_output_1_accuracy: 0.0221 - val_participant_output_1_accuracy: 0.0239\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 2.3284 - participant_output_loss: 0.6537 - command_output_loss: 1.6746 - command_output_1_loss: 5.2610e-05 - participant_output_1_loss: 5.6377e-05 - participant_output_accuracy: 0.8771 - command_output_accuracy: 0.8929 - command_output_1_accuracy: 0.0757 - participant_output_1_accuracy: 0.1229 - val_loss: 2.3545 - val_participant_output_loss: 0.7116 - val_command_output_loss: 1.6428 - val_command_output_1_loss: 5.2012e-05 - val_participant_output_1_loss: 5.7239e-05 - val_participant_output_accuracy: 0.8637 - val_command_output_accuracy: 0.8692 - val_command_output_1_accuracy: 0.0497 - val_participant_output_1_accuracy: 0.1455\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 214ms/step - loss: 2.1016 - participant_output_loss: 0.5346 - command_output_loss: 1.5669 - command_output_1_loss: 5.4252e-05 - participant_output_1_loss: 3.9855e-05 - participant_output_accuracy: 0.9257 - command_output_accuracy: 0.9543 - command_output_1_accuracy: 0.0171 - participant_output_1_accuracy: 0.3214 - val_loss: 2.1828 - val_participant_output_loss: 0.6387 - val_command_output_loss: 1.5440 - val_command_output_1_loss: 6.7782e-05 - val_participant_output_1_loss: 3.8641e-05 - val_participant_output_accuracy: 0.8692 - val_command_output_accuracy: 0.9024 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0773\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 1.9044 - participant_output_loss: 0.4444 - command_output_loss: 1.4599 - command_output_1_loss: 8.0036e-05 - participant_output_1_loss: 3.1153e-05 - participant_output_accuracy: 0.9614 - command_output_accuracy: 0.9657 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1029 - val_loss: 1.9980 - val_participant_output_loss: 0.5478 - val_command_output_loss: 1.4501 - val_command_output_1_loss: 4.4251e-05 - val_participant_output_1_loss: 3.1341e-05 - val_participant_output_accuracy: 0.9208 - val_command_output_accuracy: 0.9171 - val_command_output_1_accuracy: 0.0037 - val_participant_output_1_accuracy: 0.1252\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 174ms/step - loss: 1.7319 - participant_output_loss: 0.3770 - command_output_loss: 1.3548 - command_output_1_loss: 5.3975e-05 - participant_output_1_loss: 2.6280e-05 - participant_output_accuracy: 0.9800 - command_output_accuracy: 0.9829 - command_output_1_accuracy: 0.0043 - participant_output_1_accuracy: 0.1429 - val_loss: 1.8659 - val_participant_output_loss: 0.5222 - val_command_output_loss: 1.3436 - val_command_output_1_loss: 5.6704e-05 - val_participant_output_1_loss: 2.9555e-05 - val_participant_output_accuracy: 0.8950 - val_command_output_accuracy: 0.9337 - val_command_output_1_accuracy: 0.0331 - val_participant_output_1_accuracy: 0.1473\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.5824 - participant_output_loss: 0.3289 - command_output_loss: 1.2534 - command_output_1_loss: 4.5462e-05 - participant_output_1_loss: 2.3717e-05 - participant_output_accuracy: 0.9857 - command_output_accuracy: 0.9886 - command_output_1_accuracy: 0.1557 - participant_output_1_accuracy: 0.2043 - val_loss: 1.6963 - val_participant_output_loss: 0.4467 - val_command_output_loss: 1.2495 - val_command_output_1_loss: 5.0339e-05 - val_participant_output_1_loss: 2.8356e-05 - val_participant_output_accuracy: 0.9319 - val_command_output_accuracy: 0.9411 - val_command_output_1_accuracy: 0.3904 - val_participant_output_1_accuracy: 0.1455\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 213ms/step - loss: 1.4256 - participant_output_loss: 0.2763 - command_output_loss: 1.1492 - command_output_1_loss: 5.1220e-05 - participant_output_1_loss: 2.3284e-05 - participant_output_accuracy: 0.9914 - command_output_accuracy: 0.9900 - command_output_1_accuracy: 0.1671 - participant_output_1_accuracy: 0.1486 - val_loss: 1.5375 - val_participant_output_loss: 0.3818 - val_command_output_loss: 1.1556 - val_command_output_1_loss: 4.4797e-05 - val_participant_output_1_loss: 2.7644e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9558 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2541\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 174ms/step - loss: 1.2897 - participant_output_loss: 0.2334 - command_output_loss: 1.0562 - command_output_1_loss: 4.2054e-05 - participant_output_1_loss: 2.1225e-05 - participant_output_accuracy: 0.9943 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2043 - val_loss: 1.4225 - val_participant_output_loss: 0.3511 - val_command_output_loss: 1.0713 - val_command_output_1_loss: 4.3193e-05 - val_participant_output_1_loss: 2.6344e-05 - val_participant_output_accuracy: 0.9576 - val_command_output_accuracy: 0.9576 - val_command_output_1_accuracy: 0.0074 - val_participant_output_1_accuracy: 0.1731\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 263ms/step - loss: 1.1660 - participant_output_loss: 0.2032 - command_output_loss: 0.9628 - command_output_1_loss: 4.6476e-05 - participant_output_1_loss: 1.9836e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.0643 - participant_output_1_accuracy: 0.2171 - val_loss: 1.3033 - val_participant_output_loss: 0.3150 - val_command_output_loss: 0.9882 - val_command_output_1_loss: 4.4683e-05 - val_participant_output_1_loss: 2.5225e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9558 - val_command_output_1_accuracy: 0.0958 - val_participant_output_1_accuracy: 0.2099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 199ms/step - loss: 1.0557 - participant_output_loss: 0.1781 - command_output_loss: 0.8776 - command_output_1_loss: 4.4289e-05 - participant_output_1_loss: 1.8728e-05 - participant_output_accuracy: 0.9986 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.0357 - participant_output_1_accuracy: 0.1943 - val_loss: 1.2165 - val_participant_output_loss: 0.2961 - val_command_output_loss: 0.9203 - val_command_output_1_loss: 5.1305e-05 - val_participant_output_1_loss: 2.4428e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.0055 - val_participant_output_1_accuracy: 0.1989\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 147ms/step - loss: 0.9566 - participant_output_loss: 0.1562 - command_output_loss: 0.8003 - command_output_1_loss: 5.9976e-05 - participant_output_1_loss: 1.7613e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.0657 - participant_output_1_accuracy: 0.2186 - val_loss: 1.1287 - val_participant_output_loss: 0.2824 - val_command_output_loss: 0.8462 - val_command_output_1_loss: 3.9252e-05 - val_participant_output_1_loss: 2.3750e-05 - val_participant_output_accuracy: 0.9576 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.0497 - val_participant_output_1_accuracy: 0.2007\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 151ms/step - loss: 0.8634 - participant_output_loss: 0.1391 - command_output_loss: 0.7243 - command_output_1_loss: 4.3718e-05 - participant_output_1_loss: 1.6960e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0029 - participant_output_1_accuracy: 0.2029 - val_loss: 1.0417 - val_participant_output_loss: 0.2690 - val_command_output_loss: 0.7727 - val_command_output_1_loss: 4.8023e-05 - val_participant_output_1_loss: 2.3773e-05 - val_participant_output_accuracy: 0.9576 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1676\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 0.7833 - participant_output_loss: 0.1312 - command_output_loss: 0.6520 - command_output_1_loss: 5.4689e-05 - participant_output_1_loss: 1.6653e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.1071 - participant_output_1_accuracy: 0.2043 - val_loss: 0.9921 - val_participant_output_loss: 0.2808 - val_command_output_loss: 0.7113 - val_command_output_1_loss: 4.4685e-05 - val_participant_output_1_loss: 2.3172e-05 - val_participant_output_accuracy: 0.9503 - val_command_output_accuracy: 0.9687 - val_command_output_1_accuracy: 0.5580 - val_participant_output_1_accuracy: 0.2394\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 205ms/step - loss: 0.7066 - participant_output_loss: 0.1169 - command_output_loss: 0.5896 - command_output_1_loss: 3.4675e-05 - participant_output_1_loss: 1.6080e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.3657 - participant_output_1_accuracy: 0.2043 - val_loss: 0.8847 - val_participant_output_loss: 0.2295 - val_command_output_loss: 0.6551 - val_command_output_1_loss: 3.3228e-05 - val_participant_output_1_loss: 2.2959e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.0129 - val_participant_output_1_accuracy: 0.1805\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 0.6376 - participant_output_loss: 0.1046 - command_output_loss: 0.5329 - command_output_1_loss: 3.3192e-05 - participant_output_1_loss: 1.5478e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0029 - participant_output_1_accuracy: 0.2329 - val_loss: 0.8257 - val_participant_output_loss: 0.2231 - val_command_output_loss: 0.6025 - val_command_output_1_loss: 3.2687e-05 - val_participant_output_1_loss: 2.2335e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2155\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 173ms/step - loss: 0.5786 - participant_output_loss: 0.0945 - command_output_loss: 0.4840 - command_output_1_loss: 3.1463e-05 - participant_output_1_loss: 1.4901e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1971 - val_loss: 0.7730 - val_participant_output_loss: 0.2155 - val_command_output_loss: 0.5575 - val_command_output_1_loss: 2.5720e-05 - val_participant_output_1_loss: 2.1685e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.0147 - val_participant_output_1_accuracy: 0.2228\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 175ms/step - loss: 0.5233 - participant_output_loss: 0.0853 - command_output_loss: 0.4380 - command_output_1_loss: 2.0657e-05 - participant_output_1_loss: 1.4106e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0700 - participant_output_1_accuracy: 0.2529 - val_loss: 0.7265 - val_participant_output_loss: 0.2094 - val_command_output_loss: 0.5170 - val_command_output_1_loss: 1.5985e-05 - val_participant_output_1_loss: 2.1479e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.2320 - val_participant_output_1_accuracy: 0.1915\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 0.4761 - participant_output_loss: 0.0771 - command_output_loss: 0.3990 - command_output_1_loss: 2.5292e-05 - participant_output_1_loss: 1.3722e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1214 - participant_output_1_accuracy: 0.2443 - val_loss: 0.6677 - val_participant_output_loss: 0.1889 - val_command_output_loss: 0.4788 - val_command_output_1_loss: 2.2732e-05 - val_participant_output_1_loss: 2.1459e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.0884 - val_participant_output_1_accuracy: 0.1860\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 343ms/step - loss: 3.7242 - participant_output_loss: 1.4594 - command_output_loss: 2.2608 - command_output_1_loss: 0.0011 - participant_output_1_loss: 0.0029 - participant_output_accuracy: 0.4300 - command_output_accuracy: 0.2729 - command_output_1_accuracy: 0.2543 - participant_output_1_accuracy: 0.2557 - val_loss: 3.4030 - val_participant_output_loss: 1.3254 - val_command_output_loss: 2.0772 - val_command_output_1_loss: 1.4910e-04 - val_participant_output_1_loss: 2.3968e-04 - val_participant_output_accuracy: 0.3978 - val_command_output_accuracy: 0.6372 - val_command_output_1_accuracy: 0.0110 - val_participant_output_1_accuracy: 0.4217\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 3.0465 - participant_output_loss: 1.0436 - command_output_loss: 2.0026 - command_output_1_loss: 2.3282e-04 - participant_output_1_loss: 1.1497e-04 - participant_output_accuracy: 0.6329 - command_output_accuracy: 0.6757 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2329 - val_loss: 3.0059 - val_participant_output_loss: 1.0600 - val_command_output_loss: 1.9457 - val_command_output_1_loss: 1.4188e-04 - val_participant_output_1_loss: 6.8118e-05 - val_participant_output_accuracy: 0.5672 - val_command_output_accuracy: 0.7716 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1289\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 2.6931 - participant_output_loss: 0.8115 - command_output_loss: 1.8815 - command_output_1_loss: 1.1126e-04 - participant_output_1_loss: 6.2335e-05 - participant_output_accuracy: 0.7700 - command_output_accuracy: 0.8557 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1886 - val_loss: 2.7243 - val_participant_output_loss: 0.8740 - val_command_output_loss: 1.8501 - val_command_output_1_loss: 1.1367e-04 - val_participant_output_1_loss: 5.2607e-05 - val_participant_output_accuracy: 0.7551 - val_command_output_accuracy: 0.8508 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 186ms/step - loss: 2.4194 - participant_output_loss: 0.6405 - command_output_loss: 1.7787 - command_output_1_loss: 8.9662e-05 - participant_output_1_loss: 3.7494e-05 - participant_output_accuracy: 0.8686 - command_output_accuracy: 0.9100 - command_output_1_accuracy: 0.0157 - participant_output_1_accuracy: 0.1386 - val_loss: 2.4720 - val_participant_output_loss: 0.7192 - val_command_output_loss: 1.7527 - val_command_output_1_loss: 5.7802e-05 - val_participant_output_1_loss: 3.2798e-05 - val_participant_output_accuracy: 0.8453 - val_command_output_accuracy: 0.8877 - val_command_output_1_accuracy: 0.2247 - val_participant_output_1_accuracy: 0.1750\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 2.1689 - participant_output_loss: 0.4936 - command_output_loss: 1.6752 - command_output_1_loss: 5.6573e-05 - participant_output_1_loss: 3.0738e-05 - participant_output_accuracy: 0.9343 - command_output_accuracy: 0.9243 - command_output_1_accuracy: 0.3714 - participant_output_1_accuracy: 0.1614 - val_loss: 2.2672 - val_participant_output_loss: 0.6117 - val_command_output_loss: 1.6555 - val_command_output_1_loss: 5.1047e-05 - val_participant_output_1_loss: 3.0234e-05 - val_participant_output_accuracy: 0.8582 - val_command_output_accuracy: 0.8987 - val_command_output_1_accuracy: 0.5359 - val_participant_output_1_accuracy: 0.0829\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 1.9682 - participant_output_loss: 0.3947 - command_output_loss: 1.5734 - command_output_1_loss: 6.6314e-05 - participant_output_1_loss: 2.5222e-05 - participant_output_accuracy: 0.9571 - command_output_accuracy: 0.9457 - command_output_1_accuracy: 0.3443 - participant_output_1_accuracy: 0.1529 - val_loss: 2.0434 - val_participant_output_loss: 0.4919 - val_command_output_loss: 1.5514 - val_command_output_1_loss: 5.6557e-05 - val_participant_output_1_loss: 2.6940e-05 - val_participant_output_accuracy: 0.9282 - val_command_output_accuracy: 0.9282 - val_command_output_1_accuracy: 0.2099 - val_participant_output_1_accuracy: 0.0976\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 209ms/step - loss: 1.7776 - participant_output_loss: 0.3112 - command_output_loss: 1.4663 - command_output_1_loss: 3.5092e-05 - participant_output_1_loss: 2.3469e-05 - participant_output_accuracy: 0.9757 - command_output_accuracy: 0.9686 - command_output_1_accuracy: 0.1343 - participant_output_1_accuracy: 0.1186 - val_loss: 1.8687 - val_participant_output_loss: 0.4083 - val_command_output_loss: 1.4603 - val_command_output_1_loss: 2.9534e-05 - val_participant_output_1_loss: 2.5180e-05 - val_participant_output_accuracy: 0.9355 - val_command_output_accuracy: 0.9484 - val_command_output_1_accuracy: 0.0129 - val_participant_output_1_accuracy: 0.1123\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 202ms/step - loss: 1.6217 - participant_output_loss: 0.2521 - command_output_loss: 1.3695 - command_output_1_loss: 4.7634e-05 - participant_output_1_loss: 2.1723e-05 - participant_output_accuracy: 0.9857 - command_output_accuracy: 0.9743 - command_output_1_accuracy: 0.0043 - participant_output_1_accuracy: 0.1314 - val_loss: 1.7173 - val_participant_output_loss: 0.3423 - val_command_output_loss: 1.3749 - val_command_output_1_loss: 3.6379e-05 - val_participant_output_1_loss: 2.3847e-05 - val_participant_output_accuracy: 0.9521 - val_command_output_accuracy: 0.9429 - val_command_output_1_accuracy: 0.0074 - val_participant_output_1_accuracy: 0.0866\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 224ms/step - loss: 1.4789 - participant_output_loss: 0.2101 - command_output_loss: 1.2687 - command_output_1_loss: 4.5218e-05 - participant_output_1_loss: 1.9960e-05 - participant_output_accuracy: 0.9929 - command_output_accuracy: 0.9843 - command_output_1_accuracy: 0.1014 - participant_output_1_accuracy: 0.1200 - val_loss: 1.5998 - val_participant_output_loss: 0.3202 - val_command_output_loss: 1.2795 - val_command_output_1_loss: 4.8138e-05 - val_participant_output_1_loss: 2.3607e-05 - val_participant_output_accuracy: 0.9540 - val_command_output_accuracy: 0.9503 - val_command_output_1_accuracy: 0.2928 - val_participant_output_1_accuracy: 0.1308\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 216ms/step - loss: 1.3497 - participant_output_loss: 0.1772 - command_output_loss: 1.1724 - command_output_1_loss: 4.9852e-05 - participant_output_1_loss: 1.9208e-05 - participant_output_accuracy: 0.9957 - command_output_accuracy: 0.9929 - command_output_1_accuracy: 0.1600 - participant_output_1_accuracy: 0.1300 - val_loss: 1.4720 - val_participant_output_loss: 0.2826 - val_command_output_loss: 1.1893 - val_command_output_1_loss: 3.7110e-05 - val_participant_output_1_loss: 2.2266e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9540 - val_command_output_1_accuracy: 0.2284 - val_participant_output_1_accuracy: 0.1179\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 1.2363 - participant_output_loss: 0.1544 - command_output_loss: 1.0819 - command_output_1_loss: 5.2515e-05 - participant_output_1_loss: 1.8129e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9914 - command_output_1_accuracy: 0.0914 - participant_output_1_accuracy: 0.1357 - val_loss: 1.3821 - val_participant_output_loss: 0.2732 - val_command_output_loss: 1.1088 - val_command_output_1_loss: 5.5286e-05 - val_participant_output_1_loss: 2.2743e-05 - val_participant_output_accuracy: 0.9613 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.0203 - val_participant_output_1_accuracy: 0.1400\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 1.1295 - participant_output_loss: 0.1324 - command_output_loss: 0.9970 - command_output_1_loss: 5.2326e-05 - participant_output_1_loss: 1.8386e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9929 - command_output_1_accuracy: 0.0086 - participant_output_1_accuracy: 0.1214 - val_loss: 1.2796 - val_participant_output_loss: 0.2473 - val_command_output_loss: 1.0323 - val_command_output_1_loss: 4.0221e-05 - val_participant_output_1_loss: 2.1834e-05 - val_participant_output_accuracy: 0.9613 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.0424 - val_participant_output_1_accuracy: 0.1381\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 186ms/step - loss: 1.0307 - participant_output_loss: 0.1161 - command_output_loss: 0.9145 - command_output_1_loss: 4.1122e-05 - participant_output_1_loss: 1.7245e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9943 - command_output_1_accuracy: 0.2229 - participant_output_1_accuracy: 0.1457 - val_loss: 1.1778 - val_participant_output_loss: 0.2188 - val_command_output_loss: 0.9590 - val_command_output_1_loss: 3.7173e-05 - val_participant_output_1_loss: 2.1990e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.3775 - val_participant_output_1_accuracy: 0.0939\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 180ms/step - loss: 0.9361 - participant_output_loss: 0.1023 - command_output_loss: 0.8338 - command_output_1_loss: 4.0495e-05 - participant_output_1_loss: 1.6461e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.1229 - participant_output_1_accuracy: 0.1586 - val_loss: 1.0919 - val_participant_output_loss: 0.2089 - val_command_output_loss: 0.8829 - val_command_output_1_loss: 4.2149e-05 - val_participant_output_1_loss: 2.1452e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.0074 - val_participant_output_1_accuracy: 0.1565\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 179ms/step - loss: 0.8518 - participant_output_loss: 0.0904 - command_output_loss: 0.7613 - command_output_1_loss: 3.3677e-05 - participant_output_1_loss: 1.5882e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.0014 - participant_output_1_accuracy: 0.1486 - val_loss: 1.0184 - val_participant_output_loss: 0.2021 - val_command_output_loss: 0.8162 - val_command_output_1_loss: 2.6017e-05 - val_participant_output_1_loss: 2.0990e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9724 - val_command_output_1_accuracy: 0.0479 - val_participant_output_1_accuracy: 0.1878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 150ms/step - loss: 0.7729 - participant_output_loss: 0.0804 - command_output_loss: 0.6924 - command_output_1_loss: 3.3704e-05 - participant_output_1_loss: 1.5703e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.3157 - participant_output_1_accuracy: 0.1757 - val_loss: 0.9427 - val_participant_output_loss: 0.1859 - val_command_output_loss: 0.7567 - val_command_output_1_loss: 2.9741e-05 - val_participant_output_1_loss: 2.1318e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9687 - val_command_output_1_accuracy: 0.5801 - val_participant_output_1_accuracy: 0.0866\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 142ms/step - loss: 0.7060 - participant_output_loss: 0.0741 - command_output_loss: 0.6319 - command_output_1_loss: 2.9410e-05 - participant_output_1_loss: 1.4578e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1929 - participant_output_1_accuracy: 0.1786 - val_loss: 0.8720 - val_participant_output_loss: 0.1713 - val_command_output_loss: 0.7007 - val_command_output_1_loss: 3.0971e-05 - val_participant_output_1_loss: 2.0156e-05 - val_participant_output_accuracy: 0.9724 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.0092 - val_participant_output_1_accuracy: 0.1473\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 151ms/step - loss: 0.6425 - participant_output_loss: 0.0659 - command_output_loss: 0.5766 - command_output_1_loss: 2.7935e-05 - participant_output_1_loss: 1.3681e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0057 - participant_output_1_accuracy: 0.1571 - val_loss: 0.8124 - val_participant_output_loss: 0.1628 - val_command_output_loss: 0.6495 - val_command_output_1_loss: 2.3135e-05 - val_participant_output_1_loss: 2.0016e-05 - val_participant_output_accuracy: 0.9724 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.0239 - val_participant_output_1_accuracy: 0.1473\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 180ms/step - loss: 0.5863 - participant_output_loss: 0.0600 - command_output_loss: 0.5262 - command_output_1_loss: 2.5741e-05 - participant_output_1_loss: 1.3113e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0371 - participant_output_1_accuracy: 0.1757 - val_loss: 0.7554 - val_participant_output_loss: 0.1551 - val_command_output_loss: 0.6002 - val_command_output_1_loss: 2.4246e-05 - val_participant_output_1_loss: 1.9964e-05 - val_participant_output_accuracy: 0.9761 - val_command_output_accuracy: 0.9724 - val_command_output_1_accuracy: 0.2210 - val_participant_output_1_accuracy: 0.1326\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 181ms/step - loss: 0.5359 - participant_output_loss: 0.0553 - command_output_loss: 0.4806 - command_output_1_loss: 2.4081e-05 - participant_output_1_loss: 1.2704e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1386 - participant_output_1_accuracy: 0.1671 - val_loss: 0.7113 - val_participant_output_loss: 0.1492 - val_command_output_loss: 0.5620 - val_command_output_1_loss: 1.9692e-05 - val_participant_output_1_loss: 2.0036e-05 - val_participant_output_accuracy: 0.9724 - val_command_output_accuracy: 0.9742 - val_command_output_1_accuracy: 0.4291 - val_participant_output_1_accuracy: 0.0921\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 358ms/step - loss: 3.6637 - participant_output_loss: 1.3934 - command_output_loss: 2.2657 - command_output_1_loss: 0.0011 - participant_output_1_loss: 0.0034 - participant_output_accuracy: 0.4571 - command_output_accuracy: 0.2971 - command_output_1_accuracy: 0.0900 - participant_output_1_accuracy: 0.3243 - val_loss: 3.3736 - val_participant_output_loss: 1.2557 - val_command_output_loss: 2.1171 - val_command_output_1_loss: 3.4441e-04 - val_participant_output_1_loss: 4.4387e-04 - val_participant_output_accuracy: 0.4972 - val_command_output_accuracy: 0.6317 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.4549\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 213ms/step - loss: 3.0142 - participant_output_loss: 0.9637 - command_output_loss: 2.0501 - command_output_1_loss: 2.3434e-04 - participant_output_1_loss: 2.6040e-04 - participant_output_accuracy: 0.7229 - command_output_accuracy: 0.6643 - command_output_1_accuracy: 0.0014 - participant_output_1_accuracy: 0.1000 - val_loss: 2.9391 - val_participant_output_loss: 0.9439 - val_command_output_loss: 1.9948 - val_command_output_1_loss: 1.5046e-04 - val_participant_output_1_loss: 2.0893e-04 - val_participant_output_accuracy: 0.7440 - val_command_output_accuracy: 0.6998 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.3260\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 2.6707 - participant_output_loss: 0.7275 - command_output_loss: 1.9429 - command_output_1_loss: 1.6248e-04 - participant_output_1_loss: 1.1126e-04 - participant_output_accuracy: 0.8571 - command_output_accuracy: 0.7800 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2814 - val_loss: 2.7053 - val_participant_output_loss: 0.7923 - val_command_output_loss: 1.9128 - val_command_output_1_loss: 1.7788e-04 - val_participant_output_1_loss: 6.5068e-05 - val_participant_output_accuracy: 0.8140 - val_command_output_accuracy: 0.7974 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0534\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 2.4472 - participant_output_loss: 0.5901 - command_output_loss: 1.8569 - command_output_1_loss: 1.1362e-04 - participant_output_1_loss: 4.8107e-05 - participant_output_accuracy: 0.9157 - command_output_accuracy: 0.8614 - command_output_1_accuracy: 0.0014 - participant_output_1_accuracy: 0.2257 - val_loss: 2.5162 - val_participant_output_loss: 0.6893 - val_command_output_loss: 1.8268 - val_command_output_1_loss: 5.5532e-05 - val_participant_output_1_loss: 4.4937e-05 - val_participant_output_accuracy: 0.8343 - val_command_output_accuracy: 0.8343 - val_command_output_1_accuracy: 0.0773 - val_participant_output_1_accuracy: 0.0939\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 215ms/step - loss: 2.2637 - participant_output_loss: 0.4831 - command_output_loss: 1.7805 - command_output_1_loss: 6.2717e-05 - participant_output_1_loss: 3.5139e-05 - participant_output_accuracy: 0.9371 - command_output_accuracy: 0.8557 - command_output_1_accuracy: 0.2786 - participant_output_1_accuracy: 0.1614 - val_loss: 2.3210 - val_participant_output_loss: 0.5651 - val_command_output_loss: 1.7558 - val_command_output_1_loss: 3.4598e-05 - val_participant_output_1_loss: 3.3920e-05 - val_participant_output_accuracy: 0.8950 - val_command_output_accuracy: 0.8306 - val_command_output_1_accuracy: 0.6390 - val_participant_output_1_accuracy: 0.2155\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 210ms/step - loss: 2.0958 - participant_output_loss: 0.3937 - command_output_loss: 1.7020 - command_output_1_loss: 3.8713e-05 - participant_output_1_loss: 2.8408e-05 - participant_output_accuracy: 0.9743 - command_output_accuracy: 0.8600 - command_output_1_accuracy: 0.0843 - participant_output_1_accuracy: 0.2214 - val_loss: 2.1973 - val_participant_output_loss: 0.5060 - val_command_output_loss: 1.6912 - val_command_output_1_loss: 3.6539e-05 - val_participant_output_1_loss: 3.1730e-05 - val_participant_output_accuracy: 0.9171 - val_command_output_accuracy: 0.8435 - val_command_output_1_accuracy: 0.0129 - val_participant_output_1_accuracy: 0.2026\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 217ms/step - loss: 1.9576 - participant_output_loss: 0.3336 - command_output_loss: 1.6240 - command_output_1_loss: 4.1496e-05 - participant_output_1_loss: 2.6443e-05 - participant_output_accuracy: 0.9757 - command_output_accuracy: 0.8800 - command_output_1_accuracy: 0.0671 - participant_output_1_accuracy: 0.2057 - val_loss: 2.0534 - val_participant_output_loss: 0.4382 - val_command_output_loss: 1.6151 - val_command_output_1_loss: 4.0312e-05 - val_participant_output_1_loss: 3.3445e-05 - val_participant_output_accuracy: 0.9503 - val_command_output_accuracy: 0.8490 - val_command_output_1_accuracy: 0.1510 - val_participant_output_1_accuracy: 0.1142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 217ms/step - loss: 1.8274 - participant_output_loss: 0.2799 - command_output_loss: 1.5474 - command_output_1_loss: 4.3163e-05 - participant_output_1_loss: 2.4728e-05 - participant_output_accuracy: 0.9843 - command_output_accuracy: 0.9143 - command_output_1_accuracy: 0.0871 - participant_output_1_accuracy: 0.2143 - val_loss: 1.9156 - val_participant_output_loss: 0.3768 - val_command_output_loss: 1.5388 - val_command_output_1_loss: 2.1931e-05 - val_participant_output_1_loss: 3.0689e-05 - val_participant_output_accuracy: 0.9595 - val_command_output_accuracy: 0.8950 - val_command_output_1_accuracy: 0.0994 - val_participant_output_1_accuracy: 0.1952\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 224ms/step - loss: 1.7007 - participant_output_loss: 0.2321 - command_output_loss: 1.4685 - command_output_1_loss: 2.8879e-05 - participant_output_1_loss: 2.3527e-05 - participant_output_accuracy: 0.9929 - command_output_accuracy: 0.9486 - command_output_1_accuracy: 0.0243 - participant_output_1_accuracy: 0.2257 - val_loss: 1.8087 - val_participant_output_loss: 0.3428 - val_command_output_loss: 1.4659 - val_command_output_1_loss: 2.8534e-05 - val_participant_output_1_loss: 3.0010e-05 - val_participant_output_accuracy: 0.9521 - val_command_output_accuracy: 0.9300 - val_command_output_1_accuracy: 0.0092 - val_participant_output_1_accuracy: 0.1639\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 1.5882 - participant_output_loss: 0.1975 - command_output_loss: 1.3907 - command_output_1_loss: 2.9451e-05 - participant_output_1_loss: 2.2595e-05 - participant_output_accuracy: 0.9943 - command_output_accuracy: 0.9671 - command_output_1_accuracy: 0.0071 - participant_output_1_accuracy: 0.2386 - val_loss: 1.7096 - val_participant_output_loss: 0.3148 - val_command_output_loss: 1.3948 - val_command_output_1_loss: 3.8494e-05 - val_participant_output_1_loss: 2.9165e-05 - val_participant_output_accuracy: 0.9521 - val_command_output_accuracy: 0.9282 - val_command_output_1_accuracy: 0.0847 - val_participant_output_1_accuracy: 0.1915\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 1.4826 - participant_output_loss: 0.1671 - command_output_loss: 1.3155 - command_output_1_loss: 4.1322e-05 - participant_output_1_loss: 2.1879e-05 - participant_output_accuracy: 0.9957 - command_output_accuracy: 0.9729 - command_output_1_accuracy: 0.5843 - participant_output_1_accuracy: 0.2043 - val_loss: 1.5971 - val_participant_output_loss: 0.2683 - val_command_output_loss: 1.3286 - val_command_output_1_loss: 4.7777e-05 - val_participant_output_1_loss: 2.9885e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9282 - val_command_output_1_accuracy: 0.5635 - val_participant_output_1_accuracy: 0.1400\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 1.3828 - participant_output_loss: 0.1436 - command_output_loss: 1.2392 - command_output_1_loss: 3.4066e-05 - participant_output_1_loss: 2.1630e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9714 - command_output_1_accuracy: 0.1671 - participant_output_1_accuracy: 0.2157 - val_loss: 1.5057 - val_participant_output_loss: 0.2483 - val_command_output_loss: 1.2573 - val_command_output_1_loss: 2.7188e-05 - val_participant_output_1_loss: 3.1499e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9576 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1529\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 209ms/step - loss: 1.2918 - participant_output_loss: 0.1271 - command_output_loss: 1.1647 - command_output_1_loss: 4.7179e-05 - participant_output_1_loss: 2.1627e-05 - participant_output_accuracy: 0.9986 - command_output_accuracy: 0.9843 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2129 - val_loss: 1.4144 - val_participant_output_loss: 0.2306 - val_command_output_loss: 1.1837 - val_command_output_1_loss: 4.4466e-05 - val_participant_output_1_loss: 3.0675e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1308\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 212ms/step - loss: 1.1991 - participant_output_loss: 0.1130 - command_output_loss: 1.0860 - command_output_1_loss: 3.6547e-05 - participant_output_1_loss: 2.1231e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9900 - command_output_1_accuracy: 0.0071 - participant_output_1_accuracy: 0.2043 - val_loss: 1.3327 - val_participant_output_loss: 0.2241 - val_command_output_loss: 1.1085 - val_command_output_1_loss: 3.3275e-05 - val_participant_output_1_loss: 2.9455e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.1731 - val_participant_output_1_accuracy: 0.1805\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 212ms/step - loss: 1.1149 - participant_output_loss: 0.1024 - command_output_loss: 1.0124 - command_output_1_loss: 3.4586e-05 - participant_output_1_loss: 2.0718e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9900 - command_output_1_accuracy: 0.7214 - participant_output_1_accuracy: 0.2214 - val_loss: 1.2572 - val_participant_output_loss: 0.2109 - val_command_output_loss: 1.0462 - val_command_output_1_loss: 2.5503e-05 - val_participant_output_1_loss: 2.9595e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9687 - val_command_output_1_accuracy: 0.5193 - val_participant_output_1_accuracy: 0.1657\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.0367 - participant_output_loss: 0.0936 - command_output_loss: 0.9431 - command_output_1_loss: 2.7998e-05 - participant_output_1_loss: 2.0561e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9929 - command_output_1_accuracy: 0.0829 - participant_output_1_accuracy: 0.2014 - val_loss: 1.1788 - val_participant_output_loss: 0.1993 - val_command_output_loss: 0.9794 - val_command_output_1_loss: 2.8380e-05 - val_participant_output_1_loss: 2.8723e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.2007\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 209ms/step - loss: 0.9613 - participant_output_loss: 0.0854 - command_output_loss: 0.8758 - command_output_1_loss: 2.4233e-05 - participant_output_1_loss: 1.9824e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1986 - val_loss: 1.1121 - val_participant_output_loss: 0.1942 - val_command_output_loss: 0.9178 - val_command_output_1_loss: 2.4335e-05 - val_participant_output_1_loss: 2.8423e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9742 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1768\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 0.8936 - participant_output_loss: 0.0796 - command_output_loss: 0.8139 - command_output_1_loss: 2.5458e-05 - participant_output_1_loss: 1.9603e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2243 - val_loss: 1.0490 - val_participant_output_loss: 0.1844 - val_command_output_loss: 0.8646 - val_command_output_1_loss: 2.2383e-05 - val_participant_output_1_loss: 3.0121e-05 - val_participant_output_accuracy: 0.9742 - val_command_output_accuracy: 0.9742 - val_command_output_1_accuracy: 0.0092 - val_participant_output_1_accuracy: 0.1289\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 178ms/step - loss: 0.8317 - participant_output_loss: 0.0734 - command_output_loss: 0.7582 - command_output_1_loss: 3.1652e-05 - participant_output_1_loss: 1.8857e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.1671 - participant_output_1_accuracy: 0.2129 - val_loss: 0.9813 - val_participant_output_loss: 0.1672 - val_command_output_loss: 0.8140 - val_command_output_1_loss: 2.4992e-05 - val_participant_output_1_loss: 2.8608e-05 - val_participant_output_accuracy: 0.9761 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.4622 - val_participant_output_1_accuracy: 0.1529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 168ms/step - loss: 0.7716 - participant_output_loss: 0.0671 - command_output_loss: 0.7045 - command_output_1_loss: 1.4885e-05 - participant_output_1_loss: 1.8672e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.2071 - participant_output_1_accuracy: 0.2000 - val_loss: 0.9358 - val_participant_output_loss: 0.1696 - val_command_output_loss: 0.7661 - val_command_output_1_loss: 1.2904e-05 - val_participant_output_1_loss: 2.8660e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1878\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 365ms/step - loss: 3.6233 - participant_output_loss: 1.3477 - command_output_loss: 2.2662 - command_output_1_loss: 0.0031 - participant_output_1_loss: 0.0064 - participant_output_accuracy: 0.5000 - command_output_accuracy: 0.3214 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2843 - val_loss: 3.3602 - val_participant_output_loss: 1.2465 - val_command_output_loss: 2.1123 - val_command_output_1_loss: 7.2384e-05 - val_participant_output_1_loss: 0.0014 - val_participant_output_accuracy: 0.4715 - val_command_output_accuracy: 0.5378 - val_command_output_1_accuracy: 0.0239 - val_participant_output_1_accuracy: 0.0110\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 219ms/step - loss: 3.0442 - participant_output_loss: 0.9714 - command_output_loss: 2.0723 - command_output_1_loss: 8.4837e-05 - participant_output_1_loss: 4.3183e-04 - participant_output_accuracy: 0.6714 - command_output_accuracy: 0.5157 - command_output_1_accuracy: 0.7086 - participant_output_1_accuracy: 0.1143 - val_loss: 3.0464 - val_participant_output_loss: 1.0100 - val_command_output_loss: 2.0362 - val_command_output_1_loss: 6.8247e-05 - val_participant_output_1_loss: 1.5981e-04 - val_participant_output_accuracy: 0.6114 - val_command_output_accuracy: 0.5985 - val_command_output_1_accuracy: 0.5175 - val_participant_output_1_accuracy: 0.0497\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 2.7317 - participant_output_loss: 0.7381 - command_output_loss: 1.9935 - command_output_1_loss: 3.9102e-05 - participant_output_1_loss: 9.0790e-05 - participant_output_accuracy: 0.8029 - command_output_accuracy: 0.7486 - command_output_1_accuracy: 0.1086 - participant_output_1_accuracy: 0.0214 - val_loss: 2.7249 - val_participant_output_loss: 0.7540 - val_command_output_loss: 1.9708 - val_command_output_1_loss: 4.0690e-05 - val_participant_output_1_loss: 5.3843e-05 - val_participant_output_accuracy: 0.8379 - val_command_output_accuracy: 0.7716 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0700\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 2.4923 - participant_output_loss: 0.5607 - command_output_loss: 1.9315 - command_output_1_loss: 4.1406e-05 - participant_output_1_loss: 4.3458e-05 - participant_output_accuracy: 0.9143 - command_output_accuracy: 0.8414 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.3871 - val_loss: 2.5175 - val_participant_output_loss: 0.6052 - val_command_output_loss: 1.9123 - val_command_output_1_loss: 3.4085e-05 - val_participant_output_1_loss: 2.6609e-05 - val_participant_output_accuracy: 0.9153 - val_command_output_accuracy: 0.8453 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1308\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 208ms/step - loss: 2.3003 - participant_output_loss: 0.4320 - command_output_loss: 1.8683 - command_output_1_loss: 3.0471e-05 - participant_output_1_loss: 2.3769e-05 - participant_output_accuracy: 0.9571 - command_output_accuracy: 0.8857 - command_output_1_accuracy: 0.0443 - participant_output_1_accuracy: 0.0871 - val_loss: 2.3976 - val_participant_output_loss: 0.5445 - val_command_output_loss: 1.8531 - val_command_output_1_loss: 2.0334e-05 - val_participant_output_1_loss: 1.8751e-05 - val_participant_output_accuracy: 0.9116 - val_command_output_accuracy: 0.8913 - val_command_output_1_accuracy: 0.2026 - val_participant_output_1_accuracy: 0.2099\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 183ms/step - loss: 2.1745 - participant_output_loss: 0.3724 - command_output_loss: 1.8021 - command_output_1_loss: 2.2868e-05 - participant_output_1_loss: 1.8716e-05 - participant_output_accuracy: 0.9629 - command_output_accuracy: 0.9329 - command_output_1_accuracy: 0.0614 - participant_output_1_accuracy: 0.3229 - val_loss: 2.2535 - val_participant_output_loss: 0.4627 - val_command_output_loss: 1.7907 - val_command_output_1_loss: 1.7109e-05 - val_participant_output_1_loss: 1.7447e-05 - val_participant_output_accuracy: 0.9392 - val_command_output_accuracy: 0.9134 - val_command_output_1_accuracy: 0.0331 - val_participant_output_1_accuracy: 0.1418\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 2.0322 - participant_output_loss: 0.2975 - command_output_loss: 1.7347 - command_output_1_loss: 1.9567e-05 - participant_output_1_loss: 1.6332e-05 - participant_output_accuracy: 0.9786 - command_output_accuracy: 0.9471 - command_output_1_accuracy: 0.0800 - participant_output_1_accuracy: 0.1443 - val_loss: 2.1251 - val_participant_output_loss: 0.3964 - val_command_output_loss: 1.7287 - val_command_output_1_loss: 2.2676e-05 - val_participant_output_1_loss: 1.5448e-05 - val_participant_output_accuracy: 0.9448 - val_command_output_accuracy: 0.9190 - val_command_output_1_accuracy: 0.3702 - val_participant_output_1_accuracy: 0.1842\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 1.9110 - participant_output_loss: 0.2431 - command_output_loss: 1.6678 - command_output_1_loss: 3.4536e-05 - participant_output_1_loss: 1.5407e-05 - participant_output_accuracy: 0.9900 - command_output_accuracy: 0.9729 - command_output_1_accuracy: 0.1500 - participant_output_1_accuracy: 0.2343 - val_loss: 2.0043 - val_participant_output_loss: 0.3360 - val_command_output_loss: 1.6682 - val_command_output_1_loss: 4.5675e-05 - val_participant_output_1_loss: 1.5745e-05 - val_participant_output_accuracy: 0.9613 - val_command_output_accuracy: 0.9448 - val_command_output_1_accuracy: 0.0037 - val_participant_output_1_accuracy: 0.1308\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 1.8089 - participant_output_loss: 0.2035 - command_output_loss: 1.6053 - command_output_1_loss: 3.5356e-05 - participant_output_1_loss: 1.6355e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9786 - command_output_1_accuracy: 0.0057 - participant_output_1_accuracy: 0.1686 - val_loss: 1.9163 - val_participant_output_loss: 0.3086 - val_command_output_loss: 1.6076 - val_command_output_1_loss: 1.9136e-05 - val_participant_output_1_loss: 1.6428e-05 - val_participant_output_accuracy: 0.9558 - val_command_output_accuracy: 0.9540 - val_command_output_1_accuracy: 0.0239 - val_participant_output_1_accuracy: 0.1529\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 1.7144 - participant_output_loss: 0.1766 - command_output_loss: 1.5377 - command_output_1_loss: 2.6685e-05 - participant_output_1_loss: 1.6748e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9786 - command_output_1_accuracy: 0.0129 - participant_output_1_accuracy: 0.1400 - val_loss: 1.8396 - val_participant_output_loss: 0.2923 - val_command_output_loss: 1.5472 - val_command_output_1_loss: 3.3086e-05 - val_participant_output_1_loss: 1.6028e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9540 - val_command_output_1_accuracy: 0.0166 - val_participant_output_1_accuracy: 0.1529\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 274ms/step - loss: 1.6307 - participant_output_loss: 0.1555 - command_output_loss: 1.4752 - command_output_1_loss: 2.1828e-05 - participant_output_1_loss: 1.6164e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9914 - command_output_1_accuracy: 0.0829 - participant_output_1_accuracy: 0.1514 - val_loss: 1.7479 - val_participant_output_loss: 0.2615 - val_command_output_loss: 1.4864 - val_command_output_1_loss: 2.2578e-05 - val_participant_output_1_loss: 1.5966e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9484 - val_command_output_1_accuracy: 0.0497 - val_participant_output_1_accuracy: 0.1750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 208ms/step - loss: 1.5470 - participant_output_loss: 0.1364 - command_output_loss: 1.4106 - command_output_1_loss: 2.4366e-05 - participant_output_1_loss: 1.5680e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.0543 - participant_output_1_accuracy: 0.1514 - val_loss: 1.6756 - val_participant_output_loss: 0.2434 - val_command_output_loss: 1.4321 - val_command_output_1_loss: 2.3521e-05 - val_participant_output_1_loss: 1.5536e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9558 - val_command_output_1_accuracy: 0.1436 - val_participant_output_1_accuracy: 0.1694\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 1.4698 - participant_output_loss: 0.1211 - command_output_loss: 1.3486 - command_output_1_loss: 2.7143e-05 - participant_output_1_loss: 1.4782e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.1071 - participant_output_1_accuracy: 0.1643 - val_loss: 1.6053 - val_participant_output_loss: 0.2321 - val_command_output_loss: 1.3732 - val_command_output_1_loss: 1.8879e-05 - val_participant_output_1_loss: 1.4873e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9558 - val_command_output_1_accuracy: 0.2394 - val_participant_output_1_accuracy: 0.1713\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 1.3965 - participant_output_loss: 0.1093 - command_output_loss: 1.2872 - command_output_1_loss: 2.3818e-05 - participant_output_1_loss: 1.3803e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.1314 - participant_output_1_accuracy: 0.1557 - val_loss: 1.5431 - val_participant_output_loss: 0.2278 - val_command_output_loss: 1.3153 - val_command_output_1_loss: 3.3174e-05 - val_participant_output_1_loss: 1.4013e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.0055 - val_participant_output_1_accuracy: 0.1842\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 1.3297 - participant_output_loss: 0.0994 - command_output_loss: 1.2303 - command_output_1_loss: 2.9475e-05 - participant_output_1_loss: 1.2698e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0743 - participant_output_1_accuracy: 0.1857 - val_loss: 1.4747 - val_participant_output_loss: 0.2102 - val_command_output_loss: 1.2645 - val_command_output_1_loss: 1.4902e-05 - val_participant_output_1_loss: 1.3326e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.1326 - val_participant_output_1_accuracy: 0.2118\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 145ms/step - loss: 1.2650 - participant_output_loss: 0.0907 - command_output_loss: 1.1742 - command_output_1_loss: 3.8432e-05 - participant_output_1_loss: 1.1660e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.0071 - participant_output_1_accuracy: 0.1771 - val_loss: 1.4194 - val_participant_output_loss: 0.2107 - val_command_output_loss: 1.2086 - val_command_output_1_loss: 4.0123e-05 - val_participant_output_1_loss: 1.2327e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9687 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1915\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 153ms/step - loss: 1.2021 - participant_output_loss: 0.0845 - command_output_loss: 1.1175 - command_output_1_loss: 4.4364e-05 - participant_output_1_loss: 1.0852e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1771 - val_loss: 1.3466 - val_participant_output_loss: 0.1895 - val_command_output_loss: 1.1571 - val_command_output_1_loss: 3.2553e-05 - val_participant_output_1_loss: 1.1796e-05 - val_participant_output_accuracy: 0.9724 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.0166 - val_participant_output_1_accuracy: 0.1971\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 148ms/step - loss: 1.1380 - participant_output_loss: 0.0778 - command_output_loss: 1.0602 - command_output_1_loss: 3.6776e-05 - participant_output_1_loss: 1.0094e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.2300 - participant_output_1_accuracy: 0.2271 - val_loss: 1.2929 - val_participant_output_loss: 0.1812 - val_command_output_loss: 1.1116 - val_command_output_1_loss: 4.1665e-05 - val_participant_output_1_loss: 1.1494e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9632 - val_command_output_1_accuracy: 0.2726 - val_participant_output_1_accuracy: 0.1823\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 172ms/step - loss: 1.0804 - participant_output_loss: 0.0724 - command_output_loss: 1.0080 - command_output_1_loss: 3.7203e-05 - participant_output_1_loss: 9.7610e-06 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.2714 - participant_output_1_accuracy: 0.1986 - val_loss: 1.2277 - val_participant_output_loss: 0.1723 - val_command_output_loss: 1.0553 - val_command_output_1_loss: 1.4140e-05 - val_participant_output_1_loss: 1.0925e-05 - val_participant_output_accuracy: 0.9724 - val_command_output_accuracy: 0.9687 - val_command_output_1_accuracy: 0.6280 - val_participant_output_1_accuracy: 0.2394\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 173ms/step - loss: 1.0248 - participant_output_loss: 0.0669 - command_output_loss: 0.9578 - command_output_1_loss: 2.7969e-05 - participant_output_1_loss: 9.2971e-06 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1586 - participant_output_1_accuracy: 0.1843 - val_loss: 1.1780 - val_participant_output_loss: 0.1715 - val_command_output_loss: 1.0065 - val_command_output_1_loss: 3.6618e-05 - val_participant_output_1_loss: 1.0872e-05 - val_participant_output_accuracy: 0.9724 - val_command_output_accuracy: 0.9687 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2597\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 398ms/step - loss: 3.7107 - participant_output_loss: 1.4904 - command_output_loss: 2.2098 - command_output_1_loss: 8.6093e-04 - participant_output_1_loss: 0.0097 - participant_output_accuracy: 0.4071 - command_output_accuracy: 0.3000 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.1986 - val_loss: 3.3211 - val_participant_output_loss: 1.3111 - val_command_output_loss: 2.0088 - val_command_output_1_loss: 1.8493e-04 - val_participant_output_1_loss: 0.0010 - val_participant_output_accuracy: 0.5083 - val_command_output_accuracy: 0.5617 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 2.9984 - participant_output_loss: 1.0825 - command_output_loss: 1.9154 - command_output_1_loss: 1.6251e-04 - participant_output_1_loss: 3.7345e-04 - participant_output_accuracy: 0.6357 - command_output_accuracy: 0.6871 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.0629 - val_loss: 2.9503 - val_participant_output_loss: 1.1046 - val_command_output_loss: 1.8454 - val_command_output_1_loss: 9.8211e-05 - val_participant_output_1_loss: 2.2891e-04 - val_participant_output_accuracy: 0.6096 - val_command_output_accuracy: 0.7495 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0350\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 2.6677 - participant_output_loss: 0.8919 - command_output_loss: 1.7756 - command_output_1_loss: 8.2744e-05 - participant_output_1_loss: 1.2452e-04 - participant_output_accuracy: 0.7471 - command_output_accuracy: 0.7871 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.0657 - val_loss: 2.6744 - val_participant_output_loss: 0.9444 - val_command_output_loss: 1.7299 - val_command_output_1_loss: 7.4442e-05 - val_participant_output_1_loss: 9.3285e-05 - val_participant_output_accuracy: 0.7182 - val_command_output_accuracy: 0.8637 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.2081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 181ms/step - loss: 2.3865 - participant_output_loss: 0.7352 - command_output_loss: 1.6511 - command_output_1_loss: 8.1390e-05 - participant_output_1_loss: 6.7106e-05 - participant_output_accuracy: 0.8343 - command_output_accuracy: 0.9071 - command_output_1_accuracy: 0.0371 - participant_output_1_accuracy: 0.1643 - val_loss: 2.4232 - val_participant_output_loss: 0.8054 - val_command_output_loss: 1.6177 - val_command_output_1_loss: 4.9341e-05 - val_participant_output_1_loss: 6.1516e-05 - val_participant_output_accuracy: 0.8324 - val_command_output_accuracy: 0.8766 - val_command_output_1_accuracy: 0.3517 - val_participant_output_1_accuracy: 0.1160\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 180ms/step - loss: 2.1403 - participant_output_loss: 0.6097 - command_output_loss: 1.5305 - command_output_1_loss: 6.7377e-05 - participant_output_1_loss: 4.7278e-05 - participant_output_accuracy: 0.9200 - command_output_accuracy: 0.9371 - command_output_1_accuracy: 0.5486 - participant_output_1_accuracy: 0.0971 - val_loss: 2.2053 - val_participant_output_loss: 0.7033 - val_command_output_loss: 1.5018 - val_command_output_1_loss: 7.9390e-05 - val_participant_output_1_loss: 4.7388e-05 - val_participant_output_accuracy: 0.8821 - val_command_output_accuracy: 0.9134 - val_command_output_1_accuracy: 0.3573 - val_participant_output_1_accuracy: 0.0645\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 1.9318 - participant_output_loss: 0.5213 - command_output_loss: 1.4104 - command_output_1_loss: 6.9876e-05 - participant_output_1_loss: 3.8110e-05 - participant_output_accuracy: 0.9343 - command_output_accuracy: 0.9514 - command_output_1_accuracy: 0.0857 - participant_output_1_accuracy: 0.1600 - val_loss: 2.0006 - val_participant_output_loss: 0.6101 - val_command_output_loss: 1.3904 - val_command_output_1_loss: 3.9172e-05 - val_participant_output_1_loss: 4.0703e-05 - val_participant_output_accuracy: 0.9171 - val_command_output_accuracy: 0.9503 - val_command_output_1_accuracy: 0.0442 - val_participant_output_1_accuracy: 0.1731\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 184ms/step - loss: 1.7264 - participant_output_loss: 0.4352 - command_output_loss: 1.2911 - command_output_1_loss: 4.0097e-05 - participant_output_1_loss: 3.5695e-05 - participant_output_accuracy: 0.9600 - command_output_accuracy: 0.9786 - command_output_1_accuracy: 0.1457 - participant_output_1_accuracy: 0.1000 - val_loss: 1.8251 - val_participant_output_loss: 0.5358 - val_command_output_loss: 1.2892 - val_command_output_1_loss: 2.8859e-05 - val_participant_output_1_loss: 3.9759e-05 - val_participant_output_accuracy: 0.9282 - val_command_output_accuracy: 0.9540 - val_command_output_1_accuracy: 0.3315 - val_participant_output_1_accuracy: 0.0792\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 1.5448 - participant_output_loss: 0.3614 - command_output_loss: 1.1834 - command_output_1_loss: 3.3800e-05 - participant_output_1_loss: 3.3754e-05 - participant_output_accuracy: 0.9800 - command_output_accuracy: 0.9886 - command_output_1_accuracy: 0.0900 - participant_output_1_accuracy: 0.1257 - val_loss: 1.6659 - val_participant_output_loss: 0.4716 - val_command_output_loss: 1.1942 - val_command_output_1_loss: 2.9003e-05 - val_participant_output_1_loss: 3.7980e-05 - val_participant_output_accuracy: 0.9392 - val_command_output_accuracy: 0.9576 - val_command_output_1_accuracy: 0.0645 - val_participant_output_1_accuracy: 0.1308\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 213ms/step - loss: 1.3924 - participant_output_loss: 0.3076 - command_output_loss: 1.0847 - command_output_1_loss: 4.3838e-05 - participant_output_1_loss: 3.2010e-05 - participant_output_accuracy: 0.9900 - command_output_accuracy: 0.9857 - command_output_1_accuracy: 0.0214 - participant_output_1_accuracy: 0.1286 - val_loss: 1.5254 - val_participant_output_loss: 0.4239 - val_command_output_loss: 1.1014 - val_command_output_1_loss: 3.9353e-05 - val_participant_output_1_loss: 3.7243e-05 - val_participant_output_accuracy: 0.9448 - val_command_output_accuracy: 0.9724 - val_command_output_1_accuracy: 0.0129 - val_participant_output_1_accuracy: 0.1179\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 156ms/step - loss: 1.2515 - participant_output_loss: 0.2622 - command_output_loss: 0.9892 - command_output_1_loss: 4.7699e-05 - participant_output_1_loss: 3.0613e-05 - participant_output_accuracy: 0.9957 - command_output_accuracy: 0.9943 - command_output_1_accuracy: 0.3100 - participant_output_1_accuracy: 0.1429 - val_loss: 1.3995 - val_participant_output_loss: 0.3840 - val_command_output_loss: 1.0154 - val_command_output_1_loss: 4.6032e-05 - val_participant_output_1_loss: 3.6666e-05 - val_participant_output_accuracy: 0.9503 - val_command_output_accuracy: 0.9595 - val_command_output_1_accuracy: 0.3002 - val_participant_output_1_accuracy: 0.1510\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 1.1237 - participant_output_loss: 0.2267 - command_output_loss: 0.8969 - command_output_1_loss: 5.0333e-05 - participant_output_1_loss: 2.9680e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.0886 - participant_output_1_accuracy: 0.1114 - val_loss: 1.2813 - val_participant_output_loss: 0.3536 - val_command_output_loss: 0.9276 - val_command_output_1_loss: 3.0289e-05 - val_participant_output_1_loss: 3.6359e-05 - val_participant_output_accuracy: 0.9503 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1418\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 1.0060 - participant_output_loss: 0.1966 - command_output_loss: 0.8094 - command_output_1_loss: 3.5294e-05 - participant_output_1_loss: 2.8102e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.0129 - participant_output_1_accuracy: 0.1971 - val_loss: 1.1591 - val_participant_output_loss: 0.3109 - val_command_output_loss: 0.8481 - val_command_output_1_loss: 3.8628e-05 - val_participant_output_1_loss: 3.4560e-05 - val_participant_output_accuracy: 0.9521 - val_command_output_accuracy: 0.9724 - val_command_output_1_accuracy: 0.0626 - val_participant_output_1_accuracy: 0.1105\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 185ms/step - loss: 0.8978 - participant_output_loss: 0.1680 - command_output_loss: 0.7297 - command_output_1_loss: 3.8021e-05 - participant_output_1_loss: 2.6812e-05 - participant_output_accuracy: 0.9986 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.2500 - participant_output_1_accuracy: 0.1086 - val_loss: 1.0522 - val_participant_output_loss: 0.2760 - val_command_output_loss: 0.7761 - val_command_output_1_loss: 2.6167e-05 - val_participant_output_1_loss: 3.4626e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.2468 - val_participant_output_1_accuracy: 0.2136\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 0.8029 - participant_output_loss: 0.1454 - command_output_loss: 0.6574 - command_output_1_loss: 3.7501e-05 - participant_output_1_loss: 2.5259e-05 - participant_output_accuracy: 0.9986 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0443 - participant_output_1_accuracy: 0.1743 - val_loss: 0.9693 - val_participant_output_loss: 0.2554 - val_command_output_loss: 0.7138 - val_command_output_1_loss: 3.7767e-05 - val_participant_output_1_loss: 3.3579e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1381\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 156ms/step - loss: 0.7165 - participant_output_loss: 0.1263 - command_output_loss: 0.5902 - command_output_1_loss: 3.0588e-05 - participant_output_1_loss: 2.4090e-05 - participant_output_accuracy: 0.9986 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.1986 - participant_output_1_accuracy: 0.1500 - val_loss: 0.8897 - val_participant_output_loss: 0.2322 - val_command_output_loss: 0.6574 - val_command_output_1_loss: 4.0755e-05 - val_participant_output_1_loss: 3.1618e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.3886 - val_participant_output_1_accuracy: 0.1400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 149ms/step - loss: 0.6421 - participant_output_loss: 0.1103 - command_output_loss: 0.5318 - command_output_1_loss: 2.8108e-05 - participant_output_1_loss: 2.2688e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.2686 - participant_output_1_accuracy: 0.1057 - val_loss: 0.8173 - val_participant_output_loss: 0.2142 - val_command_output_loss: 0.6031 - val_command_output_1_loss: 1.9260e-05 - val_participant_output_1_loss: 3.0986e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9687 - val_command_output_1_accuracy: 0.2707 - val_participant_output_1_accuracy: 0.1492\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 0.5795 - participant_output_loss: 0.0992 - command_output_loss: 0.4802 - command_output_1_loss: 2.0634e-05 - participant_output_1_loss: 2.2023e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0986 - participant_output_1_accuracy: 0.1157 - val_loss: 0.7602 - val_participant_output_loss: 0.2060 - val_command_output_loss: 0.5542 - val_command_output_1_loss: 2.2426e-05 - val_participant_output_1_loss: 2.9517e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9742 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1473\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 184ms/step - loss: 0.5224 - participant_output_loss: 0.0905 - command_output_loss: 0.4319 - command_output_1_loss: 2.1085e-05 - participant_output_1_loss: 2.0754e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0543 - participant_output_1_accuracy: 0.1814 - val_loss: 0.7025 - val_participant_output_loss: 0.1894 - val_command_output_loss: 0.5131 - val_command_output_1_loss: 2.3643e-05 - val_participant_output_1_loss: 2.9272e-05 - val_participant_output_accuracy: 0.9724 - val_command_output_accuracy: 0.9742 - val_command_output_1_accuracy: 0.3168 - val_participant_output_1_accuracy: 0.0994\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 150ms/step - loss: 0.4717 - participant_output_loss: 0.0833 - command_output_loss: 0.3884 - command_output_1_loss: 2.3511e-05 - participant_output_1_loss: 1.9717e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.2100 - participant_output_1_accuracy: 0.1271 - val_loss: 0.6562 - val_participant_output_loss: 0.1867 - val_command_output_loss: 0.4694 - val_command_output_1_loss: 2.4483e-05 - val_participant_output_1_loss: 2.8277e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9742 - val_command_output_1_accuracy: 0.1068 - val_participant_output_1_accuracy: 0.1621\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 142ms/step - loss: 0.4268 - participant_output_loss: 0.0763 - command_output_loss: 0.3504 - command_output_1_loss: 2.0488e-05 - participant_output_1_loss: 1.8403e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.4886 - participant_output_1_accuracy: 0.1386 - val_loss: 0.6142 - val_participant_output_loss: 0.1794 - val_command_output_loss: 0.4348 - val_command_output_1_loss: 1.5108e-05 - val_participant_output_1_loss: 2.7955e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9742 - val_command_output_1_accuracy: 0.2578 - val_participant_output_1_accuracy: 0.1492\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 335ms/step - loss: 3.6776 - participant_output_loss: 1.4354 - command_output_loss: 2.2386 - command_output_1_loss: 7.5308e-04 - participant_output_1_loss: 0.0029 - participant_output_accuracy: 0.4157 - command_output_accuracy: 0.3529 - command_output_1_accuracy: 0.0200 - participant_output_1_accuracy: 0.1386 - val_loss: 3.3301 - val_participant_output_loss: 1.2720 - val_command_output_loss: 2.0576 - val_command_output_1_loss: 5.5338e-05 - val_participant_output_1_loss: 4.5336e-04 - val_participant_output_accuracy: 0.4715 - val_command_output_accuracy: 0.5985 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0055\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 2.9596 - participant_output_loss: 0.9784 - command_output_loss: 1.9809 - command_output_1_loss: 9.3217e-05 - participant_output_1_loss: 2.5253e-04 - participant_output_accuracy: 0.6786 - command_output_accuracy: 0.7057 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2471 - val_loss: 2.9089 - val_participant_output_loss: 0.9847 - val_command_output_loss: 1.9239 - val_command_output_1_loss: 1.0531e-04 - val_participant_output_1_loss: 1.6570e-04 - val_participant_output_accuracy: 0.6832 - val_command_output_accuracy: 0.8158 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1179\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 2.5906 - participant_output_loss: 0.7316 - command_output_loss: 1.8588 - command_output_1_loss: 8.3943e-05 - participant_output_1_loss: 8.9091e-05 - participant_output_accuracy: 0.8386 - command_output_accuracy: 0.8671 - command_output_1_accuracy: 0.1843 - participant_output_1_accuracy: 0.0329 - val_loss: 2.5815 - val_participant_output_loss: 0.7584 - val_command_output_loss: 1.8230 - val_command_output_1_loss: 6.1590e-05 - val_participant_output_1_loss: 7.6378e-05 - val_participant_output_accuracy: 0.8416 - val_command_output_accuracy: 0.8269 - val_command_output_1_accuracy: 0.4788 - val_participant_output_1_accuracy: 0.1087\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 178ms/step - loss: 2.3139 - participant_output_loss: 0.5558 - command_output_loss: 1.7580 - command_output_1_loss: 4.6593e-05 - participant_output_1_loss: 5.7355e-05 - participant_output_accuracy: 0.9214 - command_output_accuracy: 0.8729 - command_output_1_accuracy: 0.1886 - participant_output_1_accuracy: 0.2414 - val_loss: 2.3367 - val_participant_output_loss: 0.6124 - val_command_output_loss: 1.7242 - val_command_output_1_loss: 3.7014e-05 - val_participant_output_1_loss: 4.4331e-05 - val_participant_output_accuracy: 0.9098 - val_command_output_accuracy: 0.8692 - val_command_output_1_accuracy: 0.0110 - val_participant_output_1_accuracy: 0.1694\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 183ms/step - loss: 2.1081 - participant_output_loss: 0.4495 - command_output_loss: 1.6585 - command_output_1_loss: 6.4437e-05 - participant_output_1_loss: 3.8423e-05 - participant_output_accuracy: 0.9543 - command_output_accuracy: 0.9271 - command_output_1_accuracy: 0.0014 - participant_output_1_accuracy: 0.1257 - val_loss: 2.1528 - val_participant_output_loss: 0.5200 - val_command_output_loss: 1.6327 - val_command_output_1_loss: 8.2842e-05 - val_participant_output_1_loss: 3.9695e-05 - val_participant_output_accuracy: 0.9411 - val_command_output_accuracy: 0.8729 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1639\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 1.9186 - participant_output_loss: 0.3618 - command_output_loss: 1.5567 - command_output_1_loss: 5.3184e-05 - participant_output_1_loss: 3.3666e-05 - participant_output_accuracy: 0.9700 - command_output_accuracy: 0.9557 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2386 - val_loss: 1.9725 - val_participant_output_loss: 0.4444 - val_command_output_loss: 1.5280 - val_command_output_1_loss: 3.9027e-05 - val_participant_output_1_loss: 3.5932e-05 - val_participant_output_accuracy: 0.9411 - val_command_output_accuracy: 0.9300 - val_command_output_1_accuracy: 0.0184 - val_participant_output_1_accuracy: 0.1621\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 183ms/step - loss: 1.7539 - participant_output_loss: 0.3020 - command_output_loss: 1.4518 - command_output_1_loss: 4.8703e-05 - participant_output_1_loss: 3.1719e-05 - participant_output_accuracy: 0.9857 - command_output_accuracy: 0.9714 - command_output_1_accuracy: 0.0329 - participant_output_1_accuracy: 0.1657 - val_loss: 1.8334 - val_participant_output_loss: 0.3897 - val_command_output_loss: 1.4436 - val_command_output_1_loss: 3.8087e-05 - val_participant_output_1_loss: 3.7804e-05 - val_participant_output_accuracy: 0.9503 - val_command_output_accuracy: 0.9319 - val_command_output_1_accuracy: 0.0368 - val_participant_output_1_accuracy: 0.1381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 1.6024 - participant_output_loss: 0.2499 - command_output_loss: 1.3524 - command_output_1_loss: 4.9646e-05 - participant_output_1_loss: 2.9425e-05 - participant_output_accuracy: 0.9914 - command_output_accuracy: 0.9857 - command_output_1_accuracy: 0.1186 - participant_output_1_accuracy: 0.1971 - val_loss: 1.7064 - val_participant_output_loss: 0.3583 - val_command_output_loss: 1.3480 - val_command_output_1_loss: 5.1865e-05 - val_participant_output_1_loss: 3.5819e-05 - val_participant_output_accuracy: 0.9521 - val_command_output_accuracy: 0.9521 - val_command_output_1_accuracy: 0.1308 - val_participant_output_1_accuracy: 0.1510\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 177ms/step - loss: 1.4672 - participant_output_loss: 0.2112 - command_output_loss: 1.2559 - command_output_1_loss: 6.4918e-05 - participant_output_1_loss: 2.7227e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9914 - command_output_1_accuracy: 0.0243 - participant_output_1_accuracy: 0.1700 - val_loss: 1.5783 - val_participant_output_loss: 0.3175 - val_command_output_loss: 1.2607 - val_command_output_1_loss: 4.7487e-05 - val_participant_output_1_loss: 3.3033e-05 - val_participant_output_accuracy: 0.9595 - val_command_output_accuracy: 0.9540 - val_command_output_1_accuracy: 0.0258 - val_participant_output_1_accuracy: 0.1952\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 187ms/step - loss: 1.3419 - participant_output_loss: 0.1832 - command_output_loss: 1.1586 - command_output_1_loss: 6.0504e-05 - participant_output_1_loss: 2.5180e-05 - participant_output_accuracy: 0.9971 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.0057 - participant_output_1_accuracy: 0.2014 - val_loss: 1.4605 - val_participant_output_loss: 0.2865 - val_command_output_loss: 1.1739 - val_command_output_1_loss: 6.9839e-05 - val_participant_output_1_loss: 3.2444e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9576 - val_command_output_1_accuracy: 0.0497 - val_participant_output_1_accuracy: 0.1547\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 1.2259 - participant_output_loss: 0.1587 - command_output_loss: 1.0672 - command_output_1_loss: 4.2147e-05 - participant_output_1_loss: 2.4000e-05 - participant_output_accuracy: 0.9986 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.2129 - participant_output_1_accuracy: 0.1829 - val_loss: 1.3640 - val_participant_output_loss: 0.2719 - val_command_output_loss: 1.0921 - val_command_output_1_loss: 3.5997e-05 - val_participant_output_1_loss: 3.1989e-05 - val_participant_output_accuracy: 0.9613 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.3462 - val_participant_output_1_accuracy: 0.1455\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 154ms/step - loss: 1.1195 - participant_output_loss: 0.1390 - command_output_loss: 0.9805 - command_output_1_loss: 4.9667e-05 - participant_output_1_loss: 2.2602e-05 - participant_output_accuracy: 0.9986 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.2743 - participant_output_1_accuracy: 0.1843 - val_loss: 1.2629 - val_participant_output_loss: 0.2467 - val_command_output_loss: 1.0162 - val_command_output_1_loss: 3.6869e-05 - val_participant_output_1_loss: 3.1130e-05 - val_participant_output_accuracy: 0.9632 - val_command_output_accuracy: 0.9595 - val_command_output_1_accuracy: 0.1326 - val_participant_output_1_accuracy: 0.1529\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 184ms/step - loss: 1.0184 - participant_output_loss: 0.1225 - command_output_loss: 0.8958 - command_output_1_loss: 4.6223e-05 - participant_output_1_loss: 2.1833e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0343 - participant_output_1_accuracy: 0.1929 - val_loss: 1.1534 - val_participant_output_loss: 0.2245 - val_command_output_loss: 0.9289 - val_command_output_1_loss: 3.9205e-05 - val_participant_output_1_loss: 3.0968e-05 - val_participant_output_accuracy: 0.9687 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.0166 - val_participant_output_1_accuracy: 0.1657\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 156ms/step - loss: 0.9226 - participant_output_loss: 0.1083 - command_output_loss: 0.8142 - command_output_1_loss: 2.5820e-05 - participant_output_1_loss: 2.0872e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0400 - participant_output_1_accuracy: 0.1957 - val_loss: 1.0677 - val_participant_output_loss: 0.2103 - val_command_output_loss: 0.8574 - val_command_output_1_loss: 2.6449e-05 - val_participant_output_1_loss: 3.1635e-05 - val_participant_output_accuracy: 0.9669 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.0350 - val_participant_output_1_accuracy: 0.1786\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.8379 - participant_output_loss: 0.0976 - command_output_loss: 0.7402 - command_output_1_loss: 3.0176e-05 - participant_output_1_loss: 2.0138e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0414 - participant_output_1_accuracy: 0.1843 - val_loss: 0.9942 - val_participant_output_loss: 0.1948 - val_command_output_loss: 0.7993 - val_command_output_1_loss: 2.8425e-05 - val_participant_output_1_loss: 3.0898e-05 - val_participant_output_accuracy: 0.9779 - val_command_output_accuracy: 0.9669 - val_command_output_1_accuracy: 0.2026 - val_participant_output_1_accuracy: 0.1750\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 0.7631 - participant_output_loss: 0.0891 - command_output_loss: 0.6740 - command_output_1_loss: 3.3531e-05 - participant_output_1_loss: 1.9446e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.3971 - participant_output_1_accuracy: 0.2100 - val_loss: 0.9370 - val_participant_output_loss: 0.1975 - val_command_output_loss: 0.7395 - val_command_output_1_loss: 3.5411e-05 - val_participant_output_1_loss: 3.1095e-05 - val_participant_output_accuracy: 0.9650 - val_command_output_accuracy: 0.9724 - val_command_output_1_accuracy: 0.0792 - val_participant_output_1_accuracy: 0.1915\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 157ms/step - loss: 0.6922 - participant_output_loss: 0.0821 - command_output_loss: 0.6100 - command_output_1_loss: 2.9758e-05 - participant_output_1_loss: 1.9032e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.0143 - participant_output_1_accuracy: 0.2157 - val_loss: 0.8500 - val_participant_output_loss: 0.1700 - val_command_output_loss: 0.6799 - val_command_output_1_loss: 2.6121e-05 - val_participant_output_1_loss: 3.2174e-05 - val_participant_output_accuracy: 0.9742 - val_command_output_accuracy: 0.9687 - val_command_output_1_accuracy: 0.0331 - val_participant_output_1_accuracy: 0.1271\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 153ms/step - loss: 0.6288 - participant_output_loss: 0.0751 - command_output_loss: 0.5536 - command_output_1_loss: 3.3507e-05 - participant_output_1_loss: 1.8050e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.4114 - participant_output_1_accuracy: 0.2043 - val_loss: 0.7919 - val_participant_output_loss: 0.1629 - val_command_output_loss: 0.6290 - val_command_output_1_loss: 2.8316e-05 - val_participant_output_1_loss: 3.1946e-05 - val_participant_output_accuracy: 0.9742 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.7109 - val_participant_output_1_accuracy: 0.1271\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 169ms/step - loss: 0.5712 - participant_output_loss: 0.0696 - command_output_loss: 0.5016 - command_output_1_loss: 1.9286e-05 - participant_output_1_loss: 1.7266e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.2843 - participant_output_1_accuracy: 0.2071 - val_loss: 0.7382 - val_participant_output_loss: 0.1585 - val_command_output_loss: 0.5796 - val_command_output_1_loss: 1.9014e-05 - val_participant_output_1_loss: 3.1720e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9724 - val_command_output_1_accuracy: 0.0626 - val_participant_output_1_accuracy: 0.1473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 204ms/step - loss: 0.5193 - participant_output_loss: 0.0651 - command_output_loss: 0.4541 - command_output_1_loss: 3.6147e-05 - participant_output_1_loss: 1.6638e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0243 - participant_output_1_accuracy: 0.1843 - val_loss: 0.6861 - val_participant_output_loss: 0.1510 - val_command_output_loss: 0.5350 - val_command_output_1_loss: 3.6312e-05 - val_participant_output_1_loss: 3.2711e-05 - val_participant_output_accuracy: 0.9742 - val_command_output_accuracy: 0.9724 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.1713\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s 361ms/step - loss: 3.5749 - participant_output_loss: 1.3737 - command_output_loss: 2.1950 - command_output_1_loss: 0.0015 - participant_output_1_loss: 0.0047 - participant_output_accuracy: 0.4500 - command_output_accuracy: 0.2971 - command_output_1_accuracy: 0.4186 - participant_output_1_accuracy: 0.1943 - val_loss: 3.3461 - val_participant_output_loss: 1.3460 - val_command_output_loss: 1.9983 - val_command_output_1_loss: 6.7295e-04 - val_participant_output_1_loss: 0.0012 - val_participant_output_accuracy: 0.3536 - val_command_output_accuracy: 0.5396 - val_command_output_1_accuracy: 0.1308 - val_participant_output_1_accuracy: 0.9816\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 2.8751 - participant_output_loss: 0.9650 - command_output_loss: 1.9093 - command_output_1_loss: 4.1804e-04 - participant_output_1_loss: 4.4276e-04 - participant_output_accuracy: 0.6829 - command_output_accuracy: 0.7129 - command_output_1_accuracy: 0.0100 - participant_output_1_accuracy: 0.2714 - val_loss: 2.8417 - val_participant_output_loss: 0.9952 - val_command_output_loss: 1.8460 - val_command_output_1_loss: 1.7168e-04 - val_participant_output_1_loss: 2.3004e-04 - val_participant_output_accuracy: 0.6796 - val_command_output_accuracy: 0.7145 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0166\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 183ms/step - loss: 2.4879 - participant_output_loss: 0.7200 - command_output_loss: 1.7676 - command_output_1_loss: 9.0398e-05 - participant_output_1_loss: 1.3231e-04 - participant_output_accuracy: 0.8571 - command_output_accuracy: 0.8557 - command_output_1_accuracy: 0.0000e+00 - participant_output_1_accuracy: 0.2157 - val_loss: 2.5112 - val_participant_output_loss: 0.7830 - val_command_output_loss: 1.7281 - val_command_output_1_loss: 4.7450e-05 - val_participant_output_1_loss: 1.0044e-04 - val_participant_output_accuracy: 0.8637 - val_command_output_accuracy: 0.8527 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.0921\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 2.2051 - participant_output_loss: 0.5559 - command_output_loss: 1.6491 - command_output_1_loss: 3.7873e-05 - participant_output_1_loss: 7.1537e-05 - participant_output_accuracy: 0.9243 - command_output_accuracy: 0.9057 - command_output_1_accuracy: 0.0843 - participant_output_1_accuracy: 0.1257 - val_loss: 2.2560 - val_participant_output_loss: 0.6361 - val_command_output_loss: 1.6198 - val_command_output_1_loss: 3.3251e-05 - val_participant_output_1_loss: 6.2774e-05 - val_participant_output_accuracy: 0.8987 - val_command_output_accuracy: 0.8766 - val_command_output_1_accuracy: 0.2192 - val_participant_output_1_accuracy: 0.0405\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 186ms/step - loss: 1.9855 - participant_output_loss: 0.4505 - command_output_loss: 1.5349 - command_output_1_loss: 3.4014e-05 - participant_output_1_loss: 5.4085e-05 - participant_output_accuracy: 0.9471 - command_output_accuracy: 0.9486 - command_output_1_accuracy: 0.1557 - participant_output_1_accuracy: 0.1014 - val_loss: 2.0906 - val_participant_output_loss: 0.5773 - val_command_output_loss: 1.5133 - val_command_output_1_loss: 3.8862e-05 - val_participant_output_1_loss: 5.5069e-05 - val_participant_output_accuracy: 0.8932 - val_command_output_accuracy: 0.9116 - val_command_output_1_accuracy: 0.0221 - val_participant_output_1_accuracy: 0.3923\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 1.7866 - participant_output_loss: 0.3653 - command_output_loss: 1.4212 - command_output_1_loss: 3.9389e-05 - participant_output_1_loss: 4.4598e-05 - participant_output_accuracy: 0.9686 - command_output_accuracy: 0.9629 - command_output_1_accuracy: 0.0171 - participant_output_1_accuracy: 0.2043 - val_loss: 1.8722 - val_participant_output_loss: 0.4721 - val_command_output_loss: 1.4000 - val_command_output_1_loss: 4.0670e-05 - val_participant_output_1_loss: 5.0519e-05 - val_participant_output_accuracy: 0.9245 - val_command_output_accuracy: 0.9319 - val_command_output_1_accuracy: 0.0368 - val_participant_output_1_accuracy: 0.0350\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 215ms/step - loss: 1.5948 - participant_output_loss: 0.2916 - command_output_loss: 1.3031 - command_output_1_loss: 4.4793e-05 - participant_output_1_loss: 3.9909e-05 - participant_output_accuracy: 0.9829 - command_output_accuracy: 0.9743 - command_output_1_accuracy: 0.0400 - participant_output_1_accuracy: 0.1071 - val_loss: 1.6938 - val_participant_output_loss: 0.3940 - val_command_output_loss: 1.2997 - val_command_output_1_loss: 4.2829e-05 - val_participant_output_1_loss: 4.1689e-05 - val_participant_output_accuracy: 0.9448 - val_command_output_accuracy: 0.9374 - val_command_output_1_accuracy: 0.2652 - val_participant_output_1_accuracy: 0.1694\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 204ms/step - loss: 1.4327 - participant_output_loss: 0.2397 - command_output_loss: 1.1929 - command_output_1_loss: 4.4828e-05 - participant_output_1_loss: 3.6113e-05 - participant_output_accuracy: 0.9900 - command_output_accuracy: 0.9771 - command_output_1_accuracy: 0.3786 - participant_output_1_accuracy: 0.1300 - val_loss: 1.5185 - val_participant_output_loss: 0.3242 - val_command_output_loss: 1.1943 - val_command_output_1_loss: 6.1382e-05 - val_participant_output_1_loss: 4.2299e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9411 - val_command_output_1_accuracy: 0.1713 - val_participant_output_1_accuracy: 0.1547\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 205ms/step - loss: 1.2778 - participant_output_loss: 0.1913 - command_output_loss: 1.0864 - command_output_1_loss: 6.1572e-05 - participant_output_1_loss: 3.5852e-05 - participant_output_accuracy: 0.9914 - command_output_accuracy: 0.9886 - command_output_1_accuracy: 0.0300 - participant_output_1_accuracy: 0.1214 - val_loss: 1.3989 - val_participant_output_loss: 0.3017 - val_command_output_loss: 1.0970 - val_command_output_1_loss: 5.2263e-05 - val_participant_output_1_loss: 4.2119e-05 - val_participant_output_accuracy: 0.9576 - val_command_output_accuracy: 0.9558 - val_command_output_1_accuracy: 0.0000e+00 - val_participant_output_1_accuracy: 0.1179\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 216ms/step - loss: 1.1434 - participant_output_loss: 0.1573 - command_output_loss: 0.9860 - command_output_1_loss: 4.9888e-05 - participant_output_1_loss: 3.4376e-05 - participant_output_accuracy: 0.9986 - command_output_accuracy: 0.9929 - command_output_1_accuracy: 0.0429 - participant_output_1_accuracy: 0.1600 - val_loss: 1.2727 - val_participant_output_loss: 0.2641 - val_command_output_loss: 1.0084 - val_command_output_1_loss: 5.0139e-05 - val_participant_output_1_loss: 4.1525e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.1584 - val_participant_output_1_accuracy: 0.1584\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 274ms/step - loss: 1.0155 - participant_output_loss: 0.1240 - command_output_loss: 0.8914 - command_output_1_loss: 4.9381e-05 - participant_output_1_loss: 3.4446e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.2714 - participant_output_1_accuracy: 0.1343 - val_loss: 1.1513 - val_participant_output_loss: 0.2248 - val_command_output_loss: 0.9265 - val_command_output_1_loss: 4.3569e-05 - val_participant_output_1_loss: 4.3200e-05 - val_participant_output_accuracy: 0.9742 - val_command_output_accuracy: 0.9595 - val_command_output_1_accuracy: 0.1842 - val_participant_output_1_accuracy: 0.1123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s 186ms/step - loss: 0.9038 - participant_output_loss: 0.1038 - command_output_loss: 0.7999 - command_output_1_loss: 3.5722e-05 - participant_output_1_loss: 3.2842e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9957 - command_output_1_accuracy: 0.0586 - participant_output_1_accuracy: 0.1357 - val_loss: 1.0560 - val_participant_output_loss: 0.2087 - val_command_output_loss: 0.8472 - val_command_output_1_loss: 4.4064e-05 - val_participant_output_1_loss: 4.0833e-05 - val_participant_output_accuracy: 0.9761 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.0055 - val_participant_output_1_accuracy: 0.1842\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s 144ms/step - loss: 0.8113 - participant_output_loss: 0.0892 - command_output_loss: 0.7220 - command_output_1_loss: 5.5311e-05 - participant_output_1_loss: 3.1232e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9971 - command_output_1_accuracy: 0.0129 - participant_output_1_accuracy: 0.1686 - val_loss: 0.9694 - val_participant_output_loss: 0.1960 - val_command_output_loss: 0.7733 - val_command_output_1_loss: 3.8962e-05 - val_participant_output_1_loss: 4.0408e-05 - val_participant_output_accuracy: 0.9761 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.0497 - val_participant_output_1_accuracy: 0.1473\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s 151ms/step - loss: 0.7231 - participant_output_loss: 0.0772 - command_output_loss: 0.6458 - command_output_1_loss: 4.6649e-05 - participant_output_1_loss: 3.0092e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 0.9986 - command_output_1_accuracy: 0.1929 - participant_output_1_accuracy: 0.1643 - val_loss: 0.8954 - val_participant_output_loss: 0.1891 - val_command_output_loss: 0.7062 - val_command_output_1_loss: 5.2410e-05 - val_participant_output_1_loss: 3.9867e-05 - val_participant_output_accuracy: 0.9724 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.1031 - val_participant_output_1_accuracy: 0.1750\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s 143ms/step - loss: 0.6468 - participant_output_loss: 0.0672 - command_output_loss: 0.5795 - command_output_1_loss: 4.3599e-05 - participant_output_1_loss: 2.8850e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0443 - participant_output_1_accuracy: 0.1900 - val_loss: 0.8210 - val_participant_output_loss: 0.1719 - val_command_output_loss: 0.6490 - val_command_output_1_loss: 3.9098e-05 - val_participant_output_1_loss: 3.9573e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9613 - val_command_output_1_accuracy: 0.0442 - val_participant_output_1_accuracy: 0.1860\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s 188ms/step - loss: 0.5809 - participant_output_loss: 0.0596 - command_output_loss: 0.5213 - command_output_1_loss: 3.2368e-05 - participant_output_1_loss: 2.7856e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1386 - participant_output_1_accuracy: 0.1800 - val_loss: 0.7493 - val_participant_output_loss: 0.1537 - val_command_output_loss: 0.5955 - val_command_output_1_loss: 3.5837e-05 - val_participant_output_1_loss: 3.8835e-05 - val_participant_output_accuracy: 0.9761 - val_command_output_accuracy: 0.9650 - val_command_output_1_accuracy: 0.0608 - val_participant_output_1_accuracy: 0.1934\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s 141ms/step - loss: 0.5199 - participant_output_loss: 0.0526 - command_output_loss: 0.4672 - command_output_1_loss: 3.7839e-05 - participant_output_1_loss: 2.6923e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0171 - participant_output_1_accuracy: 0.1857 - val_loss: 0.6847 - val_participant_output_loss: 0.1425 - val_command_output_loss: 0.5421 - val_command_output_1_loss: 3.0206e-05 - val_participant_output_1_loss: 3.7976e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.0018 - val_participant_output_1_accuracy: 0.2063\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 0.4652 - participant_output_loss: 0.0465 - command_output_loss: 0.4187 - command_output_1_loss: 2.4205e-05 - participant_output_1_loss: 2.5784e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.0314 - participant_output_1_accuracy: 0.1871 - val_loss: 0.6380 - val_participant_output_loss: 0.1342 - val_command_output_loss: 0.5038 - val_command_output_1_loss: 2.7049e-05 - val_participant_output_1_loss: 3.6974e-05 - val_participant_output_accuracy: 0.9724 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.1418 - val_participant_output_1_accuracy: 0.1621\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s 178ms/step - loss: 0.4206 - participant_output_loss: 0.0419 - command_output_loss: 0.3786 - command_output_1_loss: 2.4225e-05 - participant_output_1_loss: 2.4404e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1543 - participant_output_1_accuracy: 0.1571 - val_loss: 0.5929 - val_participant_output_loss: 0.1288 - val_command_output_loss: 0.4640 - val_command_output_1_loss: 2.5188e-05 - val_participant_output_1_loss: 3.6130e-05 - val_participant_output_accuracy: 0.9742 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.1786 - val_participant_output_1_accuracy: 0.1860\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s 147ms/step - loss: 0.3786 - participant_output_loss: 0.0379 - command_output_loss: 0.3407 - command_output_1_loss: 2.2692e-05 - participant_output_1_loss: 2.3157e-05 - participant_output_accuracy: 1.0000 - command_output_accuracy: 1.0000 - command_output_1_accuracy: 0.1043 - participant_output_1_accuracy: 0.1814 - val_loss: 0.5585 - val_participant_output_loss: 0.1310 - val_command_output_loss: 0.4274 - val_command_output_1_loss: 1.8565e-05 - val_participant_output_1_loss: 3.5125e-05 - val_participant_output_accuracy: 0.9705 - val_command_output_accuracy: 0.9705 - val_command_output_1_accuracy: 0.0203 - val_participant_output_1_accuracy: 0.1381\n"
     ]
    }
   ],
   "source": [
    "high_acc = 0\n",
    "for run in range(0,10):\n",
    "    # feature extraction layers\n",
    "    resnet_model = ResNet50(input_shape=(224, 224,3), include_top=False, weights=\"imagenet\")\n",
    "    for layer in resnet_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    d = resnet_model.output.shape[-1] # dimension of last layer\n",
    "\n",
    "    ###################### model 1 ###################### \n",
    "    layer_1_0 = tf.keras.layers.Dense(d,name=\"weight_1\")(resnet_model.output) #times weight before flatten\n",
    "    layer_1_1 = tf.keras.layers.Flatten(name='flatten_1')(layer_1_0)\n",
    "\n",
    "    Dense_1_1 = tf.keras.layers.Dense(shape_1_1, activation=actv_fun_1_1,name='fc1_1')\n",
    "    layer_1_2 = Dense_1_1(layer_1_1)\n",
    "    Dense_1_2 = tf.keras.layers.Dense(shape_1_2, activation=actv_fun_1_2,name='fc1_2')\n",
    "    layer_1_3 = Dense_1_2(layer_1_2)\n",
    "\n",
    "    Dense_1_3 = tf.keras.layers.Dense(train_number, activation='softmax' ,name='participant_output')\n",
    "    out_layer_1 = Dense_1_3(layer_1_3)\n",
    "\n",
    "    ###################### model 2 ###################### \n",
    "    layer_2_0 = tf.keras.layers.Dense(d,name=\"weight_2\")(resnet_model.output) #times weight before flatten\n",
    "    layer_2_1 = tf.keras.layers.Flatten(name='flatten_2')(layer_2_0)\n",
    "\n",
    "    Dense_2_1 = tf.keras.layers.Dense(shape_2_1, activation=actv_fun_2_1,name='fc2_1')\n",
    "    layer_2_2  = Dense_2_1(layer_2_1)\n",
    "    Dense_2_2 = tf.keras.layers.Dense(shape_2_2, activation=actv_fun_2_2,name='fc2_2')\n",
    "    layer_2_3  = Dense_2_2(layer_2_2)\n",
    "\n",
    "    Dense_2_3 = tf.keras.layers.Dense(10, activation='softmax',name='command_output')\n",
    "    out_layer_2 = Dense_2_3(layer_2_3)\n",
    "\n",
    "    ###################### model 1' ###################### \n",
    "    layer_1_2_  = Dense_2_1(layer_1_1)\n",
    "    layer_1_3_  = Dense_2_2(layer_1_2_)\n",
    "    out_layer_1_ = Dense_2_3(layer_1_3_)\n",
    "\n",
    "    ###################### model 1' ###################### \n",
    "    layer_2_2_  = Dense_1_1(layer_2_1)\n",
    "    layer_2_3_  = Dense_1_2(layer_2_2_)\n",
    "    out_layer_2_ = Dense_1_3(layer_2_3_)\n",
    "\n",
    "    resnet_model = tf.keras.Model(resnet_model.input, [out_layer_1,out_layer_2,out_layer_1_,out_layer_2_])\n",
    "\n",
    "\n",
    "    # resnet_model.summary() \n",
    "\n",
    "    w_1, w_2, w_1_, w_2_ = 1,1,1,1\n",
    "    ##################### training ############################\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    callbacks = [MyEarlyStopping(monitor1 = 'val_' + resnet_model.layers[-1].name+'_accuracy',\n",
    "                                  monitor2 = 'val_' + resnet_model.layers[-2].name+'_accuracy',\n",
    "                                  patience=5,restore_best_weights=True)]\n",
    "    resnet_model.compile(optimizer=opt, loss=[\"categorical_crossentropy\",\"categorical_crossentropy\",\"mse\",\"mse\"],\n",
    "                         loss_weights=[w_1, w_2, w_1_, w_2_], metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "       \n",
    "    history1 = resnet_model.fit(Train_Inputs, \n",
    "                           {resnet_model.layers[-2].name:Train_participant_class, \n",
    "                            resnet_model.layers[-1].name:Train_command_class,\n",
    "                            resnet_model.layers[-1].name+\"_1\":Train_command_uniform, \n",
    "                            resnet_model.layers[-2].name+\"_1\":Train_participant_uniform}, \n",
    "                            validation_data=(Val_Inputs,\n",
    "                                             {resnet_model.layers[-2].name:Val_participant_class,\n",
    "                                              resnet_model.layers[-1].name:Val_command_class,\n",
    "                                              resnet_model.layers[-1].name+\"_1\":Val_command_uniform,\n",
    "                                              resnet_model.layers[-2].name+\"_1\":Val_participant_uniform}), \n",
    "                           callbacks=callbacks,\n",
    "                           batch_size=64,\n",
    "                           epochs=10)\n",
    "    \n",
    "    for layer in resnet_model.layers[0:175]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    history2 = resnet_model.fit(Train_Inputs, \n",
    "                               {resnet_model.layers[-2].name:Train_participant_class, \n",
    "                                resnet_model.layers[-1].name:Train_command_class,\n",
    "                                resnet_model.layers[-1].name+\"_1\":Train_command_uniform, \n",
    "                                resnet_model.layers[-2].name+\"_1\":Train_participant_uniform}, \n",
    "                                validation_data=(Val_Inputs,\n",
    "                                                 {resnet_model.layers[-2].name:Val_participant_class,\n",
    "                                                  resnet_model.layers[-1].name:Val_command_class,\n",
    "                                                  resnet_model.layers[-1].name+\"_1\":Val_command_uniform,\n",
    "                                                  resnet_model.layers[-2].name+\"_1\":Val_participant_uniform}), \n",
    "                               callbacks=callbacks,\n",
    "                               batch_size=64,\n",
    "                               epochs=10)\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    run_time = stop - start\n",
    "    \n",
    "    ##################### test performance ############################\n",
    "    predictions = resnet_model.predict(Test_Inputs)[0]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)+1\n",
    "    true_classes = test_unit_participant_class\n",
    "    acc_p15_s = metrics.accuracy_score(true_classes, predicted_classes).round(4)\n",
    "\n",
    "    predictions = resnet_model.predict(Test_Inputs)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class\n",
    "    acc_p15_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)\n",
    "\n",
    "\n",
    "    # test on p1\n",
    "    ## speaker classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_1)[0]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)+1\n",
    "    true_classes = np.array([1]*len(predicted_classes))\n",
    "    acc_p1_s = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_1)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_1, axis=-1)\n",
    "    acc_p1_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p2\n",
    "    ## speaker classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_2)[0]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)+1\n",
    "    true_classes = np.array([2]*len(predicted_classes))\n",
    "    acc_p2_s = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_2)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_2, axis=-1)\n",
    "    acc_p2_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p3\n",
    "    ## speaker classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_3)[0]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)+1\n",
    "    true_classes = np.array([3]*len(predicted_classes))\n",
    "    acc_p3_s = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_3)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_3, axis=-1)\n",
    "    acc_p3_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p4\n",
    "    ## speaker classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_4)[0]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)+1\n",
    "    true_classes = np.array([4]*len(predicted_classes))\n",
    "    acc_p4_s = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_4)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_4, axis=-1)\n",
    "    acc_p4_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p5\n",
    "    ## speaker classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_5)[0]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)+1\n",
    "    true_classes = np.array([5]*len(predicted_classes))\n",
    "    acc_p5_s = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_5)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_5, axis=-1)\n",
    "    acc_p5_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p6 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_6)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class_6\n",
    "    acc_p6 = metrics.accuracy_score(true_classes, predicted_classes).round(4)                \n",
    "\n",
    "    # test on p7 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_7)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class_7\n",
    "    acc_p7 = metrics.accuracy_score(true_classes, predicted_classes).round(4)    \n",
    "\n",
    "    # test on p8 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_8)[1]\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class_8\n",
    "    acc_p8 = metrics.accuracy_score(true_classes, predicted_classes).round(4)    \n",
    "\n",
    "    Perfomance = Perfomance.append({'Model': \"Group\",'Size':'mix_20%&s12_40%','Time':run_time,\n",
    "                                    'Partcp_Acc_p15':acc_p15_s,'Command_Acc_p15':acc_p15_c,'Partcp_Acc_p1':acc_p1_s,\n",
    "                                    'Command_Acc_p1':acc_p1_c,'Partcp_Acc_p2':acc_p2_s,'Command_Acc_p2':acc_p2_c,\n",
    "                                    'Partcp_Acc_p3':acc_p3_s,'Command_Acc_p3':acc_p3_c,'Partcp_Acc_p4':acc_p4_s,\n",
    "                                    'Command_Acc_p4':acc_p4_c,'Partcp_Acc_p5':acc_p5_s,'Command_Acc_p5':acc_p5_c,\n",
    "                                    'Acc_p6':acc_p6,'Acc_p7':acc_p7,'Acc_p8':acc_p8}, ignore_index=True)\n",
    "\n",
    "\n",
    "    if high_acc < acc_p15_c + acc_p15_s:\n",
    "        resnet_model.save('Initial_group_model_mix_20p_s12(40p)_0608.h5')\n",
    "        high_acc = acc_p15_c + acc_p15_s\n",
    "        best_index = run\n",
    "        pd.DataFrame.from_dict(history1.history).to_csv('mix_20p_s12(40p)_history1_0608.csv',index=False)\n",
    "        pd.DataFrame.from_dict(history2.history).to_csv('mix_20p_s12(40p)_history2_0608.csv',index=False)\n",
    "        \n",
    "    del resnet_model\n",
    "    run = run + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "0.9834 0.9705\n"
     ]
    }
   ],
   "source": [
    "resnet_model = tf.keras.models.load_model('Initial_group_model_mix_20p_s12(40p)_0608.h5')\n",
    "predictions = resnet_model.predict(Test_Inputs)[0]\n",
    "predicted_classes = np.argmax(predictions, axis=1)+1\n",
    "acc_c = round(sum(x == y for x, y in zip(test_unit_participant_class, predicted_classes)) / len(test_unit_participant_class),4)\n",
    "\n",
    "predictions = resnet_model.predict(Test_Inputs)[1]\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "acc_s = round(sum(x == y for x, y in zip(test_unit_command_class, predicted_classes)) / len(test_unit_command_class),4)\n",
    "overall_acc = acc_c + acc_s\n",
    "print(acc_c,acc_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Size</th>\n",
       "      <th>Time</th>\n",
       "      <th>Partcp_Acc_p15</th>\n",
       "      <th>Command_Acc_p15</th>\n",
       "      <th>Partcp_Acc_p1</th>\n",
       "      <th>Command_Acc_p1</th>\n",
       "      <th>Partcp_Acc_p2</th>\n",
       "      <th>Command_Acc_p2</th>\n",
       "      <th>Partcp_Acc_p3</th>\n",
       "      <th>Command_Acc_p3</th>\n",
       "      <th>Partcp_Acc_p4</th>\n",
       "      <th>Command_Acc_p4</th>\n",
       "      <th>Partcp_Acc_p5</th>\n",
       "      <th>Command_Acc_p5</th>\n",
       "      <th>Acc_p6</th>\n",
       "      <th>Acc_p7</th>\n",
       "      <th>Acc_p8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>47.540156</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.9484</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.3366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>48.307249</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>46.201615</td>\n",
       "      <td>0.9613</td>\n",
       "      <td>0.9448</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9358</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>43.692362</td>\n",
       "      <td>0.9650</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9358</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>44.504580</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>0.9484</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>44.229166</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>44.535541</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>42.948731</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.2574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>45.444911</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.9503</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Group</td>\n",
       "      <td>20%</td>\n",
       "      <td>42.707019</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.4059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>67.289367</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9640</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>64.328163</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>61.978212</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>60.036626</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>58.215136</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>49.347837</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>59.879732</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.3366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>57.413757</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9640</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>50.219540</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Group</td>\n",
       "      <td>40%</td>\n",
       "      <td>53.871787</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>80.966223</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>77.376384</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.2178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>79.340686</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>63.791425</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>78.678261</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.2376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>71.382334</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>76.130331</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9853</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>70.832085</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>73.439023</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.2277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Group</td>\n",
       "      <td>60%</td>\n",
       "      <td>72.148432</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>47.769887</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>49.705597</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.9576</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>44.363904</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9576</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9478</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.2376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>46.206470</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9650</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>47.192904</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>47.685570</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.9558</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>44.867667</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>44.795298</td>\n",
       "      <td>0.9853</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>40.871670</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.9558</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.9391</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s2_40%</td>\n",
       "      <td>42.479896</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>51.959552</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>52.339809</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9478</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9358</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>53.653035</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.8899</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>49.510542</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9640</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9304</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>49.548754</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>53.723940</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9391</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>50.595989</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9174</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>48.617480</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.2475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>47.476377</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Group</td>\n",
       "      <td>mix_20%&amp;s12_40%</td>\n",
       "      <td>48.240296</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.3069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model             Size       Time  Partcp_Acc_p15  Command_Acc_p15  \\\n",
       "0   Group              20%  47.540156          0.9724           0.9484   \n",
       "1   Group              20%  48.307249          0.9779           0.9521   \n",
       "2   Group              20%  46.201615          0.9613           0.9448   \n",
       "3   Group              20%  43.692362          0.9650           0.9521   \n",
       "4   Group              20%  44.504580          0.9687           0.9484   \n",
       "5   Group              20%  44.229166          0.9705           0.9521   \n",
       "6   Group              20%  44.535541          0.9632           0.9540   \n",
       "7   Group              20%  42.948731          0.9724           0.9595   \n",
       "8   Group              20%  45.444911          0.9724           0.9503   \n",
       "9   Group              20%  42.707019          0.9669           0.9540   \n",
       "10  Group              40%  67.289367          0.9945           0.9761   \n",
       "11  Group              40%  64.328163          0.9890           0.9761   \n",
       "12  Group              40%  61.978212          0.9926           0.9797   \n",
       "13  Group              40%  60.036626          0.9926           0.9779   \n",
       "14  Group              40%  58.215136          0.9890           0.9761   \n",
       "15  Group              40%  49.347837          0.9871           0.9761   \n",
       "16  Group              40%  59.879732          0.9908           0.9779   \n",
       "17  Group              40%  57.413757          0.9908           0.9761   \n",
       "18  Group              40%  50.219540          0.9908           0.9761   \n",
       "19  Group              40%  53.871787          0.9945           0.9742   \n",
       "20  Group              60%  80.966223          0.9926           0.9871   \n",
       "21  Group              60%  77.376384          0.9926           0.9797   \n",
       "22  Group              60%  79.340686          0.9945           0.9816   \n",
       "23  Group              60%  63.791425          0.9890           0.9871   \n",
       "24  Group              60%  78.678261          0.9926           0.9797   \n",
       "25  Group              60%  71.382334          0.9945           0.9834   \n",
       "26  Group              60%  76.130331          0.9908           0.9853   \n",
       "27  Group              60%  70.832085          0.9945           0.9871   \n",
       "28  Group              60%  73.439023          0.9945           0.9890   \n",
       "29  Group              60%  72.148432          0.9945           0.9834   \n",
       "30  Group   mix_20%&s2_40%  47.769887          0.9834           0.9669   \n",
       "31  Group   mix_20%&s2_40%  49.705597          0.9816           0.9576   \n",
       "32  Group   mix_20%&s2_40%  44.363904          0.9797           0.9576   \n",
       "33  Group   mix_20%&s2_40%  46.206470          0.9945           0.9650   \n",
       "34  Group   mix_20%&s2_40%  47.192904          0.9797           0.9632   \n",
       "35  Group   mix_20%&s2_40%  47.685570          0.9761           0.9558   \n",
       "36  Group   mix_20%&s2_40%  44.867667          0.9816           0.9687   \n",
       "37  Group   mix_20%&s2_40%  44.795298          0.9853           0.9724   \n",
       "38  Group   mix_20%&s2_40%  40.871670          0.9742           0.9558   \n",
       "39  Group   mix_20%&s2_40%  42.479896          0.9834           0.9687   \n",
       "40  Group  mix_20%&s12_40%  51.959552          0.9834           0.9687   \n",
       "41  Group  mix_20%&s12_40%  52.339809          0.9705           0.9669   \n",
       "42  Group  mix_20%&s12_40%  53.653035          0.9724           0.9705   \n",
       "43  Group  mix_20%&s12_40%  49.510542          0.9705           0.9669   \n",
       "44  Group  mix_20%&s12_40%  49.548754          0.9834           0.9669   \n",
       "45  Group  mix_20%&s12_40%  53.723940          0.9761           0.9669   \n",
       "46  Group  mix_20%&s12_40%  50.595989          0.9761           0.9595   \n",
       "47  Group  mix_20%&s12_40%  48.617480          0.9797           0.9632   \n",
       "48  Group  mix_20%&s12_40%  47.476377          0.9834           0.9705   \n",
       "49  Group  mix_20%&s12_40%  48.240296          0.9797           0.9687   \n",
       "\n",
       "    Partcp_Acc_p1  Command_Acc_p1  Partcp_Acc_p2  Command_Acc_p2  \\\n",
       "0           0.991          0.9550         0.9259          0.8796   \n",
       "1           1.000          0.9369         0.9352          0.8796   \n",
       "2           0.991          0.9369         0.9074          0.8611   \n",
       "3           1.000          0.9459         0.9074          0.8704   \n",
       "4           0.991          0.9550         0.9259          0.8796   \n",
       "5           1.000          0.9550         0.9074          0.8796   \n",
       "6           0.991          0.9369         0.9259          0.8981   \n",
       "7           1.000          0.9550         0.9074          0.8889   \n",
       "8           0.991          0.9459         0.9167          0.8796   \n",
       "9           0.991          0.9459         0.9167          0.8704   \n",
       "10          0.991          0.9640         0.9907          0.9352   \n",
       "11          0.982          0.9730         0.9815          0.9259   \n",
       "12          0.991          0.9730         0.9907          0.9444   \n",
       "13          0.991          0.9730         0.9907          0.9352   \n",
       "14          0.982          0.9730         0.9815          0.9352   \n",
       "15          0.982          0.9730         0.9815          0.9352   \n",
       "16          0.991          0.9730         0.9815          0.9444   \n",
       "17          0.991          0.9640         0.9815          0.9444   \n",
       "18          0.991          0.9730         0.9815          0.9352   \n",
       "19          1.000          0.9730         0.9907          0.9259   \n",
       "20          0.991          0.9820         0.9815          0.9722   \n",
       "21          0.982          0.9820         1.0000          0.9537   \n",
       "22          0.982          0.9820         1.0000          0.9444   \n",
       "23          0.973          0.9910         0.9815          0.9630   \n",
       "24          0.982          0.9820         1.0000          0.9537   \n",
       "25          0.982          0.9820         1.0000          0.9537   \n",
       "26          0.982          0.9910         0.9907          0.9537   \n",
       "27          0.991          0.9820         0.9907          0.9722   \n",
       "28          0.982          0.9910         1.0000          0.9722   \n",
       "29          0.982          0.9910         1.0000          0.9444   \n",
       "30          0.991          0.9459         0.9815          0.9352   \n",
       "31          0.991          0.9369         1.0000          0.9259   \n",
       "32          0.991          0.9550         0.9815          0.9259   \n",
       "33          0.991          0.9550         1.0000          0.9259   \n",
       "34          0.982          0.9369         0.9907          0.9444   \n",
       "35          0.991          0.9279         0.9907          0.9352   \n",
       "36          0.991          0.9550         1.0000          0.9537   \n",
       "37          1.000          0.9550         1.0000          0.9630   \n",
       "38          1.000          0.9459         0.9815          0.9167   \n",
       "39          0.982          0.9550         1.0000          0.9352   \n",
       "40          1.000          0.9730         0.9907          0.9444   \n",
       "41          1.000          0.9730         0.9907          0.9444   \n",
       "42          1.000          0.9910         0.9907          0.9259   \n",
       "43          0.991          0.9640         0.9907          0.9444   \n",
       "44          1.000          0.9550         0.9907          0.9444   \n",
       "45          0.991          0.9820         0.9907          0.9444   \n",
       "46          1.000          0.9550         0.9815          0.9352   \n",
       "47          0.991          0.9730         0.9907          0.9352   \n",
       "48          0.991          0.9910         0.9815          0.9259   \n",
       "49          1.000          0.9730         0.9907          0.9444   \n",
       "\n",
       "    Partcp_Acc_p3  Command_Acc_p3  Partcp_Acc_p4  Command_Acc_p4  \\\n",
       "0          0.9652          0.9565         0.9817          0.9633   \n",
       "1          0.9739          0.9652         0.9908          0.9908   \n",
       "2          0.9826          0.9652         0.9358          0.9725   \n",
       "3          0.9826          0.9652         0.9358          0.9908   \n",
       "4          0.9739          0.9652         0.9541          0.9541   \n",
       "5          0.9826          0.9739         0.9633          0.9633   \n",
       "6          0.9565          0.9652         0.9541          0.9817   \n",
       "7          0.9913          0.9739         0.9633          0.9908   \n",
       "8          0.9913          0.9652         0.9633          0.9725   \n",
       "9          0.9739          0.9652         0.9541          0.9908   \n",
       "10         1.0000          0.9913         0.9908          0.9908   \n",
       "11         0.9913          0.9913         0.9908          0.9908   \n",
       "12         0.9913          0.9913         0.9908          0.9908   \n",
       "13         0.9913          0.9913         0.9908          0.9908   \n",
       "14         0.9913          0.9826         0.9908          0.9908   \n",
       "15         0.9913          0.9913         0.9817          0.9908   \n",
       "16         0.9913          0.9826         0.9908          0.9908   \n",
       "17         0.9913          0.9826         0.9908          0.9908   \n",
       "18         0.9913          0.9913         0.9908          0.9908   \n",
       "19         0.9913          0.9913         0.9908          0.9817   \n",
       "20         1.0000          0.9913         0.9908          0.9908   \n",
       "21         0.9913          0.9826         0.9908          0.9817   \n",
       "22         1.0000          0.9913         0.9908          0.9908   \n",
       "23         1.0000          0.9913         0.9908          0.9908   \n",
       "24         0.9913          0.9826         0.9908          0.9817   \n",
       "25         1.0000          0.9913         0.9908          0.9908   \n",
       "26         0.9913          0.9913         0.9908          0.9908   \n",
       "27         1.0000          0.9913         0.9908          0.9908   \n",
       "28         1.0000          0.9913         0.9908          0.9908   \n",
       "29         1.0000          0.9913         0.9908          0.9908   \n",
       "30         0.9739          0.9826         0.9725          0.9817   \n",
       "31         0.9739          0.9652         0.9450          0.9817   \n",
       "32         0.9652          0.9478         0.9908          0.9725   \n",
       "33         0.9913          0.9652         0.9908          0.9908   \n",
       "34         0.9739          0.9652         0.9541          0.9817   \n",
       "35         0.9565          0.9565         0.9633          0.9817   \n",
       "36         0.9652          0.9652         0.9541          0.9817   \n",
       "37         0.9652          0.9739         0.9633          0.9817   \n",
       "38         0.9391          0.9565         0.9541          0.9725   \n",
       "39         0.9739          0.9826         0.9633          0.9817   \n",
       "40         0.9826          0.9565         0.9541          0.9817   \n",
       "41         0.9478          0.9652         0.9358          0.9725   \n",
       "42         0.9826          0.9652         0.8899          0.9817   \n",
       "43         0.9304          0.9565         0.9541          0.9725   \n",
       "44         0.9826          0.9652         0.9450          0.9817   \n",
       "45         0.9739          0.9391         0.9450          0.9817   \n",
       "46         0.9826          0.9565         0.9174          0.9633   \n",
       "47         0.9652          0.9565         0.9541          0.9725   \n",
       "48         0.9739          0.9739         0.9725          0.9817   \n",
       "49         0.9652          0.9565         0.9450          0.9817   \n",
       "\n",
       "    Partcp_Acc_p5  Command_Acc_p5  Acc_p6  Acc_p7  Acc_p8  \n",
       "0            1.00            0.99    0.67   0.528  0.3366  \n",
       "1            0.99            0.99    0.59   0.528  0.2970  \n",
       "2            0.99            0.99    0.67   0.432  0.2673  \n",
       "3            1.00            0.99    0.62   0.504  0.2673  \n",
       "4            1.00            0.99    0.59   0.472  0.3267  \n",
       "5            1.00            0.99    0.62   0.456  0.3069  \n",
       "6            0.99            0.99    0.67   0.496  0.3267  \n",
       "7            1.00            0.99    0.67   0.496  0.2574  \n",
       "8            1.00            0.99    0.60   0.552  0.2673  \n",
       "9            1.00            1.00    0.66   0.480  0.4059  \n",
       "10           1.00            1.00    0.65   0.472  0.2772  \n",
       "11           1.00            1.00    0.63   0.480  0.2871  \n",
       "12           1.00            1.00    0.69   0.400  0.3168  \n",
       "13           1.00            1.00    0.70   0.456  0.3267  \n",
       "14           1.00            1.00    0.70   0.448  0.2871  \n",
       "15           1.00            0.99    0.70   0.472  0.2772  \n",
       "16           1.00            1.00    0.70   0.440  0.3366  \n",
       "17           1.00            1.00    0.67   0.440  0.3168  \n",
       "18           1.00            0.99    0.63   0.456  0.2772  \n",
       "19           1.00            1.00    0.65   0.424  0.2673  \n",
       "20           1.00            1.00    0.69   0.440  0.2772  \n",
       "21           1.00            1.00    0.72   0.456  0.2178  \n",
       "22           1.00            1.00    0.68   0.440  0.2970  \n",
       "23           1.00            1.00    0.68   0.424  0.3069  \n",
       "24           1.00            1.00    0.68   0.424  0.2376  \n",
       "25           1.00            1.00    0.66   0.408  0.2475  \n",
       "26           1.00            1.00    0.73   0.416  0.2673  \n",
       "27           1.00            1.00    0.70   0.440  0.2673  \n",
       "28           1.00            1.00    0.70   0.424  0.2277  \n",
       "29           1.00            1.00    0.70   0.416  0.2772  \n",
       "30           1.00            0.99    0.69   0.552  0.3267  \n",
       "31           1.00            0.98    0.69   0.512  0.3168  \n",
       "32           0.97            0.99    0.72   0.536  0.2376  \n",
       "33           1.00            0.99    0.70   0.480  0.2673  \n",
       "34           1.00            0.99    0.69   0.504  0.2871  \n",
       "35           0.98            0.98    0.72   0.472  0.2871  \n",
       "36           1.00            0.99    0.66   0.496  0.2673  \n",
       "37           1.00            0.99    0.68   0.544  0.3069  \n",
       "38           1.00            0.99    0.71   0.480  0.2970  \n",
       "39           1.00            0.99    0.73   0.536  0.2673  \n",
       "40           0.99            0.99    0.66   0.504  0.3168  \n",
       "41           0.98            0.98    0.67   0.424  0.2673  \n",
       "42           1.00            0.99    0.67   0.544  0.3168  \n",
       "43           0.99            1.00    0.67   0.464  0.3168  \n",
       "44           1.00            0.99    0.65   0.488  0.2970  \n",
       "45           0.98            0.99    0.66   0.488  0.3069  \n",
       "46           1.00            0.99    0.64   0.456  0.3069  \n",
       "47           1.00            0.98    0.65   0.480  0.2475  \n",
       "48           1.00            0.98    0.59   0.472  0.3168  \n",
       "49           1.00            0.99    0.69   0.496  0.3069  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perfomance.to_csv('Performance_0608_training_Data_Size.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
