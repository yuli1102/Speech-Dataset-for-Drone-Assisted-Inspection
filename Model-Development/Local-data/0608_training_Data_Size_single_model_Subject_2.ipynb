{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"   #(xxxx is your specific GPU ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from MyEarlyStopping import MyEarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# from keras.optimizers import adam\n",
    "\n",
    "import timeit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import LabelEncoder  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training dataset (40%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1714 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "train_generator = train_data.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_Data/train',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = train_generator.filenames\n",
    "image_no = [i.split(\"/\")[1].split(\"_\")[2].split(\".\")[0] for i in image_names]\n",
    "image_no = np.array(list(map(int, image_no)))\n",
    "ALL_participant_class = [i.split(\"/\")[1].split(\"_\")[1] for i in image_names]\n",
    "ALL_participant_class = np.array(list(map(int, ALL_participant_class)))\n",
    "command_class = train_generator.classes\n",
    "All_command_class = tf.keras.utils.to_categorical(command_class, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Inputs = [next(train_generator)[0][0] for _ in range(len(train_generator))]\n",
    "All_Inputs = np.array(All_Inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 2\n",
    "train_image = 10 #10:20%, 20: 40%, 30:60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10., 10., 10., 10., 10., 10., 10., 10., 10., 10.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_indexs_train = (image_no<train_image)&(ALL_participant_class==subject)\n",
    "Train_Inputs = All_Inputs[select_indexs_train]\n",
    "Train_command_class = All_command_class[select_indexs_train]\n",
    "sum(Train_command_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 543 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_generator = val_data.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_Data/validation',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = val_generator.filenames\n",
    "participant_class = [i.split(\"/\", 1)[1].split(\"_\")[1] for i in image_names]\n",
    "participant_class = np.array(list(map(int, participant_class)))\n",
    "command_class = val_generator.classes\n",
    "Val_command_class = tf.keras.utils.to_categorical(command_class, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_Val_Inputs = [next(val_generator)[0][0] for _ in range(len(val_generator))]\n",
    "ALL_Val_Inputs = np.array(ALL_Val_Inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11., 11., 10., 11., 11., 11., 11., 10., 11., 11.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_indexs_val = participant_class==subject\n",
    "Val_Inputs = ALL_Val_Inputs[select_indexs_val]\n",
    "Val_command_class = Val_command_class[select_indexs_val]\n",
    "sum(Val_command_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 543 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator = test_data.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_Data/test',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = test_generator.filenames\n",
    "participant_class = [i.split(\"/\", 1)[1].split(\"_\")[1] for i in image_names]\n",
    "test_unit_participant_class = np.array(list(map(int, participant_class)))\n",
    "test_unit_command_class = test_generator.classes\n",
    "Test_command_class = tf.keras.utils.to_categorical(test_unit_command_class, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Inputs = [next(test_generator)[0][0] for _ in range(len(test_generator))]\n",
    "Test_Inputs = np.array(Test_Inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Inputs_1 = Test_Inputs[np.where(test_unit_participant_class == 1)]\n",
    "Test_command_class_1 = Test_command_class[np.where(test_unit_participant_class == 1)]\n",
    "Test_Inputs_2 = Test_Inputs[np.where(test_unit_participant_class == 2)]\n",
    "Test_command_class_2 = Test_command_class[np.where(test_unit_participant_class == 2)]\n",
    "Test_Inputs_3 = Test_Inputs[np.where(test_unit_participant_class == 3)]\n",
    "Test_command_class_3 = Test_command_class[np.where(test_unit_participant_class == 3)]\n",
    "Test_Inputs_4 = Test_Inputs[np.where(test_unit_participant_class == 4)]\n",
    "Test_command_class_4 = Test_command_class[np.where(test_unit_participant_class == 4)]\n",
    "Test_Inputs_5 = Test_Inputs[np.where(test_unit_participant_class == 5)]\n",
    "Test_command_class_5 = Test_command_class[np.where(test_unit_participant_class == 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(Test_command_class_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker 6 Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_6 = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator_6 = test_data_6.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_subject/p_6_split/test',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = test_generator_6.filenames\n",
    "command_class = [i.split(\"/\", 1)[0]for i in image_names]\n",
    "le = LabelEncoder()\n",
    "test_unit_command_class_6 = le.fit_transform(command_class)\n",
    "Test_command_class_6 = tf.keras.utils.to_categorical(test_unit_command_class_6, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Inputs_6 = [next(test_generator_6)[0][0] for _ in range(len(test_generator_6))]\n",
    "Test_Inputs_6 = np.array(Test_Inputs_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker 7 Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_7 = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator_7 = test_data_6.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_subject/p_7_split/test',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = test_generator_7.filenames\n",
    "command_class = [i.split(\"/\", 1)[0]for i in image_names]\n",
    "le = LabelEncoder()\n",
    "test_unit_command_class_7 = le.fit_transform(command_class)\n",
    "Test_command_class_7 = tf.keras.utils.to_categorical(test_unit_command_class_7, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Inputs_7 = [next(test_generator_7)[0][0] for _ in range(len(test_generator_7))]\n",
    "Test_Inputs_7 = np.array(Test_Inputs_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker 8 Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 101 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_8 = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator_8 = test_data_6.flow_from_directory('/data/home/cou/yuli5/semg/Audio_MEL_subject/p_8_split/test',\n",
    "                                                batch_size=1,\n",
    "                                                target_size=(224,224),\n",
    "                                                class_mode='categorical',\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = test_generator_8.filenames\n",
    "command_class = [i.split(\"/\", 1)[0]for i in image_names]\n",
    "le = LabelEncoder()\n",
    "test_unit_command_class_8 = le.fit_transform(command_class)\n",
    "Test_command_class_8 = tf.keras.utils.to_categorical(test_unit_command_class_8, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Inputs_8 = [next(test_generator_8)[0][0] for _ in range(len(test_generator_8))]\n",
    "Test_Inputs_8 = np.array(Test_Inputs_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pd file store performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Time</th>\n",
       "      <th>Command_Acc_p15</th>\n",
       "      <th>Command_Acc_p1</th>\n",
       "      <th>Command_Acc_p2</th>\n",
       "      <th>Command_Acc_p3</th>\n",
       "      <th>Command_Acc_p4</th>\n",
       "      <th>Command_Acc_p5</th>\n",
       "      <th>Acc_p6</th>\n",
       "      <th>Acc_p7</th>\n",
       "      <th>Acc_p8</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.269878</td>\n",
       "      <td>0.6243</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>12.062418</td>\n",
       "      <td>0.5709</td>\n",
       "      <td>0.9189</td>\n",
       "      <td>0.3148</td>\n",
       "      <td>0.4696</td>\n",
       "      <td>0.5413</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.520935</td>\n",
       "      <td>0.6335</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.6514</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.664869</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.2685</td>\n",
       "      <td>0.5217</td>\n",
       "      <td>0.6789</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.2970</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.347304</td>\n",
       "      <td>0.6188</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.2574</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.131006</td>\n",
       "      <td>0.5672</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>0.4696</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.651466</td>\n",
       "      <td>0.5930</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.2685</td>\n",
       "      <td>0.5130</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>12.100064</td>\n",
       "      <td>0.5727</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>0.4957</td>\n",
       "      <td>0.5413</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2574</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.176442</td>\n",
       "      <td>0.6151</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.3611</td>\n",
       "      <td>0.4957</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.929373</td>\n",
       "      <td>0.6446</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.4722</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.902663</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.6514</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.2178</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.805319</td>\n",
       "      <td>0.6538</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3519</td>\n",
       "      <td>0.5913</td>\n",
       "      <td>0.6697</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.685424</td>\n",
       "      <td>0.6298</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.194757</td>\n",
       "      <td>0.6390</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3981</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.364569</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>0.5739</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.957827</td>\n",
       "      <td>0.6317</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.3981</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.528849</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.4352</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.993745</td>\n",
       "      <td>0.6538</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.3611</td>\n",
       "      <td>0.5913</td>\n",
       "      <td>0.6697</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.823134</td>\n",
       "      <td>0.6446</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.6606</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.812350</td>\n",
       "      <td>0.6354</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>0.5304</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.680686</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>0.5913</td>\n",
       "      <td>0.6697</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.529958</td>\n",
       "      <td>0.6851</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.4259</td>\n",
       "      <td>0.5826</td>\n",
       "      <td>0.7248</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2178</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>17.791133</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4352</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.228115</td>\n",
       "      <td>0.6483</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.6697</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.815590</td>\n",
       "      <td>0.6740</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.099657</td>\n",
       "      <td>0.6980</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4630</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.7523</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.533707</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.3981</td>\n",
       "      <td>0.5826</td>\n",
       "      <td>0.6514</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.714685</td>\n",
       "      <td>0.6501</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.6606</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.797302</td>\n",
       "      <td>0.6556</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4259</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.6514</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>19.045552</td>\n",
       "      <td>0.6924</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5463</td>\n",
       "      <td>0.5739</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model       Time  Command_Acc_p15  Command_Acc_p1  Command_Acc_p2  \\\n",
       "0   Subjet_1  15.269878           0.6243          0.9369          0.3889   \n",
       "1   Subjet_1  12.062418           0.5709          0.9189          0.3148   \n",
       "2   Subjet_1  13.520935           0.6335          0.9550          0.3889   \n",
       "3   Subjet_1  14.664869           0.6225          0.9550          0.2685   \n",
       "4   Subjet_1  13.347304           0.6188          0.9279          0.3796   \n",
       "5   Subjet_1  14.131006           0.5672          0.9279          0.2870   \n",
       "6   Subjet_1  13.651466           0.5930          0.9459          0.2685   \n",
       "7   Subjet_1  12.100064           0.5727          0.9279          0.2870   \n",
       "8   Subjet_1  13.176442           0.6151          0.9459          0.3611   \n",
       "9   Subjet_1  13.929373           0.6446          0.9369          0.4722   \n",
       "10  Subjet_1  15.902663           0.6409          0.9820          0.3704   \n",
       "11  Subjet_1  13.805319           0.6538          1.0000          0.3519   \n",
       "12  Subjet_1  15.685424           0.6298          0.9820          0.3333   \n",
       "13  Subjet_1  16.194757           0.6390          0.9820          0.3981   \n",
       "14  Subjet_1  15.364569           0.6611          0.9820          0.3796   \n",
       "15  Subjet_1  14.957827           0.6317          0.9730          0.3981   \n",
       "16  Subjet_1  15.528849           0.6409          0.9910          0.4352   \n",
       "17  Subjet_1  15.993745           0.6538          0.9910          0.3611   \n",
       "18  Subjet_1  16.823134           0.6446          0.9730          0.3704   \n",
       "19  Subjet_1  15.812350           0.6354          0.9820          0.3889   \n",
       "20  Subjet_1  16.680686           0.6667          1.0000          0.3889   \n",
       "21  Subjet_1  14.529958           0.6851          0.9910          0.4259   \n",
       "22  Subjet_1  17.791133           0.6796          1.0000          0.4352   \n",
       "23  Subjet_1  16.228115           0.6483          1.0000          0.3704   \n",
       "24  Subjet_1  14.815590           0.6740          1.0000          0.4167   \n",
       "25  Subjet_1  16.099657           0.6980          1.0000          0.4630   \n",
       "26  Subjet_1  14.533707           0.6667          0.9910          0.3981   \n",
       "27  Subjet_1  15.714685           0.6501          1.0000          0.4167   \n",
       "28  Subjet_1  15.797302           0.6556          1.0000          0.4259   \n",
       "29  Subjet_1  19.045552           0.6924          1.0000          0.5463   \n",
       "\n",
       "    Command_Acc_p3  Command_Acc_p4  Command_Acc_p5  Acc_p6  Acc_p7  Acc_p8  \\\n",
       "0           0.5391          0.6239            0.63    0.38   0.384  0.2673   \n",
       "1           0.4696          0.5413            0.61    0.33   0.320  0.2673   \n",
       "2           0.5478          0.6514            0.62    0.48   0.384  0.2475   \n",
       "3           0.5217          0.6789            0.69    0.26   0.456  0.2970   \n",
       "4           0.5652          0.5780            0.64    0.39   0.416  0.2574   \n",
       "5           0.4696          0.5780            0.57    0.25   0.328  0.2673   \n",
       "6           0.5130          0.6239            0.61    0.43   0.440  0.2673   \n",
       "7           0.4957          0.5413            0.61    0.41   0.384  0.2574   \n",
       "8           0.4957          0.6147            0.66    0.41   0.408  0.2871   \n",
       "9           0.5391          0.6147            0.66    0.48   0.408  0.2673   \n",
       "10          0.5565          0.6514            0.64    0.34   0.400  0.2178   \n",
       "11          0.5913          0.6697            0.65    0.45   0.360  0.2277   \n",
       "12          0.5391          0.6422            0.65    0.43   0.408  0.2277   \n",
       "13          0.6000          0.6147            0.59    0.36   0.384  0.2475   \n",
       "14          0.5739          0.6972            0.67    0.44   0.400  0.2673   \n",
       "15          0.5652          0.6239            0.59    0.38   0.440  0.2475   \n",
       "16          0.5565          0.6239            0.59    0.37   0.360  0.2079   \n",
       "17          0.5913          0.6697            0.65    0.41   0.360  0.2277   \n",
       "18          0.5478          0.6606            0.67    0.37   0.384  0.2475   \n",
       "19          0.5304          0.6422            0.63    0.33   0.408  0.2772   \n",
       "20          0.5913          0.6697            0.68    0.43   0.352  0.2079   \n",
       "21          0.5826          0.7248            0.70    0.42   0.360  0.2178   \n",
       "22          0.5652          0.7156            0.68    0.36   0.328  0.2277   \n",
       "23          0.5565          0.6697            0.64    0.41   0.344  0.2079   \n",
       "24          0.5565          0.7339            0.66    0.45   0.344  0.2079   \n",
       "25          0.6000          0.7523            0.67    0.37   0.352  0.2079   \n",
       "26          0.5826          0.6514            0.71    0.36   0.360  0.2277   \n",
       "27          0.5391          0.6606            0.63    0.38   0.360  0.2277   \n",
       "28          0.5565          0.6514            0.64    0.39   0.352  0.2079   \n",
       "29          0.5739          0.7156            0.62    0.41   0.376  0.2277   \n",
       "\n",
       "    Size  \n",
       "0     10  \n",
       "1     10  \n",
       "2     10  \n",
       "3     10  \n",
       "4     10  \n",
       "5     10  \n",
       "6     10  \n",
       "7     10  \n",
       "8     10  \n",
       "9     10  \n",
       "10    20  \n",
       "11    20  \n",
       "12    20  \n",
       "13    20  \n",
       "14    20  \n",
       "15    20  \n",
       "16    20  \n",
       "17    20  \n",
       "18    20  \n",
       "19    20  \n",
       "20    30  \n",
       "21    30  \n",
       "22    30  \n",
       "23    30  \n",
       "24    30  \n",
       "25    30  \n",
       "26    30  \n",
       "27    30  \n",
       "28    30  \n",
       "29    30  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd file store performance\n",
    "# Perfomance = pd.DataFrame(columns = ['Model','Time','Command_Acc_p15','Command_Acc_p1','Command_Acc_p2',\n",
    "#                                      'Command_Acc_p3','Command_Acc_p4','Command_Acc_p5','Acc_p6','Acc_p7','Acc_p8'])\n",
    "\n",
    "Perfomance = pd.read_csv('Performance_0608_single_model.csv')\n",
    "# Perfomance\n",
    "Perfomance                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "actv_fun_1_1 = \"relu\" \n",
    "actv_fun_1_2 = \"sigmoid\"\n",
    "shape_1_1 = 128\n",
    "shape_1_2 = 256\n",
    "actv_fun_2_1 = \"sigmoid\" \n",
    "actv_fun_2_2 = \"sigmoid\"\n",
    "shape_2_1 = 512\n",
    "shape_2_2 = 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single model （size:10）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 2s 940ms/step - loss: 2.4774 - accuracy: 0.0800 - val_loss: 2.2043 - val_accuracy: 0.4907\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 275ms/step - loss: 2.1630 - accuracy: 0.6000 - val_loss: 2.0980 - val_accuracy: 0.5926\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 1s 285ms/step - loss: 2.0399 - accuracy: 0.7100 - val_loss: 2.0006 - val_accuracy: 0.7500\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 1s 277ms/step - loss: 1.9459 - accuracy: 0.8200 - val_loss: 1.9270 - val_accuracy: 0.7870\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 1s 263ms/step - loss: 1.8744 - accuracy: 0.8500 - val_loss: 1.8859 - val_accuracy: 0.7963\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 1.8247 - accuracy: 0.8300 - val_loss: 1.8430 - val_accuracy: 0.7963\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 1s 293ms/step - loss: 1.7799 - accuracy: 0.8300 - val_loss: 1.7938 - val_accuracy: 0.8796\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 1.7312 - accuracy: 0.8700 - val_loss: 1.7621 - val_accuracy: 0.8333\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 1s 293ms/step - loss: 1.6969 - accuracy: 0.8600 - val_loss: 1.7259 - val_accuracy: 0.8889\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 1.6565 - accuracy: 0.8900 - val_loss: 1.6939 - val_accuracy: 0.8704\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 315ms/step - loss: 1.6251 - accuracy: 0.9200 - val_loss: 1.6671 - val_accuracy: 0.8889\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 1.5921 - accuracy: 0.9600 - val_loss: 1.6397 - val_accuracy: 0.9259\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.5602 - accuracy: 0.9900 - val_loss: 1.6123 - val_accuracy: 0.9259\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 1.5312 - accuracy: 1.0000 - val_loss: 1.5891 - val_accuracy: 0.9259\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 1.5066 - accuracy: 1.0000 - val_loss: 1.5638 - val_accuracy: 0.9259\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 1s 278ms/step - loss: 1.4783 - accuracy: 1.0000 - val_loss: 1.5400 - val_accuracy: 0.9352\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 1s 276ms/step - loss: 1.4538 - accuracy: 1.0000 - val_loss: 1.5180 - val_accuracy: 0.9444\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.4292 - accuracy: 1.0000 - val_loss: 1.4982 - val_accuracy: 0.9352\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 1.4046 - accuracy: 1.0000 - val_loss: 1.4795 - val_accuracy: 0.9444\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 1.3807 - accuracy: 1.0000 - val_loss: 1.4570 - val_accuracy: 0.9259\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 729ms/step - loss: 2.5216 - accuracy: 0.0500 - val_loss: 2.2296 - val_accuracy: 0.2407\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 296ms/step - loss: 2.1948 - accuracy: 0.2800 - val_loss: 2.1353 - val_accuracy: 0.3426\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 1s 284ms/step - loss: 2.0864 - accuracy: 0.4600 - val_loss: 2.0551 - val_accuracy: 0.4722\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 1s 271ms/step - loss: 2.0157 - accuracy: 0.5700 - val_loss: 2.0110 - val_accuracy: 0.5278\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 1.9662 - accuracy: 0.6800 - val_loss: 1.9660 - val_accuracy: 0.6667\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 1s 293ms/step - loss: 1.9231 - accuracy: 0.7700 - val_loss: 1.9484 - val_accuracy: 0.7407\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 1s 276ms/step - loss: 1.8941 - accuracy: 0.8200 - val_loss: 1.9133 - val_accuracy: 0.8056\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 1.8607 - accuracy: 0.9100 - val_loss: 1.8838 - val_accuracy: 0.7963\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 1s 297ms/step - loss: 1.8334 - accuracy: 0.9000 - val_loss: 1.8613 - val_accuracy: 0.8148\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 1.8104 - accuracy: 0.9300 - val_loss: 1.8447 - val_accuracy: 0.8056\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 322ms/step - loss: 1.7877 - accuracy: 0.9400 - val_loss: 1.8230 - val_accuracy: 0.8148\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 1.7626 - accuracy: 0.9500 - val_loss: 1.8032 - val_accuracy: 0.7963\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 1s 287ms/step - loss: 1.7428 - accuracy: 0.9500 - val_loss: 1.7842 - val_accuracy: 0.8241\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 1s 290ms/step - loss: 1.7207 - accuracy: 0.9500 - val_loss: 1.7638 - val_accuracy: 0.8611\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 1s 277ms/step - loss: 1.6998 - accuracy: 0.9600 - val_loss: 1.7455 - val_accuracy: 0.8889\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 1.6772 - accuracy: 0.9700 - val_loss: 1.7255 - val_accuracy: 0.8889\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 1.6540 - accuracy: 0.9800 - val_loss: 1.7060 - val_accuracy: 0.9167\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 1.6332 - accuracy: 0.9900 - val_loss: 1.6828 - val_accuracy: 0.9167\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 1.6116 - accuracy: 0.9900 - val_loss: 1.6654 - val_accuracy: 0.9074\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 1.5906 - accuracy: 0.9900 - val_loss: 1.6521 - val_accuracy: 0.9167\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 700ms/step - loss: 2.3940 - accuracy: 0.1000 - val_loss: 2.1650 - val_accuracy: 0.3981\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.1208 - accuracy: 0.3600 - val_loss: 2.0144 - val_accuracy: 0.3611\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 1.9677 - accuracy: 0.5100 - val_loss: 1.9338 - val_accuracy: 0.6852\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 1.8718 - accuracy: 0.8200 - val_loss: 1.8467 - val_accuracy: 0.6111\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 1.7831 - accuracy: 0.7600 - val_loss: 1.8043 - val_accuracy: 0.5648\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 1s 316ms/step - loss: 1.7252 - accuracy: 0.8300 - val_loss: 1.7548 - val_accuracy: 0.7778\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 1s 289ms/step - loss: 1.6736 - accuracy: 0.9100 - val_loss: 1.7083 - val_accuracy: 0.7870\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 1s 270ms/step - loss: 1.6279 - accuracy: 0.8500 - val_loss: 1.6730 - val_accuracy: 0.7963\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 1s 294ms/step - loss: 1.5879 - accuracy: 0.8600 - val_loss: 1.6354 - val_accuracy: 0.8333\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 1.5490 - accuracy: 0.9000 - val_loss: 1.6003 - val_accuracy: 0.8333\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 305ms/step - loss: 1.5079 - accuracy: 0.9100 - val_loss: 1.5709 - val_accuracy: 0.8426\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 1.4695 - accuracy: 0.9000 - val_loss: 1.5388 - val_accuracy: 0.8426\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 1.4319 - accuracy: 0.9200 - val_loss: 1.4992 - val_accuracy: 0.8426\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 1.3945 - accuracy: 0.9100 - val_loss: 1.4703 - val_accuracy: 0.8241\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 1.3658 - accuracy: 0.9100 - val_loss: 1.4380 - val_accuracy: 0.8333\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 1s 250ms/step - loss: 1.3276 - accuracy: 0.9300 - val_loss: 1.4129 - val_accuracy: 0.8333\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 2s 815ms/step - loss: 2.4402 - accuracy: 0.0900 - val_loss: 2.2526 - val_accuracy: 0.2130\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 310ms/step - loss: 2.2165 - accuracy: 0.4000 - val_loss: 2.1185 - val_accuracy: 0.6667\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 272ms/step - loss: 2.0835 - accuracy: 0.6900 - val_loss: 2.0269 - val_accuracy: 0.6852\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 1s 297ms/step - loss: 1.9883 - accuracy: 0.6900 - val_loss: 1.9575 - val_accuracy: 0.7037\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 1.9166 - accuracy: 0.6900 - val_loss: 1.9022 - val_accuracy: 0.7037\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.8579 - accuracy: 0.7100 - val_loss: 1.8703 - val_accuracy: 0.7407\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 1s 284ms/step - loss: 1.8175 - accuracy: 0.8100 - val_loss: 1.8379 - val_accuracy: 0.8241\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 1s 288ms/step - loss: 1.7776 - accuracy: 0.9000 - val_loss: 1.8053 - val_accuracy: 0.8611\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 1.7450 - accuracy: 0.9600 - val_loss: 1.7832 - val_accuracy: 0.8611\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 1.7196 - accuracy: 0.9400 - val_loss: 1.7567 - val_accuracy: 0.8519\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 335ms/step - loss: 1.6932 - accuracy: 0.9300 - val_loss: 1.7345 - val_accuracy: 0.8519\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 298ms/step - loss: 1.6665 - accuracy: 0.9000 - val_loss: 1.7070 - val_accuracy: 0.9074\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 1s 276ms/step - loss: 1.6375 - accuracy: 0.9700 - val_loss: 1.6856 - val_accuracy: 0.9167\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 1.6125 - accuracy: 1.0000 - val_loss: 1.6614 - val_accuracy: 0.9167\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 1s 298ms/step - loss: 1.5845 - accuracy: 1.0000 - val_loss: 1.6401 - val_accuracy: 0.9259\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 1.5600 - accuracy: 1.0000 - val_loss: 1.6204 - val_accuracy: 0.9167\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 1.5349 - accuracy: 1.0000 - val_loss: 1.5981 - val_accuracy: 0.9074\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 1.5102 - accuracy: 0.9900 - val_loss: 1.5723 - val_accuracy: 0.9074\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 1.4846 - accuracy: 1.0000 - val_loss: 1.5518 - val_accuracy: 0.9167\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 1s 281ms/step - loss: 1.4611 - accuracy: 1.0000 - val_loss: 1.5360 - val_accuracy: 0.9259\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 641ms/step - loss: 2.4233 - accuracy: 0.1000 - val_loss: 2.2492 - val_accuracy: 0.2315\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 2.2218 - accuracy: 0.2200 - val_loss: 2.1438 - val_accuracy: 0.4537\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 1s 285ms/step - loss: 2.1054 - accuracy: 0.4800 - val_loss: 2.0595 - val_accuracy: 0.5093\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 1s 284ms/step - loss: 2.0113 - accuracy: 0.5700 - val_loss: 2.0001 - val_accuracy: 0.6667\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 1s 288ms/step - loss: 1.9487 - accuracy: 0.7600 - val_loss: 1.9569 - val_accuracy: 0.7315\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 1.9042 - accuracy: 0.8000 - val_loss: 1.9128 - val_accuracy: 0.6481\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 1.8616 - accuracy: 0.8000 - val_loss: 1.8793 - val_accuracy: 0.7315\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 1.8256 - accuracy: 0.8200 - val_loss: 1.8499 - val_accuracy: 0.6667\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 1.7926 - accuracy: 0.7600 - val_loss: 1.8219 - val_accuracy: 0.6667\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 1s 256ms/step - loss: 1.7560 - accuracy: 0.8200 - val_loss: 1.7967 - val_accuracy: 0.6852\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 340ms/step - loss: 1.9005 - accuracy: 0.8600 - val_loss: 1.9048 - val_accuracy: 0.7963\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 1.8443 - accuracy: 0.8500 - val_loss: 1.8669 - val_accuracy: 0.7963\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 1.8032 - accuracy: 0.8500 - val_loss: 1.8342 - val_accuracy: 0.7963\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 1s 300ms/step - loss: 1.7653 - accuracy: 0.8800 - val_loss: 1.8015 - val_accuracy: 0.8241\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 1.7323 - accuracy: 0.8800 - val_loss: 1.7705 - val_accuracy: 0.8056\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 1.7016 - accuracy: 0.9500 - val_loss: 1.7534 - val_accuracy: 0.8241\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 1s 302ms/step - loss: 1.6795 - accuracy: 0.9900 - val_loss: 1.7337 - val_accuracy: 0.8981\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 1.6534 - accuracy: 0.9900 - val_loss: 1.7112 - val_accuracy: 0.8981\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 184ms/step - loss: 1.6293 - accuracy: 0.9900 - val_loss: 1.6935 - val_accuracy: 0.8889\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 1s 274ms/step - loss: 1.6103 - accuracy: 0.9900 - val_loss: 1.6709 - val_accuracy: 0.9074\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 719ms/step - loss: 2.4946 - accuracy: 0.1300 - val_loss: 2.2711 - val_accuracy: 0.1759\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 341ms/step - loss: 2.2377 - accuracy: 0.3600 - val_loss: 2.1740 - val_accuracy: 0.5185\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 1s 296ms/step - loss: 2.1347 - accuracy: 0.5800 - val_loss: 2.1034 - val_accuracy: 0.7130\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 1s 282ms/step - loss: 2.0605 - accuracy: 0.7600 - val_loss: 2.0560 - val_accuracy: 0.7407\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 1s 283ms/step - loss: 2.0092 - accuracy: 0.8700 - val_loss: 2.0055 - val_accuracy: 0.7593\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 1.9586 - accuracy: 0.7800 - val_loss: 1.9758 - val_accuracy: 0.7407\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 1.9236 - accuracy: 0.8300 - val_loss: 1.9510 - val_accuracy: 0.7500\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 217ms/step - loss: 1.8940 - accuracy: 0.8500 - val_loss: 1.9176 - val_accuracy: 0.7593\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 1.8659 - accuracy: 0.8400 - val_loss: 1.8879 - val_accuracy: 0.7407\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 1s 257ms/step - loss: 1.8333 - accuracy: 0.8300 - val_loss: 1.8623 - val_accuracy: 0.7500\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 325ms/step - loss: 1.9558 - accuracy: 0.8400 - val_loss: 1.9651 - val_accuracy: 0.7963\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 288ms/step - loss: 1.9104 - accuracy: 0.8500 - val_loss: 1.9317 - val_accuracy: 0.8148\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 1.8730 - accuracy: 0.8500 - val_loss: 1.9030 - val_accuracy: 0.7870\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 1.8391 - accuracy: 0.8700 - val_loss: 1.8745 - val_accuracy: 0.8056\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 1.8132 - accuracy: 0.8900 - val_loss: 1.8461 - val_accuracy: 0.8241\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 1s 278ms/step - loss: 1.7802 - accuracy: 0.8900 - val_loss: 1.8208 - val_accuracy: 0.8426\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 1s 283ms/step - loss: 1.7590 - accuracy: 0.9100 - val_loss: 1.7987 - val_accuracy: 0.8796\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 1s 281ms/step - loss: 1.7355 - accuracy: 0.9500 - val_loss: 1.7793 - val_accuracy: 0.9352\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 1.7128 - accuracy: 0.9800 - val_loss: 1.7626 - val_accuracy: 0.9259\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 1.6924 - accuracy: 1.0000 - val_loss: 1.7443 - val_accuracy: 0.9167\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 656ms/step - loss: 2.4527 - accuracy: 0.1000 - val_loss: 2.2609 - val_accuracy: 0.1389\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 284ms/step - loss: 2.2026 - accuracy: 0.2100 - val_loss: 2.1219 - val_accuracy: 0.5278\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 1s 277ms/step - loss: 2.0653 - accuracy: 0.5800 - val_loss: 2.0014 - val_accuracy: 0.5741\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 1s 279ms/step - loss: 1.9540 - accuracy: 0.6200 - val_loss: 1.9357 - val_accuracy: 0.6111\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 1s 379ms/step - loss: 1.8750 - accuracy: 0.6700 - val_loss: 1.8645 - val_accuracy: 0.7130\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 1s 280ms/step - loss: 1.8092 - accuracy: 0.8300 - val_loss: 1.8173 - val_accuracy: 0.8611\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 1.7624 - accuracy: 0.8800 - val_loss: 1.7761 - val_accuracy: 0.8426\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 1.7138 - accuracy: 0.8900 - val_loss: 1.7371 - val_accuracy: 0.8148\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 1.6779 - accuracy: 0.8900 - val_loss: 1.7048 - val_accuracy: 0.8611\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 1s 288ms/step - loss: 1.6465 - accuracy: 0.9000 - val_loss: 1.6746 - val_accuracy: 0.8796\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 323ms/step - loss: 1.6112 - accuracy: 0.9300 - val_loss: 1.6577 - val_accuracy: 0.9537\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 1.5864 - accuracy: 0.9700 - val_loss: 1.6371 - val_accuracy: 0.9444\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 1.5553 - accuracy: 0.9800 - val_loss: 1.6031 - val_accuracy: 0.9259\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 1.5255 - accuracy: 1.0000 - val_loss: 1.5745 - val_accuracy: 0.9537\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 1.4967 - accuracy: 0.9800 - val_loss: 1.5498 - val_accuracy: 0.9537\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 1s 257ms/step - loss: 1.4735 - accuracy: 0.9700 - val_loss: 1.5265 - val_accuracy: 0.9537\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 702ms/step - loss: 2.6305 - accuracy: 0.0800 - val_loss: 2.2798 - val_accuracy: 0.2315\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 287ms/step - loss: 2.2469 - accuracy: 0.2800 - val_loss: 2.1828 - val_accuracy: 0.2778\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 1s 255ms/step - loss: 2.1522 - accuracy: 0.4000 - val_loss: 2.1290 - val_accuracy: 0.5185\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 1s 278ms/step - loss: 2.0957 - accuracy: 0.6000 - val_loss: 2.0866 - val_accuracy: 0.6296\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0529 - accuracy: 0.7000 - val_loss: 2.0602 - val_accuracy: 0.6204\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.0172 - accuracy: 0.7500 - val_loss: 2.0249 - val_accuracy: 0.6204\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 1.9783 - accuracy: 0.7800 - val_loss: 1.9872 - val_accuracy: 0.6111\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 1.9409 - accuracy: 0.7900 - val_loss: 1.9728 - val_accuracy: 0.6019\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 1s 292ms/step - loss: 1.9215 - accuracy: 0.8000 - val_loss: 1.9490 - val_accuracy: 0.6389\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 1s 277ms/step - loss: 1.8934 - accuracy: 0.8200 - val_loss: 1.9281 - val_accuracy: 0.7037\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 319ms/step - loss: 1.8722 - accuracy: 0.7700 - val_loss: 1.9140 - val_accuracy: 0.7222\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 283ms/step - loss: 1.8536 - accuracy: 0.7700 - val_loss: 1.8954 - val_accuracy: 0.7593\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 1s 273ms/step - loss: 1.8340 - accuracy: 0.8500 - val_loss: 1.8778 - val_accuracy: 0.8426\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 1s 319ms/step - loss: 1.8182 - accuracy: 0.9400 - val_loss: 1.8634 - val_accuracy: 0.8611\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 1s 286ms/step - loss: 1.8028 - accuracy: 0.9200 - val_loss: 1.8478 - val_accuracy: 0.8796\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 1.7851 - accuracy: 0.9300 - val_loss: 1.8312 - val_accuracy: 0.8796\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 1s 298ms/step - loss: 1.7660 - accuracy: 0.9500 - val_loss: 1.8167 - val_accuracy: 0.8981\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 1.7480 - accuracy: 0.9700 - val_loss: 1.7980 - val_accuracy: 0.8981\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 1s 277ms/step - loss: 1.7281 - accuracy: 0.9900 - val_loss: 1.7826 - val_accuracy: 0.9074\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 1.7134 - accuracy: 0.9900 - val_loss: 1.7686 - val_accuracy: 0.9074\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 750ms/step - loss: 2.5092 - accuracy: 0.1000 - val_loss: 2.2603 - val_accuracy: 0.1667\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.2390 - accuracy: 0.1900 - val_loss: 2.1917 - val_accuracy: 0.1574\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 1s 305ms/step - loss: 2.1483 - accuracy: 0.3000 - val_loss: 2.1141 - val_accuracy: 0.4444\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 1s 294ms/step - loss: 2.0769 - accuracy: 0.5500 - val_loss: 2.0688 - val_accuracy: 0.6944\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 1s 280ms/step - loss: 2.0248 - accuracy: 0.7600 - val_loss: 2.0217 - val_accuracy: 0.7870\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 1.9765 - accuracy: 0.7900 - val_loss: 1.9932 - val_accuracy: 0.7593\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 1s 270ms/step - loss: 1.9445 - accuracy: 0.8000 - val_loss: 1.9644 - val_accuracy: 0.7963\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 1s 286ms/step - loss: 1.9100 - accuracy: 0.8900 - val_loss: 1.9293 - val_accuracy: 0.8241\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 1.8753 - accuracy: 0.9100 - val_loss: 1.9063 - val_accuracy: 0.8148\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 1.8563 - accuracy: 0.8700 - val_loss: 1.8819 - val_accuracy: 0.8241\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 339ms/step - loss: 1.8236 - accuracy: 0.9100 - val_loss: 1.8583 - val_accuracy: 0.8519\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.8029 - accuracy: 0.9700 - val_loss: 1.8394 - val_accuracy: 0.8796\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 1.7799 - accuracy: 0.9800 - val_loss: 1.8178 - val_accuracy: 0.8796\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 1.7568 - accuracy: 1.0000 - val_loss: 1.7980 - val_accuracy: 0.8704\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 1.7325 - accuracy: 0.9900 - val_loss: 1.7810 - val_accuracy: 0.8796\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 1s 302ms/step - loss: 1.7105 - accuracy: 0.9900 - val_loss: 1.7659 - val_accuracy: 0.8889\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 1s 302ms/step - loss: 1.6909 - accuracy: 1.0000 - val_loss: 1.7467 - val_accuracy: 0.9352\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 1.6699 - accuracy: 1.0000 - val_loss: 1.7243 - val_accuracy: 0.9352\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 1s 302ms/step - loss: 1.6471 - accuracy: 1.0000 - val_loss: 1.7066 - val_accuracy: 0.9444\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 1.6262 - accuracy: 1.0000 - val_loss: 1.6910 - val_accuracy: 0.9074\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 741ms/step - loss: 2.5608 - accuracy: 0.0900 - val_loss: 2.2564 - val_accuracy: 0.2963\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 293ms/step - loss: 2.2103 - accuracy: 0.3300 - val_loss: 2.1121 - val_accuracy: 0.3611\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 1s 287ms/step - loss: 2.0694 - accuracy: 0.4800 - val_loss: 2.0097 - val_accuracy: 0.6481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "2/2 [==============================] - 1s 281ms/step - loss: 1.9728 - accuracy: 0.7300 - val_loss: 1.9501 - val_accuracy: 0.7315\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 1s 303ms/step - loss: 1.9150 - accuracy: 0.7600 - val_loss: 1.8959 - val_accuracy: 0.7870\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 1s 280ms/step - loss: 1.8523 - accuracy: 0.7900 - val_loss: 1.8441 - val_accuracy: 0.8333\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 1.7980 - accuracy: 0.8500 - val_loss: 1.8187 - val_accuracy: 0.8148\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 1.7753 - accuracy: 0.8600 - val_loss: 1.7828 - val_accuracy: 0.7963\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 1s 274ms/step - loss: 1.7357 - accuracy: 0.8500 - val_loss: 1.7537 - val_accuracy: 0.8611\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 1.7022 - accuracy: 0.9000 - val_loss: 1.7343 - val_accuracy: 0.8611\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 324ms/step - loss: 1.6754 - accuracy: 0.9100 - val_loss: 1.7096 - val_accuracy: 0.8333\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 1.6471 - accuracy: 0.9100 - val_loss: 1.6826 - val_accuracy: 0.8611\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 1.6186 - accuracy: 0.9400 - val_loss: 1.6597 - val_accuracy: 0.8611\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 1s 285ms/step - loss: 1.5948 - accuracy: 0.9500 - val_loss: 1.6383 - val_accuracy: 0.8796\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 1.5679 - accuracy: 0.9700 - val_loss: 1.6149 - val_accuracy: 0.8704\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 1s 287ms/step - loss: 1.5436 - accuracy: 0.9800 - val_loss: 1.5929 - val_accuracy: 0.9167\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 1s 263ms/step - loss: 1.5207 - accuracy: 0.9800 - val_loss: 1.5690 - val_accuracy: 0.9352\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 1.4954 - accuracy: 0.9900 - val_loss: 1.5508 - val_accuracy: 0.9352\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 1.4718 - accuracy: 0.9900 - val_loss: 1.5337 - val_accuracy: 0.9352\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 188ms/step - loss: 1.4491 - accuracy: 1.0000 - val_loss: 1.5127 - val_accuracy: 0.9352\n"
     ]
    }
   ],
   "source": [
    "high_acc = 0\n",
    "for run in range(0,10):\n",
    "    # feature extraction layers\n",
    "    resnet_model = ResNet50(input_shape=(224, 224,3), include_top=False, weights=\"imagenet\")\n",
    "    for layer in resnet_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    d = resnet_model.output.shape[-1] # dimension of last layer\n",
    "\n",
    "\n",
    "    ###################### model 2 ###################### \n",
    "    layer_2_0 = tf.keras.layers.Dense(d,name=\"weight_2\")(resnet_model.output) #times weight before flatten\n",
    "    layer_2_1 = tf.keras.layers.Flatten(name='flatten_2')(layer_2_0)\n",
    "\n",
    "    Dense_2_1 = tf.keras.layers.Dense(shape_2_1, activation=actv_fun_2_1,name='fc2_1')\n",
    "    layer_2_2  = Dense_2_1(layer_2_1)\n",
    "    Dense_2_2 = tf.keras.layers.Dense(shape_2_2, activation=actv_fun_2_2,name='fc2_2')\n",
    "    layer_2_3  = Dense_2_2(layer_2_2)\n",
    "\n",
    "    Dense_2_3 = tf.keras.layers.Dense(10, activation='softmax',name='command_output')\n",
    "    out_layer_2 = Dense_2_3(layer_2_3)\n",
    "\n",
    "\n",
    "    resnet_model = tf.keras.Model(resnet_model.input, out_layer_2)\n",
    "\n",
    "\n",
    "    # resnet_model.summary() \n",
    "\n",
    "\n",
    "    ##################### training ############################\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    callbacks = [EarlyStopping(monitor = 'val_accuracy', patience=5,restore_best_weights=True)]\n",
    "    resnet_model.compile(optimizer=opt, loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "       \n",
    "    history1 = resnet_model.fit(Train_Inputs,Train_command_class,\n",
    "                                validation_data=(Val_Inputs,Val_command_class),\n",
    "                                callbacks=callbacks,\n",
    "                                batch_size=64,\n",
    "                                epochs=10)\n",
    "    \n",
    "    for layer in resnet_model.layers[0:175]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    history2 = resnet_model.fit(Train_Inputs,Train_command_class,\n",
    "                            validation_data=(Val_Inputs,Val_command_class), \n",
    "                           callbacks=callbacks,\n",
    "                           batch_size=64,\n",
    "                           epochs=10)\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    run_time = stop - start\n",
    "    \n",
    "    ##################### test performance ############################\n",
    "    predictions = resnet_model.predict(Test_Inputs)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class\n",
    "    acc_p15_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)\n",
    "\n",
    "\n",
    "    # test on p1\n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_1)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_1, axis=-1)\n",
    "    acc_p1_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p2\n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_2)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_2, axis=-1)\n",
    "    acc_p2_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p3 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_3)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_3, axis=-1)\n",
    "    acc_p3_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p4\n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_4)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_4, axis=-1)\n",
    "    acc_p4_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p5\n",
    "\n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_5)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_5, axis=-1)\n",
    "    acc_p5_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p6 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_6)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class_6\n",
    "    acc_p6 = metrics.accuracy_score(true_classes, predicted_classes).round(4)                \n",
    "\n",
    "    # test on p7 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_7)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class_7\n",
    "    acc_p7 = metrics.accuracy_score(true_classes, predicted_classes).round(4)    \n",
    "\n",
    "    # test on p8 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_8)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class_8\n",
    "    acc_p8 = metrics.accuracy_score(true_classes, predicted_classes).round(4)    \n",
    "\n",
    "    Perfomance = Perfomance.append({'Model': \"Subjet_2\",'Size':'10','Time':run_time,'Command_Acc_p15':acc_p15_c,\n",
    "                                    'Command_Acc_p1':acc_p1_c,'Command_Acc_p2':acc_p2_c,\n",
    "                                    'Command_Acc_p3':acc_p3_c,'Command_Acc_p4':acc_p4_c,'Command_Acc_p5':acc_p5_c,\n",
    "                                    'Acc_p6':acc_p6,'Acc_p7':acc_p7,'Acc_p8':acc_p8}, ignore_index=True)\n",
    "\n",
    "\n",
    "    if high_acc < acc_p15_c :\n",
    "        resnet_model.save('Initial_subject_2_model_size10_0608.h5')\n",
    "        high_acc = acc_p15_c \n",
    "        best_index = run\n",
    "        pd.DataFrame.from_dict(history1.history).to_csv('subject_2_size10_history1_0608.csv',index=False)\n",
    "        pd.DataFrame.from_dict(history2.history).to_csv('subject_2_size10_history2_0608.csv',index=False)\n",
    "        \n",
    "    del resnet_model\n",
    "    run = run + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "0.8611\n"
     ]
    }
   ],
   "source": [
    "resnet_model = tf.keras.models.load_model('Initial_subject_2_model_size10_0608.h5')\n",
    "predictions = resnet_model.predict(Test_Inputs_2)\n",
    "predicted_classes =  np.argmax(predictions, axis=1)\n",
    "acc_c = round(sum(x == y for x, y in zip(np.argmax(Test_command_class_2, axis=1), predicted_classes)) / len(np.argmax(Test_command_class_2, axis=1)),4)\n",
    "print(acc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Time</th>\n",
       "      <th>Command_Acc_p15</th>\n",
       "      <th>Command_Acc_p1</th>\n",
       "      <th>Command_Acc_p2</th>\n",
       "      <th>Command_Acc_p3</th>\n",
       "      <th>Command_Acc_p4</th>\n",
       "      <th>Command_Acc_p5</th>\n",
       "      <th>Acc_p6</th>\n",
       "      <th>Acc_p7</th>\n",
       "      <th>Acc_p8</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.269878</td>\n",
       "      <td>0.6243</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>12.062418</td>\n",
       "      <td>0.5709</td>\n",
       "      <td>0.9189</td>\n",
       "      <td>0.3148</td>\n",
       "      <td>0.4696</td>\n",
       "      <td>0.5413</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.520935</td>\n",
       "      <td>0.6335</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.6514</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.664869</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.2685</td>\n",
       "      <td>0.5217</td>\n",
       "      <td>0.6789</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.2970</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.347304</td>\n",
       "      <td>0.6188</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.2574</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.131006</td>\n",
       "      <td>0.5672</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>0.4696</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.651466</td>\n",
       "      <td>0.5930</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.2685</td>\n",
       "      <td>0.5130</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>12.100064</td>\n",
       "      <td>0.5727</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>0.4957</td>\n",
       "      <td>0.5413</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2574</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.176442</td>\n",
       "      <td>0.6151</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.3611</td>\n",
       "      <td>0.4957</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.929373</td>\n",
       "      <td>0.6446</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.4722</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.902663</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.6514</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.2178</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.805319</td>\n",
       "      <td>0.6538</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3519</td>\n",
       "      <td>0.5913</td>\n",
       "      <td>0.6697</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.685424</td>\n",
       "      <td>0.6298</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.194757</td>\n",
       "      <td>0.6390</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3981</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.364569</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>0.5739</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.957827</td>\n",
       "      <td>0.6317</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.3981</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.528849</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.4352</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.993745</td>\n",
       "      <td>0.6538</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.3611</td>\n",
       "      <td>0.5913</td>\n",
       "      <td>0.6697</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.823134</td>\n",
       "      <td>0.6446</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.6606</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.812350</td>\n",
       "      <td>0.6354</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>0.5304</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.680686</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>0.5913</td>\n",
       "      <td>0.6697</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.529958</td>\n",
       "      <td>0.6851</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.4259</td>\n",
       "      <td>0.5826</td>\n",
       "      <td>0.7248</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2178</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>17.791133</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4352</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.228115</td>\n",
       "      <td>0.6483</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.6697</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.815590</td>\n",
       "      <td>0.6740</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.099657</td>\n",
       "      <td>0.6980</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4630</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.7523</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.533707</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.3981</td>\n",
       "      <td>0.5826</td>\n",
       "      <td>0.6514</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.714685</td>\n",
       "      <td>0.6501</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.6606</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.797302</td>\n",
       "      <td>0.6556</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4259</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.6514</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>19.045552</td>\n",
       "      <td>0.6924</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5463</td>\n",
       "      <td>0.5739</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>15.484598</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.2342</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.3130</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.2178</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.018696</td>\n",
       "      <td>0.4144</td>\n",
       "      <td>0.2432</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>0.3578</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>11.138138</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>0.3043</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.3465</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.840607</td>\n",
       "      <td>0.4033</td>\n",
       "      <td>0.2162</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>12.762539</td>\n",
       "      <td>0.3996</td>\n",
       "      <td>0.2072</td>\n",
       "      <td>0.8426</td>\n",
       "      <td>0.3565</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.125518</td>\n",
       "      <td>0.4144</td>\n",
       "      <td>0.2162</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>10.908749</td>\n",
       "      <td>0.3812</td>\n",
       "      <td>0.1802</td>\n",
       "      <td>0.8519</td>\n",
       "      <td>0.2696</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>13.933343</td>\n",
       "      <td>0.3941</td>\n",
       "      <td>0.2523</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.3578</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.2178</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>13.496468</td>\n",
       "      <td>0.4217</td>\n",
       "      <td>0.2523</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.3304</td>\n",
       "      <td>0.3670</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.3366</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>13.351789</td>\n",
       "      <td>0.3886</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model       Time  Command_Acc_p15  Command_Acc_p1  Command_Acc_p2  \\\n",
       "0   Subjet_1  15.269878           0.6243          0.9369          0.3889   \n",
       "1   Subjet_1  12.062418           0.5709          0.9189          0.3148   \n",
       "2   Subjet_1  13.520935           0.6335          0.9550          0.3889   \n",
       "3   Subjet_1  14.664869           0.6225          0.9550          0.2685   \n",
       "4   Subjet_1  13.347304           0.6188          0.9279          0.3796   \n",
       "5   Subjet_1  14.131006           0.5672          0.9279          0.2870   \n",
       "6   Subjet_1  13.651466           0.5930          0.9459          0.2685   \n",
       "7   Subjet_1  12.100064           0.5727          0.9279          0.2870   \n",
       "8   Subjet_1  13.176442           0.6151          0.9459          0.3611   \n",
       "9   Subjet_1  13.929373           0.6446          0.9369          0.4722   \n",
       "10  Subjet_1  15.902663           0.6409          0.9820          0.3704   \n",
       "11  Subjet_1  13.805319           0.6538          1.0000          0.3519   \n",
       "12  Subjet_1  15.685424           0.6298          0.9820          0.3333   \n",
       "13  Subjet_1  16.194757           0.6390          0.9820          0.3981   \n",
       "14  Subjet_1  15.364569           0.6611          0.9820          0.3796   \n",
       "15  Subjet_1  14.957827           0.6317          0.9730          0.3981   \n",
       "16  Subjet_1  15.528849           0.6409          0.9910          0.4352   \n",
       "17  Subjet_1  15.993745           0.6538          0.9910          0.3611   \n",
       "18  Subjet_1  16.823134           0.6446          0.9730          0.3704   \n",
       "19  Subjet_1  15.812350           0.6354          0.9820          0.3889   \n",
       "20  Subjet_1  16.680686           0.6667          1.0000          0.3889   \n",
       "21  Subjet_1  14.529958           0.6851          0.9910          0.4259   \n",
       "22  Subjet_1  17.791133           0.6796          1.0000          0.4352   \n",
       "23  Subjet_1  16.228115           0.6483          1.0000          0.3704   \n",
       "24  Subjet_1  14.815590           0.6740          1.0000          0.4167   \n",
       "25  Subjet_1  16.099657           0.6980          1.0000          0.4630   \n",
       "26  Subjet_1  14.533707           0.6667          0.9910          0.3981   \n",
       "27  Subjet_1  15.714685           0.6501          1.0000          0.4167   \n",
       "28  Subjet_1  15.797302           0.6556          1.0000          0.4259   \n",
       "29  Subjet_1  19.045552           0.6924          1.0000          0.5463   \n",
       "30  Subjet_2  15.484598           0.3959          0.2342          0.8611   \n",
       "31  Subjet_2  14.018696           0.4144          0.2432          0.8611   \n",
       "32  Subjet_2  11.138138           0.3738          0.1982          0.7778   \n",
       "33  Subjet_2  14.840607           0.4033          0.2162          0.8611   \n",
       "34  Subjet_2  12.762539           0.3996          0.2072          0.8426   \n",
       "35  Subjet_2  14.125518           0.4144          0.2162          0.8611   \n",
       "36  Subjet_2  10.908749           0.3812          0.1802          0.8519   \n",
       "37  Subjet_2  13.933343           0.3941          0.2523          0.8333   \n",
       "38  Subjet_2  13.496468           0.4217          0.2523          0.8611   \n",
       "39  Subjet_2  13.351789           0.3886          0.1982          0.8704   \n",
       "\n",
       "    Command_Acc_p3  Command_Acc_p4  Command_Acc_p5  Acc_p6  Acc_p7  Acc_p8  \\\n",
       "0           0.5391          0.6239            0.63    0.38   0.384  0.2673   \n",
       "1           0.4696          0.5413            0.61    0.33   0.320  0.2673   \n",
       "2           0.5478          0.6514            0.62    0.48   0.384  0.2475   \n",
       "3           0.5217          0.6789            0.69    0.26   0.456  0.2970   \n",
       "4           0.5652          0.5780            0.64    0.39   0.416  0.2574   \n",
       "5           0.4696          0.5780            0.57    0.25   0.328  0.2673   \n",
       "6           0.5130          0.6239            0.61    0.43   0.440  0.2673   \n",
       "7           0.4957          0.5413            0.61    0.41   0.384  0.2574   \n",
       "8           0.4957          0.6147            0.66    0.41   0.408  0.2871   \n",
       "9           0.5391          0.6147            0.66    0.48   0.408  0.2673   \n",
       "10          0.5565          0.6514            0.64    0.34   0.400  0.2178   \n",
       "11          0.5913          0.6697            0.65    0.45   0.360  0.2277   \n",
       "12          0.5391          0.6422            0.65    0.43   0.408  0.2277   \n",
       "13          0.6000          0.6147            0.59    0.36   0.384  0.2475   \n",
       "14          0.5739          0.6972            0.67    0.44   0.400  0.2673   \n",
       "15          0.5652          0.6239            0.59    0.38   0.440  0.2475   \n",
       "16          0.5565          0.6239            0.59    0.37   0.360  0.2079   \n",
       "17          0.5913          0.6697            0.65    0.41   0.360  0.2277   \n",
       "18          0.5478          0.6606            0.67    0.37   0.384  0.2475   \n",
       "19          0.5304          0.6422            0.63    0.33   0.408  0.2772   \n",
       "20          0.5913          0.6697            0.68    0.43   0.352  0.2079   \n",
       "21          0.5826          0.7248            0.70    0.42   0.360  0.2178   \n",
       "22          0.5652          0.7156            0.68    0.36   0.328  0.2277   \n",
       "23          0.5565          0.6697            0.64    0.41   0.344  0.2079   \n",
       "24          0.5565          0.7339            0.66    0.45   0.344  0.2079   \n",
       "25          0.6000          0.7523            0.67    0.37   0.352  0.2079   \n",
       "26          0.5826          0.6514            0.71    0.36   0.360  0.2277   \n",
       "27          0.5391          0.6606            0.63    0.38   0.360  0.2277   \n",
       "28          0.5565          0.6514            0.64    0.39   0.352  0.2079   \n",
       "29          0.5739          0.7156            0.62    0.41   0.376  0.2277   \n",
       "30          0.3130          0.3303            0.24    0.21   0.344  0.2178   \n",
       "31          0.3478          0.3578            0.26    0.14   0.320  0.2871   \n",
       "32          0.3043          0.3028            0.29    0.16   0.272  0.3465   \n",
       "33          0.3478          0.3303            0.26    0.21   0.312  0.3168   \n",
       "34          0.3565          0.3028            0.29    0.16   0.312  0.3168   \n",
       "35          0.3391          0.3486            0.31    0.22   0.352  0.3168   \n",
       "36          0.2696          0.3303            0.28    0.14   0.256  0.2673   \n",
       "37          0.2957          0.3578            0.23    0.27   0.288  0.2178   \n",
       "38          0.3304          0.3670            0.30    0.18   0.280  0.3366   \n",
       "39          0.2870          0.3303            0.26    0.13   0.272  0.2673   \n",
       "\n",
       "   Size  \n",
       "0    10  \n",
       "1    10  \n",
       "2    10  \n",
       "3    10  \n",
       "4    10  \n",
       "5    10  \n",
       "6    10  \n",
       "7    10  \n",
       "8    10  \n",
       "9    10  \n",
       "10   20  \n",
       "11   20  \n",
       "12   20  \n",
       "13   20  \n",
       "14   20  \n",
       "15   20  \n",
       "16   20  \n",
       "17   20  \n",
       "18   20  \n",
       "19   20  \n",
       "20   30  \n",
       "21   30  \n",
       "22   30  \n",
       "23   30  \n",
       "24   30  \n",
       "25   30  \n",
       "26   30  \n",
       "27   30  \n",
       "28   30  \n",
       "29   30  \n",
       "30   10  \n",
       "31   10  \n",
       "32   10  \n",
       "33   10  \n",
       "34   10  \n",
       "35   10  \n",
       "36   10  \n",
       "37   10  \n",
       "38   10  \n",
       "39   10  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single model （size:20）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 2\n",
    "train_image = 20 #10:20%, 20: 40%, 30:60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20., 20., 20., 20., 20., 20., 20., 20., 20., 20.], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_indexs_train = (image_no<train_image)&(ALL_participant_class==subject)\n",
    "Train_Inputs = All_Inputs[select_indexs_train]\n",
    "Train_command_class = All_command_class[select_indexs_train]\n",
    "sum(Train_command_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 2s 379ms/step - loss: 2.4831 - accuracy: 0.0800 - val_loss: 2.2018 - val_accuracy: 0.3796\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 182ms/step - loss: 2.1669 - accuracy: 0.3700 - val_loss: 2.0540 - val_accuracy: 0.5741\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 1s 169ms/step - loss: 2.0325 - accuracy: 0.5900 - val_loss: 1.9833 - val_accuracy: 0.6667\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 1.9646 - accuracy: 0.6300 - val_loss: 1.9286 - val_accuracy: 0.6944\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 1s 169ms/step - loss: 1.9172 - accuracy: 0.7200 - val_loss: 1.8805 - val_accuracy: 0.8148\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.8717 - accuracy: 0.8000 - val_loss: 1.8546 - val_accuracy: 0.7685\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 1.8409 - accuracy: 0.8050 - val_loss: 1.8243 - val_accuracy: 0.8333\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.8120 - accuracy: 0.7850 - val_loss: 1.7958 - val_accuracy: 0.7870\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 228ms/step - loss: 1.7794 - accuracy: 0.8300 - val_loss: 1.7647 - val_accuracy: 0.8611\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 1.7433 - accuracy: 0.8500 - val_loss: 1.7328 - val_accuracy: 0.8056\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 190ms/step - loss: 1.7080 - accuracy: 0.8650 - val_loss: 1.6969 - val_accuracy: 0.8889\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 1.6689 - accuracy: 0.9600 - val_loss: 1.6642 - val_accuracy: 0.9722\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 1.6380 - accuracy: 0.9650 - val_loss: 1.6359 - val_accuracy: 0.9537\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 1s 130ms/step - loss: 1.6104 - accuracy: 0.9750 - val_loss: 1.6106 - val_accuracy: 0.9537\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 1.5797 - accuracy: 0.9800 - val_loss: 1.5776 - val_accuracy: 0.9352\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 79ms/step - loss: 1.5531 - accuracy: 0.9700 - val_loss: 1.5524 - val_accuracy: 0.9074\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 1s 160ms/step - loss: 1.5216 - accuracy: 0.9750 - val_loss: 1.5341 - val_accuracy: 0.9352\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 352ms/step - loss: 2.4586 - accuracy: 0.0550 - val_loss: 2.1646 - val_accuracy: 0.3333\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 2.1111 - accuracy: 0.4800 - val_loss: 2.0399 - val_accuracy: 0.5648\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 2.0101 - accuracy: 0.6300 - val_loss: 1.9649 - val_accuracy: 0.6667\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 1s 184ms/step - loss: 1.9521 - accuracy: 0.6700 - val_loss: 1.9199 - val_accuracy: 0.7593\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 82ms/step - loss: 1.9019 - accuracy: 0.7000 - val_loss: 1.8852 - val_accuracy: 0.6574\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 1.8670 - accuracy: 0.6800 - val_loss: 1.8415 - val_accuracy: 0.6667\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 80ms/step - loss: 1.8263 - accuracy: 0.6600 - val_loss: 1.8106 - val_accuracy: 0.7130\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 1s 160ms/step - loss: 1.7912 - accuracy: 0.7500 - val_loss: 1.7769 - val_accuracy: 0.8056\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 1.7561 - accuracy: 0.8500 - val_loss: 1.7428 - val_accuracy: 0.8519\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 1.7242 - accuracy: 0.8350 - val_loss: 1.7115 - val_accuracy: 0.8241\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 190ms/step - loss: 1.6873 - accuracy: 0.8800 - val_loss: 1.6759 - val_accuracy: 0.8796\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 1.6531 - accuracy: 0.9400 - val_loss: 1.6472 - val_accuracy: 0.8889\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 1.6255 - accuracy: 0.9450 - val_loss: 1.6175 - val_accuracy: 0.8704\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 1.5931 - accuracy: 0.8900 - val_loss: 1.5860 - val_accuracy: 0.8426\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 118ms/step - loss: 1.5631 - accuracy: 0.8800 - val_loss: 1.5571 - val_accuracy: 0.8704\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 1.5318 - accuracy: 0.9200 - val_loss: 1.5270 - val_accuracy: 0.9352\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 1.5011 - accuracy: 0.9450 - val_loss: 1.5048 - val_accuracy: 0.9630\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.4700 - accuracy: 0.9750 - val_loss: 1.4769 - val_accuracy: 0.9259\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 1.4497 - accuracy: 0.9300 - val_loss: 1.4575 - val_accuracy: 0.8796\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 1.4239 - accuracy: 0.9050 - val_loss: 1.4308 - val_accuracy: 0.8796\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 372ms/step - loss: 2.3660 - accuracy: 0.1100 - val_loss: 2.1277 - val_accuracy: 0.3148\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 208ms/step - loss: 2.0784 - accuracy: 0.4450 - val_loss: 1.9938 - val_accuracy: 0.6111\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.9557 - accuracy: 0.6500 - val_loss: 1.9145 - val_accuracy: 0.5741\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 1.8862 - accuracy: 0.5700 - val_loss: 1.8578 - val_accuracy: 0.5833\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 1.8207 - accuracy: 0.7000 - val_loss: 1.7933 - val_accuracy: 0.8148\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 81ms/step - loss: 1.7672 - accuracy: 0.9200 - val_loss: 1.7550 - val_accuracy: 0.8056\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 1s 193ms/step - loss: 1.7220 - accuracy: 0.8350 - val_loss: 1.7126 - val_accuracy: 0.8704\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 1s 184ms/step - loss: 1.6833 - accuracy: 0.8850 - val_loss: 1.6729 - val_accuracy: 0.8796\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 167ms/step - loss: 1.6444 - accuracy: 0.9000 - val_loss: 1.6486 - val_accuracy: 0.9352\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 1.6108 - accuracy: 0.9450 - val_loss: 1.6073 - val_accuracy: 0.9167\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 191ms/step - loss: 1.5648 - accuracy: 0.9400 - val_loss: 1.5715 - val_accuracy: 0.9167\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 1.5272 - accuracy: 0.9450 - val_loss: 1.5306 - val_accuracy: 0.9074\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 1.4936 - accuracy: 0.9400 - val_loss: 1.5018 - val_accuracy: 0.8981\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 1.4561 - accuracy: 0.9450 - val_loss: 1.4627 - val_accuracy: 0.9352\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 1.4172 - accuracy: 0.9650 - val_loss: 1.4313 - val_accuracy: 0.9630\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 1.3832 - accuracy: 0.9750 - val_loss: 1.3932 - val_accuracy: 0.9444\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 1.3502 - accuracy: 0.9850 - val_loss: 1.3676 - val_accuracy: 0.9444\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 1s 165ms/step - loss: 1.3184 - accuracy: 0.9850 - val_loss: 1.3318 - val_accuracy: 0.9722\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.2829 - accuracy: 0.9850 - val_loss: 1.3026 - val_accuracy: 0.9722\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 82ms/step - loss: 1.2484 - accuracy: 0.9900 - val_loss: 1.2690 - val_accuracy: 0.9722\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 368ms/step - loss: 2.4577 - accuracy: 0.1100 - val_loss: 2.2211 - val_accuracy: 0.2685\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 2.1995 - accuracy: 0.3300 - val_loss: 2.1402 - val_accuracy: 0.5093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 2.1192 - accuracy: 0.5850 - val_loss: 2.0756 - val_accuracy: 0.6944\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 1s 189ms/step - loss: 2.0657 - accuracy: 0.6550 - val_loss: 2.0356 - val_accuracy: 0.7685\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 1s 169ms/step - loss: 2.0225 - accuracy: 0.7700 - val_loss: 2.0034 - val_accuracy: 0.8889\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 1.9956 - accuracy: 0.8150 - val_loss: 1.9878 - val_accuracy: 0.6204\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 1.9740 - accuracy: 0.6150 - val_loss: 1.9657 - val_accuracy: 0.7315\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.9494 - accuracy: 0.8000 - val_loss: 1.9393 - val_accuracy: 0.8796\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 1.9296 - accuracy: 0.8550 - val_loss: 1.9186 - val_accuracy: 0.8889\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 1s 197ms/step - loss: 1.9014 - accuracy: 0.8700 - val_loss: 1.8910 - val_accuracy: 0.8611\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 204ms/step - loss: 1.9866 - accuracy: 0.8750 - val_loss: 1.9775 - val_accuracy: 0.8333\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 1.9597 - accuracy: 0.8500 - val_loss: 1.9470 - val_accuracy: 0.8889\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.9310 - accuracy: 0.8650 - val_loss: 1.9207 - val_accuracy: 0.8333\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.9019 - accuracy: 0.8550 - val_loss: 1.8992 - val_accuracy: 0.8611\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 116ms/step - loss: 1.8816 - accuracy: 0.8900 - val_loss: 1.8721 - val_accuracy: 0.8889\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 1.8536 - accuracy: 0.9100 - val_loss: 1.8498 - val_accuracy: 0.8981\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 1.8315 - accuracy: 0.9150 - val_loss: 1.8270 - val_accuracy: 0.9259\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 1.8089 - accuracy: 0.9450 - val_loss: 1.8071 - val_accuracy: 0.9444\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 1.7843 - accuracy: 0.9400 - val_loss: 1.7850 - val_accuracy: 0.9444\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 1.7612 - accuracy: 0.9650 - val_loss: 1.7591 - val_accuracy: 0.9352\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 362ms/step - loss: 2.3828 - accuracy: 0.0700 - val_loss: 2.1349 - val_accuracy: 0.3519\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 2.0841 - accuracy: 0.3250 - val_loss: 1.9792 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 1s 167ms/step - loss: 1.9532 - accuracy: 0.6000 - val_loss: 1.9062 - val_accuracy: 0.7130\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 1s 187ms/step - loss: 1.8781 - accuracy: 0.6950 - val_loss: 1.8453 - val_accuracy: 0.7870\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 1.8257 - accuracy: 0.8200 - val_loss: 1.8148 - val_accuracy: 0.7407\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 1.7829 - accuracy: 0.7550 - val_loss: 1.7687 - val_accuracy: 0.7963\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 1.7416 - accuracy: 0.8150 - val_loss: 1.7128 - val_accuracy: 0.8148\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 1.6903 - accuracy: 0.8050 - val_loss: 1.6730 - val_accuracy: 0.7870\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 132ms/step - loss: 1.6416 - accuracy: 0.8100 - val_loss: 1.6366 - val_accuracy: 0.8148\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 1.6089 - accuracy: 0.8650 - val_loss: 1.6014 - val_accuracy: 0.8333\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 196ms/step - loss: 1.5693 - accuracy: 0.9300 - val_loss: 1.5623 - val_accuracy: 0.9352\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 74ms/step - loss: 1.5311 - accuracy: 0.9700 - val_loss: 1.5319 - val_accuracy: 0.9352\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 1.4988 - accuracy: 0.9150 - val_loss: 1.5030 - val_accuracy: 0.9167\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 1.4645 - accuracy: 0.9450 - val_loss: 1.4725 - val_accuracy: 0.9537\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 1.4319 - accuracy: 0.9850 - val_loss: 1.4393 - val_accuracy: 0.9444\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 80ms/step - loss: 1.3981 - accuracy: 0.9850 - val_loss: 1.4108 - val_accuracy: 0.9352\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.3647 - accuracy: 0.9900 - val_loss: 1.3833 - val_accuracy: 0.9444\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 81ms/step - loss: 1.3349 - accuracy: 1.0000 - val_loss: 1.3489 - val_accuracy: 0.9444\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 156ms/step - loss: 1.2968 - accuracy: 1.0000 - val_loss: 1.3104 - val_accuracy: 0.9444\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 2s 378ms/step - loss: 2.4062 - accuracy: 0.1500 - val_loss: 2.1183 - val_accuracy: 0.4722\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 181ms/step - loss: 2.0604 - accuracy: 0.4850 - val_loss: 1.9667 - val_accuracy: 0.5926\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 1s 187ms/step - loss: 1.9253 - accuracy: 0.6100 - val_loss: 1.8672 - val_accuracy: 0.6667\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 1s 185ms/step - loss: 1.8411 - accuracy: 0.6550 - val_loss: 1.8041 - val_accuracy: 0.8333\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.7769 - accuracy: 0.7700 - val_loss: 1.7437 - val_accuracy: 0.7037\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 1.7240 - accuracy: 0.7050 - val_loss: 1.7025 - val_accuracy: 0.8889\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 1.6830 - accuracy: 0.8900 - val_loss: 1.6614 - val_accuracy: 0.8889\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 111ms/step - loss: 1.6321 - accuracy: 0.8850 - val_loss: 1.6115 - val_accuracy: 0.8519\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 1.5922 - accuracy: 0.8800 - val_loss: 1.5823 - val_accuracy: 0.9167\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 1s 180ms/step - loss: 1.5552 - accuracy: 0.9350 - val_loss: 1.5428 - val_accuracy: 0.9259\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 186ms/step - loss: 1.5138 - accuracy: 0.9050 - val_loss: 1.5040 - val_accuracy: 0.9074\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 181ms/step - loss: 1.4722 - accuracy: 0.9450 - val_loss: 1.4736 - val_accuracy: 0.9167\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 80ms/step - loss: 1.4348 - accuracy: 0.9600 - val_loss: 1.4380 - val_accuracy: 0.8889\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 1.3983 - accuracy: 0.9500 - val_loss: 1.4048 - val_accuracy: 0.8981\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 79ms/step - loss: 1.3645 - accuracy: 0.9300 - val_loss: 1.3779 - val_accuracy: 0.8519\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 1.3263 - accuracy: 0.9200 - val_loss: 1.3380 - val_accuracy: 0.8519\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 1s 152ms/step - loss: 1.2901 - accuracy: 0.9350 - val_loss: 1.3001 - val_accuracy: 0.8611\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 2s 382ms/step - loss: 2.3476 - accuracy: 0.1350 - val_loss: 2.0589 - val_accuracy: 0.3889\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 2.0003 - accuracy: 0.5250 - val_loss: 1.8974 - val_accuracy: 0.5185\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 1s 191ms/step - loss: 1.8498 - accuracy: 0.5700 - val_loss: 1.7762 - val_accuracy: 0.8056\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 1s 231ms/step - loss: 1.7403 - accuracy: 0.8900 - val_loss: 1.7160 - val_accuracy: 0.8796\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 84ms/step - loss: 1.6756 - accuracy: 0.8100 - val_loss: 1.6498 - val_accuracy: 0.7778\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 1.6155 - accuracy: 0.7800 - val_loss: 1.6045 - val_accuracy: 0.8241\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 79ms/step - loss: 1.5619 - accuracy: 0.8500 - val_loss: 1.5502 - val_accuracy: 0.8704\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 1.5084 - accuracy: 0.8550 - val_loss: 1.4935 - val_accuracy: 0.8981\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 181ms/step - loss: 1.4556 - accuracy: 0.8900 - val_loss: 1.4486 - val_accuracy: 0.9352\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 79ms/step - loss: 1.4081 - accuracy: 0.9200 - val_loss: 1.4038 - val_accuracy: 0.9259\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 1.3575 - accuracy: 0.9550 - val_loss: 1.3536 - val_accuracy: 0.9537\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.3132 - accuracy: 0.9400 - val_loss: 1.3065 - val_accuracy: 0.9444\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 1.2675 - accuracy: 0.9400 - val_loss: 1.2681 - val_accuracy: 0.9537\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 1.2217 - accuracy: 0.9600 - val_loss: 1.2227 - val_accuracy: 0.9537\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 1.1764 - accuracy: 0.9650 - val_loss: 1.1800 - val_accuracy: 0.9537\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 1.1318 - accuracy: 0.9650 - val_loss: 1.1460 - val_accuracy: 0.9630\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 74ms/step - loss: 1.0887 - accuracy: 0.9750 - val_loss: 1.1010 - val_accuracy: 0.9630\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.0452 - accuracy: 0.9750 - val_loss: 1.0597 - val_accuracy: 0.9630\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 1.0034 - accuracy: 0.9800 - val_loss: 1.0319 - val_accuracy: 0.9722\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 0.9624 - accuracy: 0.9900 - val_loss: 0.9902 - val_accuracy: 0.9722\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 367ms/step - loss: 2.4476 - accuracy: 0.0600 - val_loss: 2.2041 - val_accuracy: 0.4444\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 2.1680 - accuracy: 0.4350 - val_loss: 2.0743 - val_accuracy: 0.4722\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 2.0457 - accuracy: 0.5150 - val_loss: 1.9897 - val_accuracy: 0.6389\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 1.9716 - accuracy: 0.6650 - val_loss: 1.9283 - val_accuracy: 0.6852\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 1.9166 - accuracy: 0.6600 - val_loss: 1.8821 - val_accuracy: 0.6574\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 1.8646 - accuracy: 0.7000 - val_loss: 1.8490 - val_accuracy: 0.6759\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 1s 190ms/step - loss: 1.8271 - accuracy: 0.6900 - val_loss: 1.8086 - val_accuracy: 0.7407\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 1s 186ms/step - loss: 1.7926 - accuracy: 0.7850 - val_loss: 1.7756 - val_accuracy: 0.7963\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 1.7558 - accuracy: 0.8600 - val_loss: 1.7379 - val_accuracy: 0.8704\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 1.7193 - accuracy: 0.8650 - val_loss: 1.7109 - val_accuracy: 0.8611\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 239ms/step - loss: 1.6843 - accuracy: 0.8800 - val_loss: 1.6794 - val_accuracy: 0.8519\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 161ms/step - loss: 1.6566 - accuracy: 0.8550 - val_loss: 1.6589 - val_accuracy: 0.8611\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 1.6309 - accuracy: 0.8500 - val_loss: 1.6301 - val_accuracy: 0.8704\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 1.5979 - accuracy: 0.8800 - val_loss: 1.6039 - val_accuracy: 0.8611\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 1.5715 - accuracy: 0.8750 - val_loss: 1.5635 - val_accuracy: 0.8796\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 1s 213ms/step - loss: 1.5340 - accuracy: 0.8800 - val_loss: 1.5450 - val_accuracy: 0.8889\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 1.5066 - accuracy: 0.8950 - val_loss: 1.5074 - val_accuracy: 0.8796\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 1.4717 - accuracy: 0.8950 - val_loss: 1.4741 - val_accuracy: 0.8704\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 1.4406 - accuracy: 0.9000 - val_loss: 1.4451 - val_accuracy: 0.9074\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 1s 166ms/step - loss: 1.4063 - accuracy: 0.9500 - val_loss: 1.4209 - val_accuracy: 0.9630\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 2s 384ms/step - loss: 2.3127 - accuracy: 0.1450 - val_loss: 2.0142 - val_accuracy: 0.4537\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 1.9543 - accuracy: 0.5100 - val_loss: 1.8573 - val_accuracy: 0.6296\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 1s 167ms/step - loss: 1.8047 - accuracy: 0.6600 - val_loss: 1.7540 - val_accuracy: 0.7870\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 1.7257 - accuracy: 0.7550 - val_loss: 1.6742 - val_accuracy: 0.7778\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.6417 - accuracy: 0.7650 - val_loss: 1.6216 - val_accuracy: 0.7778\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 1s 189ms/step - loss: 1.5880 - accuracy: 0.8000 - val_loss: 1.5542 - val_accuracy: 0.8519\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 1s 180ms/step - loss: 1.5127 - accuracy: 0.8950 - val_loss: 1.4933 - val_accuracy: 0.9444\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 1s 186ms/step - loss: 1.4535 - accuracy: 0.9650 - val_loss: 1.4313 - val_accuracy: 0.9630\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 1.3927 - accuracy: 0.9700 - val_loss: 1.3781 - val_accuracy: 0.9630\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 1.3353 - accuracy: 0.9850 - val_loss: 1.3328 - val_accuracy: 0.9444\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 192ms/step - loss: 1.2910 - accuracy: 0.9800 - val_loss: 1.2913 - val_accuracy: 0.9259\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 1.2438 - accuracy: 0.9600 - val_loss: 1.2436 - val_accuracy: 0.9630\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 1.1954 - accuracy: 0.9700 - val_loss: 1.2027 - val_accuracy: 0.9630\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 80ms/step - loss: 1.1474 - accuracy: 0.9750 - val_loss: 1.1602 - val_accuracy: 0.9537\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 1.1045 - accuracy: 0.9750 - val_loss: 1.1114 - val_accuracy: 0.9630\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 1s 169ms/step - loss: 1.0568 - accuracy: 0.9850 - val_loss: 1.0798 - val_accuracy: 0.9722\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.0113 - accuracy: 1.0000 - val_loss: 1.0338 - val_accuracy: 0.9722\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 0.9707 - accuracy: 1.0000 - val_loss: 0.9908 - val_accuracy: 0.9722\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 0.9384 - accuracy: 0.9950 - val_loss: 0.9646 - val_accuracy: 0.9815\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 0.9024 - accuracy: 1.0000 - val_loss: 0.9322 - val_accuracy: 0.9722\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 365ms/step - loss: 2.4330 - accuracy: 0.1900 - val_loss: 2.2050 - val_accuracy: 0.5093\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 2.1907 - accuracy: 0.4100 - val_loss: 2.1163 - val_accuracy: 0.4722\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 2.1044 - accuracy: 0.4200 - val_loss: 2.0589 - val_accuracy: 0.4722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 79ms/step - loss: 2.0504 - accuracy: 0.4900 - val_loss: 2.0114 - val_accuracy: 0.4352\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 1.9938 - accuracy: 0.5050 - val_loss: 1.9607 - val_accuracy: 0.6667\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 1.9467 - accuracy: 0.7600 - val_loss: 1.9271 - val_accuracy: 0.7222\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 82ms/step - loss: 1.9106 - accuracy: 0.6950 - val_loss: 1.8926 - val_accuracy: 0.7222\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 1.8763 - accuracy: 0.7700 - val_loss: 1.8680 - val_accuracy: 0.7315\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 1.8543 - accuracy: 0.7450 - val_loss: 1.8447 - val_accuracy: 0.7593\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 1s 180ms/step - loss: 1.8352 - accuracy: 0.7850 - val_loss: 1.8277 - val_accuracy: 0.8426\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 227ms/step - loss: 1.8008 - accuracy: 0.8550 - val_loss: 1.7871 - val_accuracy: 0.8796\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 1.7676 - accuracy: 0.8950 - val_loss: 1.7656 - val_accuracy: 0.8981\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 1.7385 - accuracy: 0.9200 - val_loss: 1.7356 - val_accuracy: 0.8981\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 1s 161ms/step - loss: 1.7123 - accuracy: 0.9400 - val_loss: 1.7033 - val_accuracy: 0.9259\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 1.6770 - accuracy: 0.9800 - val_loss: 1.6769 - val_accuracy: 0.9630\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 1s 185ms/step - loss: 1.6449 - accuracy: 0.9900 - val_loss: 1.6463 - val_accuracy: 0.9722\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 1.6182 - accuracy: 0.9850 - val_loss: 1.6221 - val_accuracy: 0.9815\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 81ms/step - loss: 1.5912 - accuracy: 0.9950 - val_loss: 1.5960 - val_accuracy: 0.9630\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 80ms/step - loss: 1.5624 - accuracy: 0.9950 - val_loss: 1.5682 - val_accuracy: 0.9815\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 81ms/step - loss: 1.5374 - accuracy: 1.0000 - val_loss: 1.5484 - val_accuracy: 0.9537\n"
     ]
    }
   ],
   "source": [
    "high_acc = 0\n",
    "for run in range(0,10):\n",
    "    # feature extraction layers\n",
    "    resnet_model = ResNet50(input_shape=(224, 224,3), include_top=False, weights=\"imagenet\")\n",
    "    for layer in resnet_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    d = resnet_model.output.shape[-1] # dimension of last layer\n",
    "\n",
    "\n",
    "    ###################### model 2 ###################### \n",
    "    layer_2_0 = tf.keras.layers.Dense(d,name=\"weight_2\")(resnet_model.output) #times weight before flatten\n",
    "    layer_2_1 = tf.keras.layers.Flatten(name='flatten_2')(layer_2_0)\n",
    "\n",
    "    Dense_2_1 = tf.keras.layers.Dense(shape_2_1, activation=actv_fun_2_1,name='fc2_1')\n",
    "    layer_2_2  = Dense_2_1(layer_2_1)\n",
    "    Dense_2_2 = tf.keras.layers.Dense(shape_2_2, activation=actv_fun_2_2,name='fc2_2')\n",
    "    layer_2_3  = Dense_2_2(layer_2_2)\n",
    "\n",
    "    Dense_2_3 = tf.keras.layers.Dense(10, activation='softmax',name='command_output')\n",
    "    out_layer_2 = Dense_2_3(layer_2_3)\n",
    "\n",
    "\n",
    "    resnet_model = tf.keras.Model(resnet_model.input, out_layer_2)\n",
    "\n",
    "\n",
    "    # resnet_model.summary() \n",
    "\n",
    "\n",
    "    ##################### training ############################\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    callbacks = [EarlyStopping(monitor = 'val_accuracy', patience=5,restore_best_weights=True)]\n",
    "    resnet_model.compile(optimizer=opt, loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "       \n",
    "    history1 = resnet_model.fit(Train_Inputs,Train_command_class,\n",
    "                                validation_data=(Val_Inputs,Val_command_class),\n",
    "                                callbacks=callbacks,\n",
    "                                batch_size=64,\n",
    "                                epochs=10)\n",
    "    \n",
    "    for layer in resnet_model.layers[0:175]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    history2 = resnet_model.fit(Train_Inputs,Train_command_class,\n",
    "                            validation_data=(Val_Inputs,Val_command_class), \n",
    "                           callbacks=callbacks,\n",
    "                           batch_size=64,\n",
    "                           epochs=10)\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    run_time = stop - start\n",
    "    \n",
    "    ##################### test performance ############################\n",
    "    predictions = resnet_model.predict(Test_Inputs)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class\n",
    "    acc_p15_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)\n",
    "\n",
    "\n",
    "    # test on p1\n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_1)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_1, axis=-1)\n",
    "    acc_p1_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p2\n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_2)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_2, axis=-1)\n",
    "    acc_p2_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p3 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_3)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_3, axis=-1)\n",
    "    acc_p3_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p4\n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_4)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_4, axis=-1)\n",
    "    acc_p4_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p5\n",
    "\n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_5)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_5, axis=-1)\n",
    "    acc_p5_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p6 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_6)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class_6\n",
    "    acc_p6 = metrics.accuracy_score(true_classes, predicted_classes).round(4)                \n",
    "\n",
    "    # test on p7 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_7)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class_7\n",
    "    acc_p7 = metrics.accuracy_score(true_classes, predicted_classes).round(4)    \n",
    "\n",
    "    # test on p8 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_8)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class_8\n",
    "    acc_p8 = metrics.accuracy_score(true_classes, predicted_classes).round(4)    \n",
    "\n",
    "    Perfomance = Perfomance.append({'Model': \"Subjet_2\",'Size':'20','Time':run_time,'Command_Acc_p15':acc_p15_c,\n",
    "                                    'Command_Acc_p1':acc_p1_c,'Command_Acc_p2':acc_p2_c,\n",
    "                                    'Command_Acc_p3':acc_p3_c,'Command_Acc_p4':acc_p4_c,'Command_Acc_p5':acc_p5_c,\n",
    "                                    'Acc_p6':acc_p6,'Acc_p7':acc_p7,'Acc_p8':acc_p8}, ignore_index=True)\n",
    "\n",
    "\n",
    "    if high_acc < acc_p15_c :\n",
    "        resnet_model.save('Initial_subject_2_model_size20_0608.h5')\n",
    "        high_acc = acc_p15_c \n",
    "        best_index = run\n",
    "        pd.DataFrame.from_dict(history1.history).to_csv('subject_2_size20_history1_0608.csv',index=False)\n",
    "        pd.DataFrame.from_dict(history2.history).to_csv('subject_2_size20_history2_0608.csv',index=False)\n",
    "        \n",
    "    del resnet_model\n",
    "    run = run + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "0.9074\n"
     ]
    }
   ],
   "source": [
    "resnet_model = tf.keras.models.load_model('Initial_subject_2_model_size20_0608.h5')\n",
    "predictions = resnet_model.predict(Test_Inputs_2)\n",
    "predicted_classes =  np.argmax(predictions, axis=1)\n",
    "acc_c = round(sum(x == y for x, y in zip(np.argmax(Test_command_class_2, axis=1), predicted_classes)) / len(np.argmax(Test_command_class_2, axis=1)),4)\n",
    "print(acc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Time</th>\n",
       "      <th>Command_Acc_p15</th>\n",
       "      <th>Command_Acc_p1</th>\n",
       "      <th>Command_Acc_p2</th>\n",
       "      <th>Command_Acc_p3</th>\n",
       "      <th>Command_Acc_p4</th>\n",
       "      <th>Command_Acc_p5</th>\n",
       "      <th>Acc_p6</th>\n",
       "      <th>Acc_p7</th>\n",
       "      <th>Acc_p8</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.269878</td>\n",
       "      <td>0.6243</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>12.062418</td>\n",
       "      <td>0.5709</td>\n",
       "      <td>0.9189</td>\n",
       "      <td>0.3148</td>\n",
       "      <td>0.4696</td>\n",
       "      <td>0.5413</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.520935</td>\n",
       "      <td>0.6335</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.6514</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.664869</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.2685</td>\n",
       "      <td>0.5217</td>\n",
       "      <td>0.6789</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.2970</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.347304</td>\n",
       "      <td>0.6188</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.2574</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.131006</td>\n",
       "      <td>0.5672</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>0.4696</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.651466</td>\n",
       "      <td>0.5930</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.2685</td>\n",
       "      <td>0.5130</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>12.100064</td>\n",
       "      <td>0.5727</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>0.4957</td>\n",
       "      <td>0.5413</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2574</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.176442</td>\n",
       "      <td>0.6151</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.3611</td>\n",
       "      <td>0.4957</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.929373</td>\n",
       "      <td>0.6446</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.4722</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.902663</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.6514</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.2178</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.805319</td>\n",
       "      <td>0.6538</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3519</td>\n",
       "      <td>0.5913</td>\n",
       "      <td>0.6697</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.685424</td>\n",
       "      <td>0.6298</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.194757</td>\n",
       "      <td>0.6390</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3981</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.364569</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>0.5739</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.957827</td>\n",
       "      <td>0.6317</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.3981</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.528849</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.4352</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.993745</td>\n",
       "      <td>0.6538</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.3611</td>\n",
       "      <td>0.5913</td>\n",
       "      <td>0.6697</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.823134</td>\n",
       "      <td>0.6446</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.6606</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.812350</td>\n",
       "      <td>0.6354</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>0.5304</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.680686</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>0.5913</td>\n",
       "      <td>0.6697</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.529958</td>\n",
       "      <td>0.6851</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.4259</td>\n",
       "      <td>0.5826</td>\n",
       "      <td>0.7248</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2178</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>17.791133</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4352</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.228115</td>\n",
       "      <td>0.6483</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.6697</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.815590</td>\n",
       "      <td>0.6740</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.099657</td>\n",
       "      <td>0.6980</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4630</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.7523</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.533707</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.3981</td>\n",
       "      <td>0.5826</td>\n",
       "      <td>0.6514</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.714685</td>\n",
       "      <td>0.6501</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.6606</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.797302</td>\n",
       "      <td>0.6556</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4259</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.6514</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>19.045552</td>\n",
       "      <td>0.6924</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5463</td>\n",
       "      <td>0.5739</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>15.484598</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.2342</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.3130</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.2178</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.018696</td>\n",
       "      <td>0.4144</td>\n",
       "      <td>0.2432</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>0.3578</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>11.138138</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>0.3043</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.3465</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.840607</td>\n",
       "      <td>0.4033</td>\n",
       "      <td>0.2162</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>12.762539</td>\n",
       "      <td>0.3996</td>\n",
       "      <td>0.2072</td>\n",
       "      <td>0.8426</td>\n",
       "      <td>0.3565</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.125518</td>\n",
       "      <td>0.4144</td>\n",
       "      <td>0.2162</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>10.908749</td>\n",
       "      <td>0.3812</td>\n",
       "      <td>0.1802</td>\n",
       "      <td>0.8519</td>\n",
       "      <td>0.2696</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>13.933343</td>\n",
       "      <td>0.3941</td>\n",
       "      <td>0.2523</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.3578</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.2178</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>13.496468</td>\n",
       "      <td>0.4217</td>\n",
       "      <td>0.2523</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.3304</td>\n",
       "      <td>0.3670</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.3366</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>13.351789</td>\n",
       "      <td>0.3886</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.123689</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.2162</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.2696</td>\n",
       "      <td>0.3211</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>15.569148</td>\n",
       "      <td>0.4015</td>\n",
       "      <td>0.2973</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.3130</td>\n",
       "      <td>0.2936</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>15.396336</td>\n",
       "      <td>0.4180</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.3130</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>16.548319</td>\n",
       "      <td>0.4180</td>\n",
       "      <td>0.2883</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.802912</td>\n",
       "      <td>0.4420</td>\n",
       "      <td>0.2973</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.3913</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.052941</td>\n",
       "      <td>0.4125</td>\n",
       "      <td>0.2432</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>0.2936</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.914502</td>\n",
       "      <td>0.4217</td>\n",
       "      <td>0.2072</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.3217</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>16.833878</td>\n",
       "      <td>0.4033</td>\n",
       "      <td>0.2072</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.3119</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2970</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>15.493673</td>\n",
       "      <td>0.4217</td>\n",
       "      <td>0.2432</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>15.888272</td>\n",
       "      <td>0.4088</td>\n",
       "      <td>0.2432</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>0.3119</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.2970</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model       Time  Command_Acc_p15  Command_Acc_p1  Command_Acc_p2  \\\n",
       "0   Subjet_1  15.269878           0.6243          0.9369          0.3889   \n",
       "1   Subjet_1  12.062418           0.5709          0.9189          0.3148   \n",
       "2   Subjet_1  13.520935           0.6335          0.9550          0.3889   \n",
       "3   Subjet_1  14.664869           0.6225          0.9550          0.2685   \n",
       "4   Subjet_1  13.347304           0.6188          0.9279          0.3796   \n",
       "5   Subjet_1  14.131006           0.5672          0.9279          0.2870   \n",
       "6   Subjet_1  13.651466           0.5930          0.9459          0.2685   \n",
       "7   Subjet_1  12.100064           0.5727          0.9279          0.2870   \n",
       "8   Subjet_1  13.176442           0.6151          0.9459          0.3611   \n",
       "9   Subjet_1  13.929373           0.6446          0.9369          0.4722   \n",
       "10  Subjet_1  15.902663           0.6409          0.9820          0.3704   \n",
       "11  Subjet_1  13.805319           0.6538          1.0000          0.3519   \n",
       "12  Subjet_1  15.685424           0.6298          0.9820          0.3333   \n",
       "13  Subjet_1  16.194757           0.6390          0.9820          0.3981   \n",
       "14  Subjet_1  15.364569           0.6611          0.9820          0.3796   \n",
       "15  Subjet_1  14.957827           0.6317          0.9730          0.3981   \n",
       "16  Subjet_1  15.528849           0.6409          0.9910          0.4352   \n",
       "17  Subjet_1  15.993745           0.6538          0.9910          0.3611   \n",
       "18  Subjet_1  16.823134           0.6446          0.9730          0.3704   \n",
       "19  Subjet_1  15.812350           0.6354          0.9820          0.3889   \n",
       "20  Subjet_1  16.680686           0.6667          1.0000          0.3889   \n",
       "21  Subjet_1  14.529958           0.6851          0.9910          0.4259   \n",
       "22  Subjet_1  17.791133           0.6796          1.0000          0.4352   \n",
       "23  Subjet_1  16.228115           0.6483          1.0000          0.3704   \n",
       "24  Subjet_1  14.815590           0.6740          1.0000          0.4167   \n",
       "25  Subjet_1  16.099657           0.6980          1.0000          0.4630   \n",
       "26  Subjet_1  14.533707           0.6667          0.9910          0.3981   \n",
       "27  Subjet_1  15.714685           0.6501          1.0000          0.4167   \n",
       "28  Subjet_1  15.797302           0.6556          1.0000          0.4259   \n",
       "29  Subjet_1  19.045552           0.6924          1.0000          0.5463   \n",
       "30  Subjet_2  15.484598           0.3959          0.2342          0.8611   \n",
       "31  Subjet_2  14.018696           0.4144          0.2432          0.8611   \n",
       "32  Subjet_2  11.138138           0.3738          0.1982          0.7778   \n",
       "33  Subjet_2  14.840607           0.4033          0.2162          0.8611   \n",
       "34  Subjet_2  12.762539           0.3996          0.2072          0.8426   \n",
       "35  Subjet_2  14.125518           0.4144          0.2162          0.8611   \n",
       "36  Subjet_2  10.908749           0.3812          0.1802          0.8519   \n",
       "37  Subjet_2  13.933343           0.3941          0.2523          0.8333   \n",
       "38  Subjet_2  13.496468           0.4217          0.2523          0.8611   \n",
       "39  Subjet_2  13.351789           0.3886          0.1982          0.8704   \n",
       "40  Subjet_2  14.123689           0.3959          0.2162          0.8889   \n",
       "41  Subjet_2  15.569148           0.4015          0.2973          0.8611   \n",
       "42  Subjet_2  15.396336           0.4180          0.2252          0.9167   \n",
       "43  Subjet_2  16.548319           0.4180          0.2883          0.8981   \n",
       "44  Subjet_2  14.802912           0.4420          0.2973          0.9074   \n",
       "45  Subjet_2  14.052941           0.4125          0.2432          0.8889   \n",
       "46  Subjet_2  14.914502           0.4217          0.2072          0.9537   \n",
       "47  Subjet_2  16.833878           0.4033          0.2072          0.9259   \n",
       "48  Subjet_2  15.493673           0.4217          0.2432          0.9444   \n",
       "49  Subjet_2  15.888272           0.4088          0.2432          0.9259   \n",
       "\n",
       "    Command_Acc_p3  Command_Acc_p4  Command_Acc_p5  Acc_p6  Acc_p7  Acc_p8  \\\n",
       "0           0.5391          0.6239            0.63    0.38   0.384  0.2673   \n",
       "1           0.4696          0.5413            0.61    0.33   0.320  0.2673   \n",
       "2           0.5478          0.6514            0.62    0.48   0.384  0.2475   \n",
       "3           0.5217          0.6789            0.69    0.26   0.456  0.2970   \n",
       "4           0.5652          0.5780            0.64    0.39   0.416  0.2574   \n",
       "5           0.4696          0.5780            0.57    0.25   0.328  0.2673   \n",
       "6           0.5130          0.6239            0.61    0.43   0.440  0.2673   \n",
       "7           0.4957          0.5413            0.61    0.41   0.384  0.2574   \n",
       "8           0.4957          0.6147            0.66    0.41   0.408  0.2871   \n",
       "9           0.5391          0.6147            0.66    0.48   0.408  0.2673   \n",
       "10          0.5565          0.6514            0.64    0.34   0.400  0.2178   \n",
       "11          0.5913          0.6697            0.65    0.45   0.360  0.2277   \n",
       "12          0.5391          0.6422            0.65    0.43   0.408  0.2277   \n",
       "13          0.6000          0.6147            0.59    0.36   0.384  0.2475   \n",
       "14          0.5739          0.6972            0.67    0.44   0.400  0.2673   \n",
       "15          0.5652          0.6239            0.59    0.38   0.440  0.2475   \n",
       "16          0.5565          0.6239            0.59    0.37   0.360  0.2079   \n",
       "17          0.5913          0.6697            0.65    0.41   0.360  0.2277   \n",
       "18          0.5478          0.6606            0.67    0.37   0.384  0.2475   \n",
       "19          0.5304          0.6422            0.63    0.33   0.408  0.2772   \n",
       "20          0.5913          0.6697            0.68    0.43   0.352  0.2079   \n",
       "21          0.5826          0.7248            0.70    0.42   0.360  0.2178   \n",
       "22          0.5652          0.7156            0.68    0.36   0.328  0.2277   \n",
       "23          0.5565          0.6697            0.64    0.41   0.344  0.2079   \n",
       "24          0.5565          0.7339            0.66    0.45   0.344  0.2079   \n",
       "25          0.6000          0.7523            0.67    0.37   0.352  0.2079   \n",
       "26          0.5826          0.6514            0.71    0.36   0.360  0.2277   \n",
       "27          0.5391          0.6606            0.63    0.38   0.360  0.2277   \n",
       "28          0.5565          0.6514            0.64    0.39   0.352  0.2079   \n",
       "29          0.5739          0.7156            0.62    0.41   0.376  0.2277   \n",
       "30          0.3130          0.3303            0.24    0.21   0.344  0.2178   \n",
       "31          0.3478          0.3578            0.26    0.14   0.320  0.2871   \n",
       "32          0.3043          0.3028            0.29    0.16   0.272  0.3465   \n",
       "33          0.3478          0.3303            0.26    0.21   0.312  0.3168   \n",
       "34          0.3565          0.3028            0.29    0.16   0.312  0.3168   \n",
       "35          0.3391          0.3486            0.31    0.22   0.352  0.3168   \n",
       "36          0.2696          0.3303            0.28    0.14   0.256  0.2673   \n",
       "37          0.2957          0.3578            0.23    0.27   0.288  0.2178   \n",
       "38          0.3304          0.3670            0.30    0.18   0.280  0.3366   \n",
       "39          0.2870          0.3303            0.26    0.13   0.272  0.2673   \n",
       "40          0.2696          0.3211            0.29    0.17   0.368  0.3168   \n",
       "41          0.3130          0.2936            0.24    0.22   0.296  0.2079   \n",
       "42          0.3130          0.3486            0.29    0.23   0.336  0.2673   \n",
       "43          0.2870          0.3028            0.32    0.25   0.360  0.2871   \n",
       "44          0.3913          0.3303            0.28    0.17   0.320  0.3564   \n",
       "45          0.3478          0.2936            0.29    0.24   0.304  0.2673   \n",
       "46          0.3217          0.3303            0.30    0.19   0.368  0.2772   \n",
       "47          0.2957          0.3119            0.28    0.23   0.360  0.2970   \n",
       "48          0.2870          0.3486            0.29    0.17   0.328  0.2772   \n",
       "49          0.2870          0.3119            0.28    0.21   0.320  0.2970   \n",
       "\n",
       "   Size  \n",
       "0    10  \n",
       "1    10  \n",
       "2    10  \n",
       "3    10  \n",
       "4    10  \n",
       "5    10  \n",
       "6    10  \n",
       "7    10  \n",
       "8    10  \n",
       "9    10  \n",
       "10   20  \n",
       "11   20  \n",
       "12   20  \n",
       "13   20  \n",
       "14   20  \n",
       "15   20  \n",
       "16   20  \n",
       "17   20  \n",
       "18   20  \n",
       "19   20  \n",
       "20   30  \n",
       "21   30  \n",
       "22   30  \n",
       "23   30  \n",
       "24   30  \n",
       "25   30  \n",
       "26   30  \n",
       "27   30  \n",
       "28   30  \n",
       "29   30  \n",
       "30   10  \n",
       "31   10  \n",
       "32   10  \n",
       "33   10  \n",
       "34   10  \n",
       "35   10  \n",
       "36   10  \n",
       "37   10  \n",
       "38   10  \n",
       "39   10  \n",
       "40   20  \n",
       "41   20  \n",
       "42   20  \n",
       "43   20  \n",
       "44   20  \n",
       "45   20  \n",
       "46   20  \n",
       "47   20  \n",
       "48   20  \n",
       "49   20  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single model （size:30）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 2\n",
    "train_image = 30 #10:20%, 20: 40%, 30:60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30., 30., 30., 30., 30., 30., 30., 30., 30., 30.], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_indexs_train = (image_no<train_image)&(ALL_participant_class==subject)\n",
    "Train_Inputs = All_Inputs[select_indexs_train]\n",
    "Train_command_class = All_command_class[select_indexs_train]\n",
    "sum(Train_command_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 2s 345ms/step - loss: 2.2726 - accuracy: 0.2400 - val_loss: 2.0401 - val_accuracy: 0.3889\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 1.9797 - accuracy: 0.6167 - val_loss: 1.8767 - val_accuracy: 0.7500\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 1.8455 - accuracy: 0.7333 - val_loss: 1.7848 - val_accuracy: 0.7685\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 1.7533 - accuracy: 0.8433 - val_loss: 1.7087 - val_accuracy: 0.8889\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 1.6802 - accuracy: 0.9167 - val_loss: 1.6480 - val_accuracy: 0.9259\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 1.6200 - accuracy: 0.9400 - val_loss: 1.5908 - val_accuracy: 0.9352\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 1.5597 - accuracy: 0.9400 - val_loss: 1.5297 - val_accuracy: 0.9722\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 1s 112ms/step - loss: 1.4975 - accuracy: 0.9667 - val_loss: 1.4722 - val_accuracy: 0.9537\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 1.4412 - accuracy: 0.9667 - val_loss: 1.4206 - val_accuracy: 0.9722\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 1.3865 - accuracy: 0.9800 - val_loss: 1.3686 - val_accuracy: 0.9630\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 1.3335 - accuracy: 0.9867 - val_loss: 1.3188 - val_accuracy: 0.9722\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 1.2804 - accuracy: 0.9867 - val_loss: 1.2726 - val_accuracy: 0.9722\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 1.2287 - accuracy: 0.9867 - val_loss: 1.2254 - val_accuracy: 0.9907\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 1.1778 - accuracy: 0.9867 - val_loss: 1.1728 - val_accuracy: 0.9907\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 1.1294 - accuracy: 0.9933 - val_loss: 1.1280 - val_accuracy: 0.9815\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 1.0818 - accuracy: 0.9933 - val_loss: 1.0820 - val_accuracy: 0.9907\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 1.0369 - accuracy: 1.0000 - val_loss: 1.0444 - val_accuracy: 0.9907\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.9914 - accuracy: 1.0000 - val_loss: 0.9989 - val_accuracy: 0.9907\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 2s 303ms/step - loss: 2.3028 - accuracy: 0.2600 - val_loss: 2.0790 - val_accuracy: 0.5463\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 2.0019 - accuracy: 0.6400 - val_loss: 1.9160 - val_accuracy: 0.8519\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 1.8768 - accuracy: 0.8467 - val_loss: 1.8121 - val_accuracy: 0.8704\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 1.7832 - accuracy: 0.8167 - val_loss: 1.7468 - val_accuracy: 0.9352\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 1.7157 - accuracy: 0.9267 - val_loss: 1.6815 - val_accuracy: 0.9537\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 1.6532 - accuracy: 0.9467 - val_loss: 1.6209 - val_accuracy: 0.9722\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 1.5912 - accuracy: 0.9367 - val_loss: 1.5660 - val_accuracy: 0.9722\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 1.5303 - accuracy: 0.9833 - val_loss: 1.5051 - val_accuracy: 0.9722\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 1.4735 - accuracy: 0.9433 - val_loss: 1.4573 - val_accuracy: 0.9630\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 1.4191 - accuracy: 0.9633 - val_loss: 1.4013 - val_accuracy: 0.9815\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 1.3609 - accuracy: 0.9667 - val_loss: 1.3506 - val_accuracy: 0.9815\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 1.3088 - accuracy: 0.9833 - val_loss: 1.3004 - val_accuracy: 0.9907\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 1.2562 - accuracy: 0.9833 - val_loss: 1.2538 - val_accuracy: 0.9907\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 1.2068 - accuracy: 0.9933 - val_loss: 1.2062 - val_accuracy: 0.9907\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 124ms/step - loss: 1.1579 - accuracy: 0.9967 - val_loss: 1.1592 - val_accuracy: 0.9907\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 1.1070 - accuracy: 0.9967 - val_loss: 1.1136 - val_accuracy: 0.9907\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 1.0595 - accuracy: 1.0000 - val_loss: 1.0711 - val_accuracy: 0.9907\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 2s 301ms/step - loss: 2.2603 - accuracy: 0.2700 - val_loss: 2.0217 - val_accuracy: 0.6574\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 1.9612 - accuracy: 0.6300 - val_loss: 1.8689 - val_accuracy: 0.6944\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 1.8259 - accuracy: 0.7200 - val_loss: 1.7551 - val_accuracy: 0.8704\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 1.7280 - accuracy: 0.9000 - val_loss: 1.6757 - val_accuracy: 0.8981\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 1.6458 - accuracy: 0.9167 - val_loss: 1.6019 - val_accuracy: 0.9722\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 1.5696 - accuracy: 0.9467 - val_loss: 1.5331 - val_accuracy: 0.9630\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 1.4984 - accuracy: 0.9633 - val_loss: 1.4652 - val_accuracy: 0.9722\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 1s 170ms/step - loss: 1.4290 - accuracy: 0.9767 - val_loss: 1.4000 - val_accuracy: 0.9907\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 1.3623 - accuracy: 0.9833 - val_loss: 1.3358 - val_accuracy: 0.9907\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 1.3005 - accuracy: 0.9733 - val_loss: 1.2832 - val_accuracy: 0.9815\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 174ms/step - loss: 1.2379 - accuracy: 0.9833 - val_loss: 1.2222 - val_accuracy: 0.9907\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 1.1787 - accuracy: 0.9867 - val_loss: 1.1698 - val_accuracy: 0.9815\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 1.1197 - accuracy: 0.9900 - val_loss: 1.1156 - val_accuracy: 0.9907\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 1.0654 - accuracy: 0.9933 - val_loss: 1.0667 - val_accuracy: 0.9907\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 1.0117 - accuracy: 1.0000 - val_loss: 1.0151 - val_accuracy: 0.9815\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.9577 - accuracy: 1.0000 - val_loss: 0.9625 - val_accuracy: 0.9907\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 2s 337ms/step - loss: 2.3367 - accuracy: 0.1667 - val_loss: 2.0709 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 2.0053 - accuracy: 0.5433 - val_loss: 1.9228 - val_accuracy: 0.7315\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 1.8768 - accuracy: 0.7767 - val_loss: 1.8209 - val_accuracy: 0.8056\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 1.7901 - accuracy: 0.7867 - val_loss: 1.7491 - val_accuracy: 0.7963\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 174ms/step - loss: 1.7267 - accuracy: 0.8167 - val_loss: 1.6882 - val_accuracy: 0.8426\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 175ms/step - loss: 1.6636 - accuracy: 0.9067 - val_loss: 1.6273 - val_accuracy: 0.8889\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 1.5999 - accuracy: 0.9433 - val_loss: 1.5690 - val_accuracy: 0.9722\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 97ms/step - loss: 1.5418 - accuracy: 0.9700 - val_loss: 1.5163 - val_accuracy: 0.9722\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 1s 170ms/step - loss: 1.4890 - accuracy: 0.9700 - val_loss: 1.4616 - val_accuracy: 0.9815\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 1.4329 - accuracy: 0.9833 - val_loss: 1.4066 - val_accuracy: 0.9907\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 1.3757 - accuracy: 0.9933 - val_loss: 1.3572 - val_accuracy: 0.9907\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 1.3235 - accuracy: 0.9867 - val_loss: 1.3052 - val_accuracy: 0.9907\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 1.2665 - accuracy: 0.9900 - val_loss: 1.2513 - val_accuracy: 0.9907\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 1.2146 - accuracy: 0.9900 - val_loss: 1.2037 - val_accuracy: 0.9907\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 1.1621 - accuracy: 0.9933 - val_loss: 1.1550 - val_accuracy: 0.9907\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 1.1145 - accuracy: 0.9933 - val_loss: 1.1088 - val_accuracy: 0.9907\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 2.4374 - accuracy: 0.0900 - val_loss: 2.1783 - val_accuracy: 0.1852\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 186ms/step - loss: 2.1016 - accuracy: 0.2967 - val_loss: 2.0080 - val_accuracy: 0.4815\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 1.9681 - accuracy: 0.6567 - val_loss: 1.9167 - val_accuracy: 0.7685\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 1.8903 - accuracy: 0.7867 - val_loss: 1.8594 - val_accuracy: 0.7593\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 1.8276 - accuracy: 0.8133 - val_loss: 1.8041 - val_accuracy: 0.8426\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 1.7821 - accuracy: 0.8433 - val_loss: 1.7558 - val_accuracy: 0.8333\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 1.7311 - accuracy: 0.8567 - val_loss: 1.7112 - val_accuracy: 0.8241\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 1.6794 - accuracy: 0.9033 - val_loss: 1.6631 - val_accuracy: 0.9630\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 1.6300 - accuracy: 0.9733 - val_loss: 1.6144 - val_accuracy: 0.9815\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 1.5787 - accuracy: 0.9833 - val_loss: 1.5644 - val_accuracy: 0.9815\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 188ms/step - loss: 1.5320 - accuracy: 0.9800 - val_loss: 1.5197 - val_accuracy: 0.9630\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 1.4858 - accuracy: 0.9933 - val_loss: 1.4758 - val_accuracy: 0.9630\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 1.4414 - accuracy: 0.9933 - val_loss: 1.4352 - val_accuracy: 0.9907\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 1.3996 - accuracy: 0.9867 - val_loss: 1.3925 - val_accuracy: 0.9815\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 1.3544 - accuracy: 0.9933 - val_loss: 1.3514 - val_accuracy: 0.9907\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 129ms/step - loss: 1.3082 - accuracy: 0.9967 - val_loss: 1.3087 - val_accuracy: 0.9815\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 1.2662 - accuracy: 0.9967 - val_loss: 1.2723 - val_accuracy: 0.9907\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 1.2258 - accuracy: 0.9967 - val_loss: 1.2313 - val_accuracy: 0.9815\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 2s 331ms/step - loss: 2.2904 - accuracy: 0.1867 - val_loss: 2.0874 - val_accuracy: 0.4259\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 2.0018 - accuracy: 0.4867 - val_loss: 1.8990 - val_accuracy: 0.6019\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 1.8501 - accuracy: 0.7633 - val_loss: 1.7842 - val_accuracy: 0.7593\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 1.7510 - accuracy: 0.7967 - val_loss: 1.7036 - val_accuracy: 0.8889\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 1.6740 - accuracy: 0.9100 - val_loss: 1.6339 - val_accuracy: 0.9537\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 1.5993 - accuracy: 0.9433 - val_loss: 1.5688 - val_accuracy: 0.9630\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 1.5387 - accuracy: 0.9500 - val_loss: 1.5063 - val_accuracy: 0.9907\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 1.4728 - accuracy: 0.9633 - val_loss: 1.4499 - val_accuracy: 0.9722\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 1.4150 - accuracy: 0.9767 - val_loss: 1.3974 - val_accuracy: 0.9907\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 1.3570 - accuracy: 0.9667 - val_loss: 1.3450 - val_accuracy: 0.9907\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 1.3020 - accuracy: 0.9700 - val_loss: 1.2930 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 1.2486 - accuracy: 0.9733 - val_loss: 1.2435 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 1.1976 - accuracy: 0.9867 - val_loss: 1.1935 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 1.1459 - accuracy: 0.9900 - val_loss: 1.1482 - val_accuracy: 0.9907\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 1.0956 - accuracy: 0.9900 - val_loss: 1.0991 - val_accuracy: 0.9907\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 1.0479 - accuracy: 0.9967 - val_loss: 1.0572 - val_accuracy: 0.9907\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 2.3165 - accuracy: 0.2833 - val_loss: 2.0720 - val_accuracy: 0.5463\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 2.0021 - accuracy: 0.6900 - val_loss: 1.9020 - val_accuracy: 0.7315\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 176ms/step - loss: 1.8612 - accuracy: 0.7867 - val_loss: 1.8105 - val_accuracy: 0.8056\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 1.7745 - accuracy: 0.8167 - val_loss: 1.7356 - val_accuracy: 0.8056\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 1.7059 - accuracy: 0.8300 - val_loss: 1.6786 - val_accuracy: 0.8889\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 1.6455 - accuracy: 0.8900 - val_loss: 1.6155 - val_accuracy: 0.9167\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 1.5893 - accuracy: 0.9100 - val_loss: 1.5628 - val_accuracy: 0.9444\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 1.5322 - accuracy: 0.9333 - val_loss: 1.5091 - val_accuracy: 0.9537\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 1.4764 - accuracy: 0.9333 - val_loss: 1.4565 - val_accuracy: 0.9444\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 1.4231 - accuracy: 0.9300 - val_loss: 1.4042 - val_accuracy: 0.9352\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 195ms/step - loss: 1.3716 - accuracy: 0.9400 - val_loss: 1.3586 - val_accuracy: 0.9630\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 169ms/step - loss: 1.3208 - accuracy: 0.9533 - val_loss: 1.3067 - val_accuracy: 0.9722\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 1.2708 - accuracy: 0.9767 - val_loss: 1.2605 - val_accuracy: 0.9630\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 1.2210 - accuracy: 0.9867 - val_loss: 1.2170 - val_accuracy: 0.9815\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 1.1714 - accuracy: 0.9933 - val_loss: 1.1699 - val_accuracy: 0.9907\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 1.1249 - accuracy: 0.9933 - val_loss: 1.1263 - val_accuracy: 0.9907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 1.0791 - accuracy: 0.9967 - val_loss: 1.0870 - val_accuracy: 0.9907\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 1.0362 - accuracy: 0.9967 - val_loss: 1.0422 - val_accuracy: 0.9907\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.9919 - accuracy: 1.0000 - val_loss: 1.0010 - val_accuracy: 0.9907\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.9502 - accuracy: 1.0000 - val_loss: 0.9585 - val_accuracy: 0.9907\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 2s 348ms/step - loss: 2.3417 - accuracy: 0.2767 - val_loss: 2.1211 - val_accuracy: 0.5370\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 2.0667 - accuracy: 0.5600 - val_loss: 1.9927 - val_accuracy: 0.6204\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 174ms/step - loss: 1.9540 - accuracy: 0.7333 - val_loss: 1.9027 - val_accuracy: 0.8056\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 1.8713 - accuracy: 0.8267 - val_loss: 1.8414 - val_accuracy: 0.8148\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 1.8166 - accuracy: 0.8933 - val_loss: 1.7862 - val_accuracy: 0.8981\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 1.7617 - accuracy: 0.9267 - val_loss: 1.7414 - val_accuracy: 0.8981\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 1.7143 - accuracy: 0.8900 - val_loss: 1.6921 - val_accuracy: 0.9259\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 1.6643 - accuracy: 0.9500 - val_loss: 1.6445 - val_accuracy: 0.9444\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 1.6159 - accuracy: 0.9600 - val_loss: 1.5988 - val_accuracy: 0.9630\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 1.5688 - accuracy: 0.9567 - val_loss: 1.5522 - val_accuracy: 0.9722\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 1.5219 - accuracy: 0.9767 - val_loss: 1.5105 - val_accuracy: 0.9815\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 1.4779 - accuracy: 0.9800 - val_loss: 1.4682 - val_accuracy: 0.9815\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 1.4333 - accuracy: 0.9900 - val_loss: 1.4276 - val_accuracy: 0.9722\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 1.3889 - accuracy: 0.9900 - val_loss: 1.3818 - val_accuracy: 0.9815\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 1.3456 - accuracy: 0.9833 - val_loss: 1.3406 - val_accuracy: 0.9907\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 1.3002 - accuracy: 0.9867 - val_loss: 1.2970 - val_accuracy: 0.9907\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 1s 112ms/step - loss: 1.2578 - accuracy: 0.9933 - val_loss: 1.2592 - val_accuracy: 0.9907\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 1.2181 - accuracy: 0.9933 - val_loss: 1.2202 - val_accuracy: 0.9907\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 1.1778 - accuracy: 0.9967 - val_loss: 1.1815 - val_accuracy: 0.9907\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 1.1378 - accuracy: 0.9967 - val_loss: 1.1456 - val_accuracy: 0.9907\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 2s 331ms/step - loss: 2.2665 - accuracy: 0.3233 - val_loss: 2.0298 - val_accuracy: 0.7222\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 1.9669 - accuracy: 0.6767 - val_loss: 1.8675 - val_accuracy: 0.7778\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 1.8281 - accuracy: 0.8000 - val_loss: 1.7737 - val_accuracy: 0.8704\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 1.7383 - accuracy: 0.9033 - val_loss: 1.6878 - val_accuracy: 0.9444\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 1.6570 - accuracy: 0.9300 - val_loss: 1.6204 - val_accuracy: 0.9722\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 1.5870 - accuracy: 0.9467 - val_loss: 1.5552 - val_accuracy: 0.9537\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 1.5170 - accuracy: 0.9567 - val_loss: 1.4883 - val_accuracy: 0.9537\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 1s 174ms/step - loss: 1.4521 - accuracy: 0.9633 - val_loss: 1.4269 - val_accuracy: 0.9815\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 1.3893 - accuracy: 0.9567 - val_loss: 1.3670 - val_accuracy: 0.9630\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 1.3280 - accuracy: 0.9700 - val_loss: 1.3083 - val_accuracy: 0.9722\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 225ms/step - loss: 1.2664 - accuracy: 0.9700 - val_loss: 1.2525 - val_accuracy: 0.9907\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 1.2107 - accuracy: 0.9767 - val_loss: 1.1952 - val_accuracy: 0.9907\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 1.1510 - accuracy: 0.9767 - val_loss: 1.1430 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 1.0942 - accuracy: 0.9800 - val_loss: 1.0881 - val_accuracy: 0.9815\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 1.0405 - accuracy: 0.9900 - val_loss: 1.0401 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.9933 - accuracy: 0.9900 - val_loss: 0.9959 - val_accuracy: 0.9907\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.9437 - accuracy: 0.9933 - val_loss: 0.9469 - val_accuracy: 0.9907\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.8974 - accuracy: 0.9967 - val_loss: 0.9042 - val_accuracy: 0.9907\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 2s 343ms/step - loss: 2.2911 - accuracy: 0.1967 - val_loss: 2.0357 - val_accuracy: 0.4259\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 1.9457 - accuracy: 0.5633 - val_loss: 1.8384 - val_accuracy: 0.7778\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 1.7848 - accuracy: 0.8400 - val_loss: 1.7293 - val_accuracy: 0.9074\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 1.6831 - accuracy: 0.8867 - val_loss: 1.6387 - val_accuracy: 0.8981\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 1.5957 - accuracy: 0.9167 - val_loss: 1.5644 - val_accuracy: 0.9352\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 175ms/step - loss: 1.5187 - accuracy: 0.9333 - val_loss: 1.4871 - val_accuracy: 0.9537\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 1.4438 - accuracy: 0.9367 - val_loss: 1.4235 - val_accuracy: 0.9630\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 1.3746 - accuracy: 0.9733 - val_loss: 1.3543 - val_accuracy: 0.9444\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 1s 186ms/step - loss: 1.3055 - accuracy: 0.9767 - val_loss: 1.2901 - val_accuracy: 0.9815\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 1s 192ms/step - loss: 1.2404 - accuracy: 0.9800 - val_loss: 1.2287 - val_accuracy: 0.9907\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 1.1752 - accuracy: 0.9867 - val_loss: 1.1667 - val_accuracy: 0.9907\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 1.1136 - accuracy: 0.9867 - val_loss: 1.1087 - val_accuracy: 0.9907\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 1.0524 - accuracy: 0.9933 - val_loss: 1.0540 - val_accuracy: 0.9907\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.9978 - accuracy: 0.9967 - val_loss: 1.0000 - val_accuracy: 0.9907\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.9402 - accuracy: 0.9967 - val_loss: 0.9508 - val_accuracy: 0.9907\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.8882 - accuracy: 1.0000 - val_loss: 0.9025 - val_accuracy: 0.9907\n"
     ]
    }
   ],
   "source": [
    "high_acc = 0\n",
    "for run in range(0,10):\n",
    "    # feature extraction layers\n",
    "    resnet_model = ResNet50(input_shape=(224, 224,3), include_top=False, weights=\"imagenet\")\n",
    "    for layer in resnet_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    d = resnet_model.output.shape[-1] # dimension of last layer\n",
    "\n",
    "\n",
    "    ###################### model 2 ###################### \n",
    "    layer_2_0 = tf.keras.layers.Dense(d,name=\"weight_2\")(resnet_model.output) #times weight before flatten\n",
    "    layer_2_1 = tf.keras.layers.Flatten(name='flatten_2')(layer_2_0)\n",
    "\n",
    "    Dense_2_1 = tf.keras.layers.Dense(shape_2_1, activation=actv_fun_2_1,name='fc2_1')\n",
    "    layer_2_2  = Dense_2_1(layer_2_1)\n",
    "    Dense_2_2 = tf.keras.layers.Dense(shape_2_2, activation=actv_fun_2_2,name='fc2_2')\n",
    "    layer_2_3  = Dense_2_2(layer_2_2)\n",
    "\n",
    "    Dense_2_3 = tf.keras.layers.Dense(10, activation='softmax',name='command_output')\n",
    "    out_layer_2 = Dense_2_3(layer_2_3)\n",
    "\n",
    "\n",
    "    resnet_model = tf.keras.Model(resnet_model.input, out_layer_2)\n",
    "\n",
    "\n",
    "    # resnet_model.summary() \n",
    "\n",
    "\n",
    "    ##################### training ############################\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    callbacks = [EarlyStopping(monitor = 'val_accuracy', patience=5,restore_best_weights=True)]\n",
    "    resnet_model.compile(optimizer=opt, loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "       \n",
    "    history1 = resnet_model.fit(Train_Inputs,Train_command_class,\n",
    "                                validation_data=(Val_Inputs,Val_command_class),\n",
    "                                callbacks=callbacks,\n",
    "                                batch_size=64,\n",
    "                                epochs=10)\n",
    "    \n",
    "    for layer in resnet_model.layers[0:175]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    history2 = resnet_model.fit(Train_Inputs,Train_command_class,\n",
    "                            validation_data=(Val_Inputs,Val_command_class), \n",
    "                           callbacks=callbacks,\n",
    "                           batch_size=64,\n",
    "                           epochs=10)\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    run_time = stop - start\n",
    "    \n",
    "    ##################### test performance ############################\n",
    "    predictions = resnet_model.predict(Test_Inputs)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class\n",
    "    acc_p15_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)\n",
    "\n",
    "\n",
    "    # test on p1\n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_1)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_1, axis=-1)\n",
    "    acc_p1_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p2\n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_2)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_2, axis=-1)\n",
    "    acc_p2_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p3 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_3)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_3, axis=-1)\n",
    "    acc_p3_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p4\n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_4)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_4, axis=-1)\n",
    "    acc_p4_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p5\n",
    "\n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_5)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(Test_command_class_5, axis=-1)\n",
    "    acc_p5_c = metrics.accuracy_score(true_classes, predicted_classes).round(4)   \n",
    "\n",
    "    # test on p6 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_6)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class_6\n",
    "    acc_p6 = metrics.accuracy_score(true_classes, predicted_classes).round(4)                \n",
    "\n",
    "    # test on p7 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_7)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class_7\n",
    "    acc_p7 = metrics.accuracy_score(true_classes, predicted_classes).round(4)    \n",
    "\n",
    "    # test on p8 \n",
    "    ## command classification\n",
    "    predictions = resnet_model.predict(Test_Inputs_8)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_unit_command_class_8\n",
    "    acc_p8 = metrics.accuracy_score(true_classes, predicted_classes).round(4)    \n",
    "\n",
    "    Perfomance = Perfomance.append({'Model': \"Subjet_2\",'Size':'30','Time':run_time,'Command_Acc_p15':acc_p15_c,\n",
    "                                    'Command_Acc_p1':acc_p1_c,'Command_Acc_p2':acc_p2_c,\n",
    "                                    'Command_Acc_p3':acc_p3_c,'Command_Acc_p4':acc_p4_c,'Command_Acc_p5':acc_p5_c,\n",
    "                                    'Acc_p6':acc_p6,'Acc_p7':acc_p7,'Acc_p8':acc_p8}, ignore_index=True)\n",
    "\n",
    "\n",
    "    if high_acc < acc_p15_c :\n",
    "        resnet_model.save('Initial_subject_2_model_size30_0608.h5')\n",
    "        high_acc = acc_p15_c \n",
    "        best_index = run\n",
    "        pd.DataFrame.from_dict(history1.history).to_csv('subject_2_size30_history1_0608.csv',index=False)\n",
    "        pd.DataFrame.from_dict(history2.history).to_csv('subject_2_size30_history2_0608.csv',index=False)\n",
    "        \n",
    "    del resnet_model\n",
    "    run = run + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "0.9444\n"
     ]
    }
   ],
   "source": [
    "resnet_model = tf.keras.models.load_model('Initial_subject_2_model_size30_0608.h5')\n",
    "predictions = resnet_model.predict(Test_Inputs_2)\n",
    "predicted_classes =  np.argmax(predictions, axis=1)\n",
    "acc_c = round(sum(x == y for x, y in zip(np.argmax(Test_command_class_2, axis=1), predicted_classes)) / len(np.argmax(Test_command_class_2, axis=1)),4)\n",
    "print(acc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Time</th>\n",
       "      <th>Command_Acc_p15</th>\n",
       "      <th>Command_Acc_p1</th>\n",
       "      <th>Command_Acc_p2</th>\n",
       "      <th>Command_Acc_p3</th>\n",
       "      <th>Command_Acc_p4</th>\n",
       "      <th>Command_Acc_p5</th>\n",
       "      <th>Acc_p6</th>\n",
       "      <th>Acc_p7</th>\n",
       "      <th>Acc_p8</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.269878</td>\n",
       "      <td>0.6243</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>12.062418</td>\n",
       "      <td>0.5709</td>\n",
       "      <td>0.9189</td>\n",
       "      <td>0.3148</td>\n",
       "      <td>0.4696</td>\n",
       "      <td>0.5413</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.520935</td>\n",
       "      <td>0.6335</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.6514</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.664869</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.2685</td>\n",
       "      <td>0.5217</td>\n",
       "      <td>0.6789</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.2970</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.347304</td>\n",
       "      <td>0.6188</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.2574</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.131006</td>\n",
       "      <td>0.5672</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>0.4696</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.651466</td>\n",
       "      <td>0.5930</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.2685</td>\n",
       "      <td>0.5130</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>12.100064</td>\n",
       "      <td>0.5727</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>0.4957</td>\n",
       "      <td>0.5413</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2574</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.176442</td>\n",
       "      <td>0.6151</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.3611</td>\n",
       "      <td>0.4957</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.929373</td>\n",
       "      <td>0.6446</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.4722</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.902663</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.6514</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.2178</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>13.805319</td>\n",
       "      <td>0.6538</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3519</td>\n",
       "      <td>0.5913</td>\n",
       "      <td>0.6697</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.685424</td>\n",
       "      <td>0.6298</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.194757</td>\n",
       "      <td>0.6390</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3981</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.364569</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>0.5739</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.957827</td>\n",
       "      <td>0.6317</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.3981</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.528849</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.4352</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.993745</td>\n",
       "      <td>0.6538</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.3611</td>\n",
       "      <td>0.5913</td>\n",
       "      <td>0.6697</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.823134</td>\n",
       "      <td>0.6446</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.5478</td>\n",
       "      <td>0.6606</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.812350</td>\n",
       "      <td>0.6354</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>0.5304</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.680686</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>0.5913</td>\n",
       "      <td>0.6697</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.529958</td>\n",
       "      <td>0.6851</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.4259</td>\n",
       "      <td>0.5826</td>\n",
       "      <td>0.7248</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2178</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>17.791133</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4352</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.228115</td>\n",
       "      <td>0.6483</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.6697</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.815590</td>\n",
       "      <td>0.6740</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>16.099657</td>\n",
       "      <td>0.6980</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4630</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.7523</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>14.533707</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.3981</td>\n",
       "      <td>0.5826</td>\n",
       "      <td>0.6514</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.714685</td>\n",
       "      <td>0.6501</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.6606</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>15.797302</td>\n",
       "      <td>0.6556</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4259</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.6514</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Subjet_1</td>\n",
       "      <td>19.045552</td>\n",
       "      <td>0.6924</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5463</td>\n",
       "      <td>0.5739</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>15.484598</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.2342</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.3130</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.2178</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.018696</td>\n",
       "      <td>0.4144</td>\n",
       "      <td>0.2432</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>0.3578</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>11.138138</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>0.3043</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.3465</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.840607</td>\n",
       "      <td>0.4033</td>\n",
       "      <td>0.2162</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>12.762539</td>\n",
       "      <td>0.3996</td>\n",
       "      <td>0.2072</td>\n",
       "      <td>0.8426</td>\n",
       "      <td>0.3565</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.125518</td>\n",
       "      <td>0.4144</td>\n",
       "      <td>0.2162</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>10.908749</td>\n",
       "      <td>0.3812</td>\n",
       "      <td>0.1802</td>\n",
       "      <td>0.8519</td>\n",
       "      <td>0.2696</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>13.933343</td>\n",
       "      <td>0.3941</td>\n",
       "      <td>0.2523</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.3578</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.2178</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>13.496468</td>\n",
       "      <td>0.4217</td>\n",
       "      <td>0.2523</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.3304</td>\n",
       "      <td>0.3670</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.3366</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>13.351789</td>\n",
       "      <td>0.3886</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.123689</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.2162</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.2696</td>\n",
       "      <td>0.3211</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>15.569148</td>\n",
       "      <td>0.4015</td>\n",
       "      <td>0.2973</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.3130</td>\n",
       "      <td>0.2936</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>15.396336</td>\n",
       "      <td>0.4180</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.3130</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>16.548319</td>\n",
       "      <td>0.4180</td>\n",
       "      <td>0.2883</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.802912</td>\n",
       "      <td>0.4420</td>\n",
       "      <td>0.2973</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.3913</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.052941</td>\n",
       "      <td>0.4125</td>\n",
       "      <td>0.2432</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>0.2936</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.914502</td>\n",
       "      <td>0.4217</td>\n",
       "      <td>0.2072</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.3217</td>\n",
       "      <td>0.3303</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>16.833878</td>\n",
       "      <td>0.4033</td>\n",
       "      <td>0.2072</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.3119</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.2970</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>15.493673</td>\n",
       "      <td>0.4217</td>\n",
       "      <td>0.2432</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>15.888272</td>\n",
       "      <td>0.4088</td>\n",
       "      <td>0.2432</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>0.3119</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.2970</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>17.029416</td>\n",
       "      <td>0.4678</td>\n",
       "      <td>0.2793</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.3304</td>\n",
       "      <td>0.4495</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.2574</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>16.197719</td>\n",
       "      <td>0.4512</td>\n",
       "      <td>0.2973</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>0.3578</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>14.890030</td>\n",
       "      <td>0.4549</td>\n",
       "      <td>0.2793</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.3652</td>\n",
       "      <td>0.3761</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.3267</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>15.636124</td>\n",
       "      <td>0.4401</td>\n",
       "      <td>0.2793</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.3670</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>16.359275</td>\n",
       "      <td>0.4715</td>\n",
       "      <td>0.2883</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.3913</td>\n",
       "      <td>0.4312</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.2970</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>15.138366</td>\n",
       "      <td>0.4346</td>\n",
       "      <td>0.2703</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.3217</td>\n",
       "      <td>0.3394</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>18.673174</td>\n",
       "      <td>0.4678</td>\n",
       "      <td>0.3153</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.3913</td>\n",
       "      <td>0.3761</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.3465</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>18.714967</td>\n",
       "      <td>0.4733</td>\n",
       "      <td>0.3243</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.3761</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>16.846094</td>\n",
       "      <td>0.4180</td>\n",
       "      <td>0.2432</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.2522</td>\n",
       "      <td>0.3761</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.3267</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Subjet_2</td>\n",
       "      <td>18.354382</td>\n",
       "      <td>0.4788</td>\n",
       "      <td>0.2883</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.3739</td>\n",
       "      <td>0.4771</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.3069</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model       Time  Command_Acc_p15  Command_Acc_p1  Command_Acc_p2  \\\n",
       "0   Subjet_1  15.269878           0.6243          0.9369          0.3889   \n",
       "1   Subjet_1  12.062418           0.5709          0.9189          0.3148   \n",
       "2   Subjet_1  13.520935           0.6335          0.9550          0.3889   \n",
       "3   Subjet_1  14.664869           0.6225          0.9550          0.2685   \n",
       "4   Subjet_1  13.347304           0.6188          0.9279          0.3796   \n",
       "5   Subjet_1  14.131006           0.5672          0.9279          0.2870   \n",
       "6   Subjet_1  13.651466           0.5930          0.9459          0.2685   \n",
       "7   Subjet_1  12.100064           0.5727          0.9279          0.2870   \n",
       "8   Subjet_1  13.176442           0.6151          0.9459          0.3611   \n",
       "9   Subjet_1  13.929373           0.6446          0.9369          0.4722   \n",
       "10  Subjet_1  15.902663           0.6409          0.9820          0.3704   \n",
       "11  Subjet_1  13.805319           0.6538          1.0000          0.3519   \n",
       "12  Subjet_1  15.685424           0.6298          0.9820          0.3333   \n",
       "13  Subjet_1  16.194757           0.6390          0.9820          0.3981   \n",
       "14  Subjet_1  15.364569           0.6611          0.9820          0.3796   \n",
       "15  Subjet_1  14.957827           0.6317          0.9730          0.3981   \n",
       "16  Subjet_1  15.528849           0.6409          0.9910          0.4352   \n",
       "17  Subjet_1  15.993745           0.6538          0.9910          0.3611   \n",
       "18  Subjet_1  16.823134           0.6446          0.9730          0.3704   \n",
       "19  Subjet_1  15.812350           0.6354          0.9820          0.3889   \n",
       "20  Subjet_1  16.680686           0.6667          1.0000          0.3889   \n",
       "21  Subjet_1  14.529958           0.6851          0.9910          0.4259   \n",
       "22  Subjet_1  17.791133           0.6796          1.0000          0.4352   \n",
       "23  Subjet_1  16.228115           0.6483          1.0000          0.3704   \n",
       "24  Subjet_1  14.815590           0.6740          1.0000          0.4167   \n",
       "25  Subjet_1  16.099657           0.6980          1.0000          0.4630   \n",
       "26  Subjet_1  14.533707           0.6667          0.9910          0.3981   \n",
       "27  Subjet_1  15.714685           0.6501          1.0000          0.4167   \n",
       "28  Subjet_1  15.797302           0.6556          1.0000          0.4259   \n",
       "29  Subjet_1  19.045552           0.6924          1.0000          0.5463   \n",
       "30  Subjet_2  15.484598           0.3959          0.2342          0.8611   \n",
       "31  Subjet_2  14.018696           0.4144          0.2432          0.8611   \n",
       "32  Subjet_2  11.138138           0.3738          0.1982          0.7778   \n",
       "33  Subjet_2  14.840607           0.4033          0.2162          0.8611   \n",
       "34  Subjet_2  12.762539           0.3996          0.2072          0.8426   \n",
       "35  Subjet_2  14.125518           0.4144          0.2162          0.8611   \n",
       "36  Subjet_2  10.908749           0.3812          0.1802          0.8519   \n",
       "37  Subjet_2  13.933343           0.3941          0.2523          0.8333   \n",
       "38  Subjet_2  13.496468           0.4217          0.2523          0.8611   \n",
       "39  Subjet_2  13.351789           0.3886          0.1982          0.8704   \n",
       "40  Subjet_2  14.123689           0.3959          0.2162          0.8889   \n",
       "41  Subjet_2  15.569148           0.4015          0.2973          0.8611   \n",
       "42  Subjet_2  15.396336           0.4180          0.2252          0.9167   \n",
       "43  Subjet_2  16.548319           0.4180          0.2883          0.8981   \n",
       "44  Subjet_2  14.802912           0.4420          0.2973          0.9074   \n",
       "45  Subjet_2  14.052941           0.4125          0.2432          0.8889   \n",
       "46  Subjet_2  14.914502           0.4217          0.2072          0.9537   \n",
       "47  Subjet_2  16.833878           0.4033          0.2072          0.9259   \n",
       "48  Subjet_2  15.493673           0.4217          0.2432          0.9444   \n",
       "49  Subjet_2  15.888272           0.4088          0.2432          0.9259   \n",
       "50  Subjet_2  17.029416           0.4678          0.2793          0.9630   \n",
       "51  Subjet_2  16.197719           0.4512          0.2973          0.9352   \n",
       "52  Subjet_2  14.890030           0.4549          0.2793          0.9352   \n",
       "53  Subjet_2  15.636124           0.4401          0.2793          0.9444   \n",
       "54  Subjet_2  16.359275           0.4715          0.2883          0.9352   \n",
       "55  Subjet_2  15.138366           0.4346          0.2703          0.9352   \n",
       "56  Subjet_2  18.673174           0.4678          0.3153          0.9352   \n",
       "57  Subjet_2  18.714967           0.4733          0.3243          0.9352   \n",
       "58  Subjet_2  16.846094           0.4180          0.2432          0.9259   \n",
       "59  Subjet_2  18.354382           0.4788          0.2883          0.9444   \n",
       "\n",
       "    Command_Acc_p3  Command_Acc_p4  Command_Acc_p5  Acc_p6  Acc_p7  Acc_p8  \\\n",
       "0           0.5391          0.6239            0.63    0.38   0.384  0.2673   \n",
       "1           0.4696          0.5413            0.61    0.33   0.320  0.2673   \n",
       "2           0.5478          0.6514            0.62    0.48   0.384  0.2475   \n",
       "3           0.5217          0.6789            0.69    0.26   0.456  0.2970   \n",
       "4           0.5652          0.5780            0.64    0.39   0.416  0.2574   \n",
       "5           0.4696          0.5780            0.57    0.25   0.328  0.2673   \n",
       "6           0.5130          0.6239            0.61    0.43   0.440  0.2673   \n",
       "7           0.4957          0.5413            0.61    0.41   0.384  0.2574   \n",
       "8           0.4957          0.6147            0.66    0.41   0.408  0.2871   \n",
       "9           0.5391          0.6147            0.66    0.48   0.408  0.2673   \n",
       "10          0.5565          0.6514            0.64    0.34   0.400  0.2178   \n",
       "11          0.5913          0.6697            0.65    0.45   0.360  0.2277   \n",
       "12          0.5391          0.6422            0.65    0.43   0.408  0.2277   \n",
       "13          0.6000          0.6147            0.59    0.36   0.384  0.2475   \n",
       "14          0.5739          0.6972            0.67    0.44   0.400  0.2673   \n",
       "15          0.5652          0.6239            0.59    0.38   0.440  0.2475   \n",
       "16          0.5565          0.6239            0.59    0.37   0.360  0.2079   \n",
       "17          0.5913          0.6697            0.65    0.41   0.360  0.2277   \n",
       "18          0.5478          0.6606            0.67    0.37   0.384  0.2475   \n",
       "19          0.5304          0.6422            0.63    0.33   0.408  0.2772   \n",
       "20          0.5913          0.6697            0.68    0.43   0.352  0.2079   \n",
       "21          0.5826          0.7248            0.70    0.42   0.360  0.2178   \n",
       "22          0.5652          0.7156            0.68    0.36   0.328  0.2277   \n",
       "23          0.5565          0.6697            0.64    0.41   0.344  0.2079   \n",
       "24          0.5565          0.7339            0.66    0.45   0.344  0.2079   \n",
       "25          0.6000          0.7523            0.67    0.37   0.352  0.2079   \n",
       "26          0.5826          0.6514            0.71    0.36   0.360  0.2277   \n",
       "27          0.5391          0.6606            0.63    0.38   0.360  0.2277   \n",
       "28          0.5565          0.6514            0.64    0.39   0.352  0.2079   \n",
       "29          0.5739          0.7156            0.62    0.41   0.376  0.2277   \n",
       "30          0.3130          0.3303            0.24    0.21   0.344  0.2178   \n",
       "31          0.3478          0.3578            0.26    0.14   0.320  0.2871   \n",
       "32          0.3043          0.3028            0.29    0.16   0.272  0.3465   \n",
       "33          0.3478          0.3303            0.26    0.21   0.312  0.3168   \n",
       "34          0.3565          0.3028            0.29    0.16   0.312  0.3168   \n",
       "35          0.3391          0.3486            0.31    0.22   0.352  0.3168   \n",
       "36          0.2696          0.3303            0.28    0.14   0.256  0.2673   \n",
       "37          0.2957          0.3578            0.23    0.27   0.288  0.2178   \n",
       "38          0.3304          0.3670            0.30    0.18   0.280  0.3366   \n",
       "39          0.2870          0.3303            0.26    0.13   0.272  0.2673   \n",
       "40          0.2696          0.3211            0.29    0.17   0.368  0.3168   \n",
       "41          0.3130          0.2936            0.24    0.22   0.296  0.2079   \n",
       "42          0.3130          0.3486            0.29    0.23   0.336  0.2673   \n",
       "43          0.2870          0.3028            0.32    0.25   0.360  0.2871   \n",
       "44          0.3913          0.3303            0.28    0.17   0.320  0.3564   \n",
       "45          0.3478          0.2936            0.29    0.24   0.304  0.2673   \n",
       "46          0.3217          0.3303            0.30    0.19   0.368  0.2772   \n",
       "47          0.2957          0.3119            0.28    0.23   0.360  0.2970   \n",
       "48          0.2870          0.3486            0.29    0.17   0.328  0.2772   \n",
       "49          0.2870          0.3119            0.28    0.21   0.320  0.2970   \n",
       "50          0.3304          0.4495            0.32    0.19   0.328  0.2574   \n",
       "51          0.3478          0.3578            0.32    0.24   0.320  0.2772   \n",
       "52          0.3652          0.3761            0.32    0.23   0.400  0.3267   \n",
       "53          0.2957          0.3670            0.32    0.19   0.344  0.2772   \n",
       "54          0.3913          0.4312            0.31    0.26   0.320  0.2970   \n",
       "55          0.3217          0.3394            0.31    0.27   0.336  0.2772   \n",
       "56          0.3913          0.3761            0.32    0.25   0.424  0.3465   \n",
       "57          0.4000          0.3761            0.33    0.26   0.368  0.2871   \n",
       "58          0.2522          0.3761            0.30    0.17   0.288  0.3267   \n",
       "59          0.3739          0.4771            0.31    0.28   0.320  0.3069   \n",
       "\n",
       "   Size  \n",
       "0    10  \n",
       "1    10  \n",
       "2    10  \n",
       "3    10  \n",
       "4    10  \n",
       "5    10  \n",
       "6    10  \n",
       "7    10  \n",
       "8    10  \n",
       "9    10  \n",
       "10   20  \n",
       "11   20  \n",
       "12   20  \n",
       "13   20  \n",
       "14   20  \n",
       "15   20  \n",
       "16   20  \n",
       "17   20  \n",
       "18   20  \n",
       "19   20  \n",
       "20   30  \n",
       "21   30  \n",
       "22   30  \n",
       "23   30  \n",
       "24   30  \n",
       "25   30  \n",
       "26   30  \n",
       "27   30  \n",
       "28   30  \n",
       "29   30  \n",
       "30   10  \n",
       "31   10  \n",
       "32   10  \n",
       "33   10  \n",
       "34   10  \n",
       "35   10  \n",
       "36   10  \n",
       "37   10  \n",
       "38   10  \n",
       "39   10  \n",
       "40   20  \n",
       "41   20  \n",
       "42   20  \n",
       "43   20  \n",
       "44   20  \n",
       "45   20  \n",
       "46   20  \n",
       "47   20  \n",
       "48   20  \n",
       "49   20  \n",
       "50   30  \n",
       "51   30  \n",
       "52   30  \n",
       "53   30  \n",
       "54   30  \n",
       "55   30  \n",
       "56   30  \n",
       "57   30  \n",
       "58   30  \n",
       "59   30  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perfomance.to_csv('Performance_0608_single_model.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
